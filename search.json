[
  {
    "objectID": "slides/slides-02.html#announcements",
    "href": "slides/slides-02.html#announcements",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Announcements",
    "text": "Announcements\n\nSlide link https://aroth85.github.io/cpsc330-slides\nThings due this week\n\nHomework 1 (hw1): Due Jan 14 11:59pm\n\nHomework 2 (hw2) has been released (Due: Jan 20, 11:59pm)\n\nThere is some autograding in this homework.\n\nYou can find the tentative due dates for all deliverables here.\nPlease monitor Piazza (especially pinned posts and instructor posts) for announcements.\nI’ll assume that you’ve watched the pre-lecture videos."
  },
  {
    "objectID": "slides/slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "href": "slides/slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Suggested workflow for working with Jupyter Notebooks",
    "text": "Suggested workflow for working with Jupyter Notebooks\n\nCreate a folder on your computer that will have all the CPSC 330 repos:\n\n~/School/Year3/CPSC330/ &lt;– Consider this your CPSC parent folder\n\nCreate subfolders for: hw, class, practice\nIn the hw folder, you will then clone hw1, hw2, hw3, etc…\nIn the class folder, you will clone the CPSC330-2024W2 repo which contains all the class jupyter notebooks\n\nDo not make any changes to files in this directory/repo, you will have trouble when you pull stuff during each class.\nIf you did make changes, you can reset to the last commit and DESTROY any changes you made (be careful with this command) using: git reset --hard\n\nIn the practice folder, you can copy any notebooks (.ipynb) and files (like data/*.csv) you want to try running locally and experiment"
  },
  {
    "objectID": "slides/slides-02.html#learning-outcomes",
    "href": "slides/slides-02.html#learning-outcomes",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nIdentify whether a given problem could be solved using supervised machine learning or not;\nDifferentiate between supervised and unsupervised machine learning;\nExplain machine learning terminology such as features, targets, predictions, training, and error;\nDifferentiate between classification and regression problems;"
  },
  {
    "objectID": "slides/slides-02.html#learning-outcomes-contd",
    "href": "slides/slides-02.html#learning-outcomes-contd",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Learning outcomes (contd)",
    "text": "Learning outcomes (contd)\n\nUse DummyClassifier and DummyRegressor as baselines for machine learning problems;\nExplain the fit and predict paradigm and use score method of ML models;\nBroadly describe how decision tree prediction works;\nUse DecisionTreeClassifier and DecisionTreeRegressor to build decision trees using scikit-learn;\nVisualize decision trees;\nExplain the difference between parameters and hyperparameters;\nExplain the concept of decision boundaries;\nExplain the relation between model complexity and decision boundaries."
  },
  {
    "objectID": "slides/slides-02.html#big-picture",
    "href": "slides/slides-02.html#big-picture",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Big picture",
    "text": "Big picture\nIn this lecture, we’ll talk about our first machine learning model: Decision trees. We will also familiarize ourselves with some common terminology in supervised machine learning."
  },
  {
    "objectID": "slides/slides-02.html#recap-what-is-ml",
    "href": "slides/slides-02.html#recap-what-is-ml",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: What is ML?",
    "text": "Recap: What is ML?\n\nML uses data to build models that find patterns, make predictions, or generate content.\nIt helps computers learn from data to make decisions.\nNo one model works for every situation."
  },
  {
    "objectID": "slides/slides-02.html#recap-supervised-learning",
    "href": "slides/slides-02.html#recap-supervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: Supervised learning",
    "text": "Recap: Supervised learning\n\nWe wish to find a model function f that relates X to y.\nWe use the model function to predict targets of new examples.\n\n\n\n\n\n\nIn the first part of this course, we’ll focus on supervised machine learning."
  },
  {
    "objectID": "slides/slides-02.html#unsupervised-learning",
    "href": "slides/slides-02.html#unsupervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nIn unsupervised learning training data consists of observations (X) without any corresponding targets.\nUnsupervised learning could be used to group similar things together in X or to provide concise summary of the data.\n\n\n\n\n\n\nWe’ll learn more about this topic later."
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.1-supervised-vs-unsupervised",
    "href": "slides/slides-02.html#iclicker-2.1-supervised-vs-unsupervised",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.1: Supervised vs unsupervised",
    "text": "iClicker 2.1: Supervised vs unsupervised\nClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are examples of supervised machine learning\n\n\nFinding groups of similar properties in a real estate data set.\n\n\nPredicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n\n\nGrouping articles on different topics from different news sources (something like the Google News app).\n\n\nDetecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n\n\nGiven some measure of employee performance, identify the key factors which are likely to influence their performance."
  },
  {
    "objectID": "slides/slides-02.html#framework",
    "href": "slides/slides-02.html#framework",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Framework",
    "text": "Framework\n\nThere are many frameworks to do do machine learning.\nWe’ll mainly be using scikit-learn framework."
  },
  {
    "objectID": "slides/slides-02.html#running-example",
    "href": "slides/slides-02.html#running-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Running example",
    "text": "Running example\nImagine you’re in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people. Here are the first few rows of this toy dataset.\n\n\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02.html#features-target-example",
    "href": "slides/slides-02.html#features-target-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Features, target, example",
    "text": "Features, target, example\n\nWhat are the features X?\n\nfeatures = inputs = predictors = explanatory variables = regressors = independent variables = covariates\n\nWhat’s the target y?\n\ntarget = output = outcome = response variable = dependent variable = labels\n\nCan you think of other relevant features for the job happiness problem?"
  },
  {
    "objectID": "slides/slides-02.html#classification-vs.-regression",
    "href": "slides/slides-02.html#classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\nIn supervised machine learning, there are two main kinds of learning problems based on what they are trying to predict.\n\nClassification problem: predicting among two or more discrete classes\n\nExample1: Predict whether a patient has a liver disease or not\nExample2: Predict whether a student would get an A+ or not in quiz2.\n\nRegression problem: predicting a continuous value\n\nExample1: Predict housing prices\nExample2: Predict a student’s score in quiz2."
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.2-classification-vs.-regression",
    "href": "slides/slides-02.html#iclicker-2.2-classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.2: Classification vs. Regression",
    "text": "iClicker 2.2: Classification vs. Regression\nClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are examples of regression problems\n\n\nPredicting the price of a house based on features such as number of bedrooms and the year built.\n\n\nPredicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n\n\nPredicting percentage grade in CPSC 330 based on past grades.\n\n\nPredicting whether you should bicycle tomorrow or not based on the weather forecast.\n\n\nPredicting appropriate thermostat temperature based on the wind speed and the number of people in a room."
  },
  {
    "objectID": "slides/slides-02.html#classification-vs.-regression-1",
    "href": "slides/slides-02.html#classification-vs.-regression-1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\nIs this a classification problem or a regression problem?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02.html#prediction-vs.-inference",
    "href": "slides/slides-02.html#prediction-vs.-inference",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction vs. Inference",
    "text": "Prediction vs. Inference\n\nInference is using the model to understand the relationship between the features and the target\n\nWhy certain factors influence happiness?\n\nPrediction is using the model to predict the target value for new examples based on learned patterns.\nOf course these goals are related, and in many situations we need both."
  },
  {
    "objectID": "slides/slides-02.html#training",
    "href": "slides/slides-02.html#training",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training",
    "text": "Training\n\nIn supervised ML, the goal is to learn a function that maps input features (X) to a target (y).\nThe relationship between X and y is often complex, making it difficult to define mathematically.\nWe use algorithms to approximate this complex relationship between X and y.\nTraining is the process of applying an algorithm to learn the best function (or model) that maps X to y.\nIn this course, I’ll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline."
  },
  {
    "objectID": "slides/slides-02.html#separating-x-and-y",
    "href": "slides/slides-02.html#separating-x-and-y",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Separating X and y",
    "text": "Separating X and y\n\nIn order to train a model we need to separate X and y from the dataframe.\n\n\n# Extract the feature set by removing the target column \"happy?\"\nX = toy_happiness_df.drop(columns=[\"happy?\"])\n# Extract the target variable \"happy?\"\ny = toy_happiness_df[\"happy?\"]"
  },
  {
    "objectID": "slides/slides-02.html#baseline",
    "href": "slides/slides-02.html#baseline",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Baseline",
    "text": "Baseline\n\nLet’s try a simplest algorithm of predicting the most popular target!\n\n\nfrom sklearn.dummy import DummyClassifier\n# Initialize the DummyClassifier to always predict the most frequent class\nmodel = DummyClassifier(strategy=\"most_frequent\") \n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\n# Add the predicted values as a new column in the dataframe\ntoy_happiness_df['dummy_predictions'] = model.predict(X) \ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\ndummy_predictions\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\nHappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\nHappy\n\n\n2\n1\n80000\n1\n0\nHappy\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy\nHappy"
  },
  {
    "objectID": "slides/slides-02.html#score-your-model",
    "href": "slides/slides-02.html#score-your-model",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "score your model",
    "text": "score your model\n\nHow do you know how well your model is doing?\nFor classification problems, by default, score gives the accuracy of the model, i.e., proportion of correctly predicted targets.\n\n\nmodel.score(X, y)\n\n0.5714285714285714"
  },
  {
    "objectID": "slides/slides-02.html#steps-to-train-a-classifier-using-sklearn",
    "href": "slides/slides-02.html#steps-to-train-a-classifier-using-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Steps to train a classifier using sklearn",
    "text": "Steps to train a classifier using sklearn\n\nRead the data\nCreate X and y\nCreate a classifier object\nfit the classifier\npredict on new examples\nscore the model"
  },
  {
    "objectID": "slides/slides-02.html#dummyregressor",
    "href": "slides/slides-02.html#dummyregressor",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "DummyRegressor",
    "text": "DummyRegressor\nYou can also do the same thing for regression problems using DummyRegressor, which predicts mean, median, or constant value of the training set for all examples."
  },
  {
    "objectID": "slides/slides-02.html#pre-intuition",
    "href": "slides/slides-02.html#pre-intuition",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Pre-Intuition",
    "text": "Pre-Intuition\nLet’s play 20 questions! You can ask me up to 20 Yes/No questions to figure out the answer."
  },
  {
    "objectID": "slides/slides-02.html#intuition",
    "href": "slides/slides-02.html#intuition",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Intuition",
    "text": "Intuition\n\nDecision trees find the “best” way to split data to make predictions.\nEach split is based on a question, like ‘Are the colleagues supportive?’\nThe goal is to group data by similar outcomes at each step.\nNow, let’s see a decision tree using sklearn."
  },
  {
    "objectID": "slides/slides-02.html#decision-tree-with-sklearn",
    "href": "slides/slides-02.html#decision-tree-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree with sklearn",
    "text": "Decision tree with sklearn\nLet’s train a simple decision tree on our toy dataset.\n\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\n# Create a decision tree object\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1)\n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\nplot_tree(\n  model, filled=True, feature_names=X.columns, \n  class_names=[\"Happy\", \"Unhappy\"], impurity=False, fontsize=12\n);"
  },
  {
    "objectID": "slides/slides-02.html#prediction",
    "href": "slides/slides-02.html#prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction",
    "text": "Prediction\n\nGiven a new example, how does a decision tree predict the class of this example?\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,"
  },
  {
    "objectID": "slides/slides-02.html#prediction-with-sklearn",
    "href": "slides/slides-02.html#prediction-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction with sklearn",
    "text": "Prediction with sklearn\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,\n\n\n\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\n\nModel prediction:  ['Unhappy']"
  },
  {
    "objectID": "slides/slides-02.html#training-high-level",
    "href": "slides/slides-02.html#training-high-level",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training (high level)",
    "text": "Training (high level)\n\nHow many possible questions could we ask in this context?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\n\n\n\n\n0\n0\n70000\n0\n1\n\n\n1\n1\n60000\n0\n0\n\n\n2\n1\n80000\n1\n0\n\n\n3\n1\n110000\n0\n1\n\n\n4\n1\n120000\n1\n0\n\n\n5\n1\n150000\n1\n1\n\n\n6\n0\n150000\n1\n0"
  },
  {
    "objectID": "slides/slides-02.html#training-high-level-1",
    "href": "slides/slides-02.html#training-high-level-1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training (high level)",
    "text": "Training (high level)\n\nDecision tree learning is a search process to find the “best” tree among many possible ones.\nWe evaluate questions using measures like information gain or the Gini index to find the most effective split.\nAt each step, we aim to split the data into groups with more certainty in their outcomes."
  },
  {
    "objectID": "slides/slides-02.html#parameters-vs.-hyperparameters",
    "href": "slides/slides-02.html#parameters-vs.-hyperparameters",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Parameters vs. Hyperparameters",
    "text": "Parameters vs. Hyperparameters\n\nParameters\n\nThe questions (features and thresholds) used to split the data at each node.\nExample: salary &lt;= 75000 at the root node\n\n\nHyperparameters\n\nSettings that control tree growth, like max_depth, which limits how deep the tree can go."
  },
  {
    "objectID": "slides/slides-02.html#decision-boundary-with-max_depth1",
    "href": "slides/slides-02.html#decision-boundary-with-max_depth1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=1",
    "text": "Decision boundary with max_depth=1"
  },
  {
    "objectID": "slides/slides-02.html#decision-boundary-with-max_depth2",
    "href": "slides/slides-02.html#decision-boundary-with-max_depth2",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=2",
    "text": "Decision boundary with max_depth=2"
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.3-baselines-and-decision-trees",
    "href": "slides/slides-02.html#iclicker-2.3-baselines-and-decision-trees",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.3: Baselines and Decision trees",
    "text": "iClicker 2.3: Baselines and Decision trees\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nChange in features (i.e., binarizing features above) would change DummyClassifier predictions.\n\n\nPredict takes only X as argument whereas fit and score take both X and y as arguments.\n\n\nFor the decision tree algorithm to work, the feature values must be binary.\n\n\nThe prediction in a decision tree works by routing the example from the root to the leaf."
  },
  {
    "objectID": "slides/slides-02.html#question-for-next-lecture-decision-boundaries",
    "href": "slides/slides-02.html#question-for-next-lecture-decision-boundaries",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Question for next lecture: Decision boundaries",
    "text": "Question for next lecture: Decision boundaries\nSelect the TRUE statement.\n\n\nThe decision boundary in the image below could come from a decision tree.\n\n\nThe decision boundary in the image below could not come from a decision tree.\n\n\nThere is not enough information to determine if a decision tree could create this boundary."
  },
  {
    "objectID": "slides/slides-02.html#what-we-learned-today",
    "href": "slides/slides-02.html#what-we-learned-today",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "What we learned today",
    "text": "What we learned today\n\nThere is a lot of terminology and jargon used in ML.\nSteps to train a supervised machine learning model.\nWhat baselines are and why they are useful.\nWhat decision trees are and how they work intuitively.\nWhat decision boundaries are."
  },
  {
    "objectID": "lecture-02.html",
    "href": "lecture-02.html",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "lecture-02.html#slides",
    "href": "lecture-02.html#slides",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "lecture-02.html#outline",
    "href": "lecture-02.html#outline",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "Outline",
    "text": "Outline\n\nFeatures, target, examples, training\nParameters and hyperparameters\nDecision boundary\nClassification vs. regression\nInference vs. prediction\nAccuracy vs. error, baselines\nIntuition of decision trees",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs. regression, inference vs. prediction, accuracy vs. error, baselines, intuition of decision trees\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "lectures.html#schedule",
    "href": "lectures.html#schedule",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs. regression, inference vs. prediction, accuracy vs. error, baselines, intuition of decision trees\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to CPSC330! Here, you’ll find slides for CPSC 330 Section 204. These slides are based on the notes present here.\n\nClass times 🕘 11 am to 12:20 pm\nWhere? 📍 Geography Building (GEOG) - 212, 1984 West Mall, Vancouver, BC V6T 1Z2"
  },
  {
    "objectID": "lecture-01.html",
    "href": "lecture-01.html",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#slides",
    "href": "lecture-01.html#slides",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#outline",
    "href": "lecture-01.html#outline",
    "title": "Lecture 1: Course introduction",
    "section": "Outline",
    "text": "Outline\n\nWhat is machine learning\nTypes of machine learning\nLearning to navigate through the course materials\nGetting familiar with the course policies",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "slides/slides-01.html#learning-outcomes",
    "href": "slides/slides-01.html#learning-outcomes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nExplain the motivation behind study machine learning.\nBriefly describe supervised learning.\nDifferentiate between traditional programming and machine learning.\nAssess whether a given problem is suitable for a machine learning solution.\nNavigate through the course material.\nBe familiar with the policies and how the class is going to run."
  },
  {
    "objectID": "slides/slides-01.html#cpsc-330-website",
    "href": "slides/slides-01.html#cpsc-330-website",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 website",
    "text": "CPSC 330 website\n\n\n\nCourse Jupyter book: https://ubc-cs.github.io/cpsc330-2024W2/README.html\nCourse GitHub repository: https://github.com/UBC-CS/cpsc330-2024W2"
  },
  {
    "objectID": "slides/slides-01.html#meet-your-instructor",
    "href": "slides/slides-01.html#meet-your-instructor",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet your instructor",
    "text": "Meet your instructor\n\n\n\n\n\nAndrew Roth\nI am an Assistant Professor Departments of Computer Science and Pathology & Laboratory Medicine.\nI received my Ph.D. in Bioinformatics at UBC.\nMy research uses statistical machine learning methods to study cancer.\nContact information\n\nEmail: aroth@cs.ubc.ca\nOffice: ICCS 359"
  },
  {
    "objectID": "slides/slides-01.html#meet-eva-a-fictitious-persona",
    "href": "slides/slides-01.html#meet-eva-a-fictitious-persona",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet Eva (a fictitious persona)!",
    "text": "Meet Eva (a fictitious persona)!\n\n\n\n\nEva is among one of you. She has some experience in Python programming. She knows machine learning as a buzz word. During her recent internship, she has developed some interest and curiosity in the field. She wants to learn what is it and how to use it. She is a curious person and usually has a lot of questions!"
  },
  {
    "objectID": "slides/slides-01.html#you-all",
    "href": "slides/slides-01.html#you-all",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "You all",
    "text": "You all\n\nIntroduce yourself to your neighbour.\nSince we’re going to spend the semester with each other, I would like to know you a bit better."
  },
  {
    "objectID": "slides/slides-01.html#asking-questions-during-class",
    "href": "slides/slides-01.html#asking-questions-during-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Asking questions during class",
    "text": "Asking questions during class\nYou are welcome to ask questions by raising your hand. There is also a reflection Google Document for this course for your questions/comments/reflections. It will be great if you can write about your takeaways, struggle points, and general comments in this document so that I’ll try to address those points in the next lecture."
  },
  {
    "objectID": "slides/slides-01.html#activity-1-httpsshorturl.atd8v0w",
    "href": "slides/slides-01.html#activity-1-httpsshorturl.atd8v0w",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 1: https://shorturl.at/D8v0w",
    "text": "Activity 1: https://shorturl.at/D8v0w\n\n\n\nWrite your answers to the questions below in this Google doc: https://shorturl.at/D8v0w\nWhat do you know about machine learning?\nWhat would you like to get out of this course?\nAre there any particular topics or aspects of this course that you are especially excited or anxious about? Why?"
  },
  {
    "objectID": "slides/slides-01.html#spam-prediction",
    "href": "slides/slides-01.html#spam-prediction",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Spam prediction",
    "text": "Spam prediction\n\nSuppose you are given some data with labeled spam and non-spam messages\n\n\nCodeOutput\n\n\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(\n    sms_df, test_size=0.10, random_state=42\n)\n\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\nspam\nLookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\n\n\nham\nAight, I'll hit you up when I get some cash\n\n\nham\nDon no da:)whats you plan?\n\n\nham\nGoing to take your babe out ?\n\n\nham\nNo need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room."
  },
  {
    "objectID": "slides/slides-01.html#traditional-programming-vs.-ml",
    "href": "slides/slides-01.html#traditional-programming-vs.-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Traditional programming vs. ML",
    "text": "Traditional programming vs. ML\n\nImagine writing a Python program for spam identification, i.e., whether a text message or an email is spam or non-spam.\nTraditional programming\n\nCome up with rules using human understanding of spam messages.\nTime consuming and hard to come up with robust set of rules.\n\nMachine learning\n\nCollect large amount of data of spam and non-spam emails and let the machine learning algorithm figure out rules."
  },
  {
    "objectID": "slides/slides-01.html#lets-train-a-model",
    "href": "slides/slides-01.html#lets-train-a-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let’s train a model",
    "text": "Let’s train a model\n\nThere are several packages that help us perform machine learning.\n\n\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\nclf = make_pipeline(\n    CountVectorizer(max_features=5000), \n    LogisticRegression(max_iter=5000)\n)\nclf.fit(X_train, y_train); # Training the model"
  },
  {
    "objectID": "slides/slides-01.html#unseen-messages",
    "href": "slides/slides-01.html#unseen-messages",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Unseen messages",
    "text": "Unseen messages\n\nNow use the trained model to predict targets of unseen messages:\n\n\n\n\n\n\n\n\n \nsms\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok."
  },
  {
    "objectID": "slides/slides-01.html#predicting-on-unseen-data",
    "href": "slides/slides-01.html#predicting-on-unseen-data",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting on unseen data",
    "text": "Predicting on unseen data\nThe model is accurately predicting labels for the unseen text messages above!\n\npred_dict = {\n    \"sms\": X_test[0:4],\n    \"spam_predictions\": clf.predict(X_test[0:4]),\n}\npd.DataFrame(pred_dict)\n\n\n\n\n\n\n\n\nsms\nspam_predictions\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\nham\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one m...\nham\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\nspam\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\nham"
  },
  {
    "objectID": "slides/slides-01.html#a-different-way-to-solve-problems",
    "href": "slides/slides-01.html#a-different-way-to-solve-problems",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "A different way to solve problems",
    "text": "A different way to solve problems\nMachine learning uses computer programs to model data. It can be used to extract hidden patterns, make predictions in new situation, or generate novel content.\n\nA field of study that gives computers the ability to learn without being explicitly programmed.  – Arthur Samuel (1959)"
  },
  {
    "objectID": "slides/slides-01.html#ml-vs.-traditional-programming",
    "href": "slides/slides-01.html#ml-vs.-traditional-programming",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ML vs. traditional programming",
    "text": "ML vs. traditional programming\n\nWith machine learning, you’re likely to\n\nSave time\nCustomize and scale products"
  },
  {
    "objectID": "slides/slides-01.html#prevalence-of-ml",
    "href": "slides/slides-01.html#prevalence-of-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Prevalence of ML",
    "text": "Prevalence of ML\nLet’s look at some examples."
  },
  {
    "objectID": "slides/slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "href": "slides/slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity: For what type of problems ML is appropriate? (~5 mins)",
    "text": "Activity: For what type of problems ML is appropriate? (~5 mins)\nDiscuss with your neighbour for which of the following problems you would use machine learning\n\nFinding a list of prime numbers up to a limit\nGiven an image, automatically identifying and labeling objects in the image\nFinding the distance between two nodes in a graph"
  },
  {
    "objectID": "slides/slides-01.html#types-of-machine-learning",
    "href": "slides/slides-01.html#types-of-machine-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nHere are some typical learning problems.\n\nSupervised learning (Gmail spam filtering)\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nUnsupervised learning (Google News)\n\nTraining a model to find patterns in a dataset, typically an unlabeled dataset.\n\nReinforcement learning (AlphaGo)\n\nA family of algorithms for finding suitable actions to take in a given situation in order to maximize a reward.\n\nRecommendation systems (Amazon item recommendation system)\n\nPredict the “rating” or “preference” a user would give to an item."
  },
  {
    "objectID": "slides/slides-01.html#what-is-supervised-learning",
    "href": "slides/slides-01.html#what-is-supervised-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is supervised learning?",
    "text": "What is supervised learning?\n\nTraining data comprises a set of observations (X) and their corresponding targets (y).\nWe wish to find a model function f that relates X to y.\nWe use the model function to predict targets of new examples."
  },
  {
    "objectID": "slides/slides-01.html#evas-questions",
    "href": "slides/slides-01.html#evas-questions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "🤔 Eva’s questions",
    "text": "🤔 Eva’s questions\nAt this point, Eva is wondering about many questions.\n\nHow are we exactly “learning” whether a message is spam and ham?\nAre we expected to get correct predictions for all possible messages? How does it predict the label for a message it has not seen before?\n\nWhat if the model mis-labels an unseen example? For instance, what if the model incorrectly predicts a non-spam as a spam? What would be the consequences?\nHow do we measure the success or failure of spam identification?\nIf you want to use this model in the wild, how do you know how reliable it is?\n\nWould it be useful to know how confident the model is about the predictions rather than just a yes or a no?\n\n\nIt’s great to think about these questions right now. But Eva has to be patient. By the end of this course you’ll know answers to many of these questions!"
  },
  {
    "objectID": "slides/slides-01.html#predicting-labels-of-a-given-image",
    "href": "slides/slides-01.html#predicting-labels-of-a-given-image",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting labels of a given image",
    "text": "Predicting labels of a given image\n\nWe can also use machine learning to predict labels of given images using a technique called transfer learning.\n\n\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\n                     tiger cat              0.636\n              tabby, tabby cat              0.174\nPembroke, Pembroke Welsh corgi              0.081\n               lynx, catamount              0.011\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.994\n                  leopard, Panthera pardus              0.005\njaguar, panther, Panthera onca, Felis onca              0.001\n       snow leopard, ounce, Panthera uncia              0.000\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                   Class  Probability score\n                                 macaque              0.885\npatas, hussar monkey, Erythrocebus patas              0.062\n      proboscis monkey, Nasalis larvatus              0.015\n                       titi, titi monkey              0.010\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                        Class  Probability score\nWalker hound, Walker foxhound              0.582\n             English foxhound              0.144\n                       beagle              0.068\n                  EntleBucher              0.059\n--------------------------------------------------------------"
  },
  {
    "objectID": "slides/slides-01.html#predicting-housing-prices",
    "href": "slides/slides-01.html#predicting-housing-prices",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting housing prices",
    "text": "Predicting housing prices\nSuppose we want to predict housing prices given a number of attributes associated with houses. The target here is continuous and not discrete.\n\n\n\n\n\ntarget\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n509000.0\n2\n1.50\n1930\n3521\n2.0\n0\n0\n3\n8\n1930\n0\n1989\n0\n98007\n47.6092\n-122.146\n1840\n3576\n\n\n675000.0\n5\n2.75\n2570\n12906\n2.0\n0\n0\n3\n8\n2570\n0\n1987\n0\n98075\n47.5814\n-122.050\n2580\n12927\n\n\n420000.0\n3\n1.00\n1150\n5120\n1.0\n0\n0\n4\n6\n800\n350\n1946\n0\n98116\n47.5588\n-122.392\n1220\n5120\n\n\n680000.0\n8\n2.75\n2530\n4800\n2.0\n0\n0\n4\n7\n1390\n1140\n1901\n0\n98112\n47.6241\n-122.305\n1540\n4800\n\n\n357823.0\n3\n1.50\n1240\n9196\n1.0\n0\n0\n3\n8\n1240\n0\n1968\n0\n98072\n47.7562\n-122.094\n1690\n10800"
  },
  {
    "objectID": "slides/slides-01.html#building-a-regression-model",
    "href": "slides/slides-01.html#building-a-regression-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Building a regression model",
    "text": "Building a regression model\n\nfrom lightgbm.sklearn import LGBMRegressor\n\nX_train, y_train = train_df.drop(columns= [\"target\"]), train_df[\"target\"]\nX_test, y_test = test_df.drop(columns= [\"target\"]), train_df[\"target\"]\n\nmodel = LGBMRegressor()\nmodel.fit(X_train, y_train);\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2333\n[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 18\n[LightGBM] [Info] Start training from score 539762.702545"
  },
  {
    "objectID": "slides/slides-01.html#predicting-prices-of-unseen-houses",
    "href": "slides/slides-01.html#predicting-prices-of-unseen-houses",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting prices of unseen houses",
    "text": "Predicting prices of unseen houses\n\n\n\n\n\nPredicted_target\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n345831.740542\n4\n2.25\n2130\n8078\n1.0\n0\n0\n4\n7\n1380\n750\n1977\n0\n98055\n47.4482\n-122.209\n2300\n8112\n\n\n601042.018745\n3\n2.50\n2210\n7620\n2.0\n0\n0\n3\n8\n2210\n0\n1994\n0\n98052\n47.6938\n-122.130\n1920\n7440\n\n\n311310.186024\n4\n1.50\n1800\n9576\n1.0\n0\n0\n4\n7\n1800\n0\n1977\n0\n98045\n47.4664\n-121.747\n1370\n9576\n\n\n597555.592401\n3\n2.50\n1580\n1321\n2.0\n0\n2\n3\n8\n1080\n500\n2014\n0\n98107\n47.6688\n-122.402\n1530\n1357\n\n\n\n\n\nWe are predicting continuous values here as apposed to discrete values in spam vs. ham example."
  },
  {
    "objectID": "slides/slides-01.html#machine-learning-workflow",
    "href": "slides/slides-01.html#machine-learning-workflow",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Machine learning workflow",
    "text": "Machine learning workflow\nSupervised machine learning is quite flexible; it can be used on a variety of problems and different kinds of data. Here is a typical workflow of a supervised machine learning systems.\n\n\n\n\n\n\nWe will build machine learning pipelines in this course, focusing on some of the steps above."
  },
  {
    "objectID": "slides/slides-01.html#questions-for-you",
    "href": "slides/slides-01.html#questions-for-you",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "❓❓ Questions for you",
    "text": "❓❓ Questions for you\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are True (iClicker)\n\n\nPredicting spam is an example of machine learning.\n\n\nPredicting housing prices is not an example of machine learning.\n\n\nFor problems such as spelling correction, translation, face recognition, spam identification, if you are a domain expert, it’s usually faster and scalable to come up with a robust set of rules manually rather than building a machine learning model.\n\n\nIf you are asked to write a program to find all prime numbers up to a limit, it is better to implement one of the algorithms for doing so rather than using machine learning.\n\n\nGoogle News is likely be using machine learning to organize news."
  },
  {
    "objectID": "slides/slides-01.html#about-this-course",
    "href": "slides/slides-01.html#about-this-course",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "About this course",
    "text": "About this course\n\n\n\n\n\n\nImportant\n\n\nCourse website: https://github.com/UBC-CS/cpsc330-2024W2 is the most important link. Please read everything on this GitHub page!\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMake sure you go through the syllabus thoroughly and complete the syllabus quiz before Friday, Jan 24th at 11:59pm."
  },
  {
    "objectID": "slides/slides-01.html#cpsc-330-vs.-340",
    "href": "slides/slides-01.html#cpsc-330-vs.-340",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 vs. 340",
    "text": "CPSC 330 vs. 340\nRead https://github.com/UBC-CS/cpsc330-2024W2/blob/main/docs/330_vs_340.md which explains the difference between two courses.\nTLDR:\n\n340: how do ML models work?\n330: how do I use ML models?\nCPSC 340 has many prerequisites.\nCPSC 340 goes deeper but has a more narrow scope.\nI think CPSC 330 will be more useful if you just plan to apply basic ML."
  },
  {
    "objectID": "slides/slides-01.html#registration-waitlist-and-prerequisites",
    "href": "slides/slides-01.html#registration-waitlist-and-prerequisites",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Registration, waitlist and prerequisites",
    "text": "Registration, waitlist and prerequisites\n\n\n\n\n\n\nImportant\n\n\nPlease go through this document carefully before contacting your instructors about these issues. Even then, we are very unlikely to be able to help with registration, waitlist or prerequisite issues.\n\n\n\n\nIf you are on waitlist and if you’d like to try your chances, you should be able to access Canvas and Piazza.\n\nIf you’re unable to make it this time, this course will be offered in the summer."
  },
  {
    "objectID": "slides/slides-01.html#lecture-format",
    "href": "slides/slides-01.html#lecture-format",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture format",
    "text": "Lecture format\n\nIn person lectures Tu/Th.\nSometimes there will be videos to watch before lecture. You will find the list of pre-watch videos in the schedule on the course webpage.\nWe will also try to work on some questions and exercises together during the class.\nAll materials will be posted in this GitHub repository.\nWeekly Tu tutorial be office hours format run by the TAs.\nWeekly Th/Fr tutorials will be worksheet format run by the TAs.\nTutorials are completely optional.\n\nYou do not need to be registered in a tutorial.\nYou can attend whatever tutorials or office hours your want, regardless of in which/whether you’re registered."
  },
  {
    "objectID": "slides/slides-01.html#home-work-assignments",
    "href": "slides/slides-01.html#home-work-assignments",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Home work assignments",
    "text": "Home work assignments\n\nFirst homework assignment is due this coming Tuesday, January 14, midnight.\n\nThis is a relatively straightforward assignment on Python.\nIf you struggle with this assignment then that could be a sign that you will struggle later on in the course.\n\n\nYou must do the first two homework assignments on your own."
  },
  {
    "objectID": "slides/slides-01.html#exams",
    "href": "slides/slides-01.html#exams",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Exams",
    "text": "Exams\n\nWe’ll have two self-scheduled midterms and one final in Computer-based Testing Facility (CBTF)."
  },
  {
    "objectID": "slides/slides-01.html#course-structure",
    "href": "slides/slides-01.html#course-structure",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course structure",
    "text": "Course structure\n\nIntroduction\n\nWeek 1\n\nPart I: ML fundamentals, preprocessing, midterm 1\n\nWeeks 2, 3, 4, 5, 6, 7, 8\n\nPart II: Unsupervised learning, transfer learning, common special cases, midterm 2\n\nWeeks 8, 9, 10, 11, 12\n\nPart III: Communication and ethics\n\nML skills are not beneficial if you can’t use them responsibly and communicate your results. In this module we’ll talk about these aspects.\nWeeks 13, 14"
  },
  {
    "objectID": "slides/slides-01.html#code-of-conduct",
    "href": "slides/slides-01.html#code-of-conduct",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Code of conduct",
    "text": "Code of conduct\n\nOur main forum for getting help will be Piazza.\n\n\n\n\n\n\n\nImportant\n\n\nPlease read this entire document about asking for help. TLDR: Be nice."
  },
  {
    "objectID": "slides/slides-01.html#homework-format-jupyter-notebooks",
    "href": "slides/slides-01.html#homework-format-jupyter-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Homework format: Jupyter notebooks",
    "text": "Homework format: Jupyter notebooks\n\nOur notes are created in a Jupyter notebook, with file extension .ipynb.\nAlso, you will complete your homework assignments using Jupyter notebooks.\nConfusingly, “Jupyter notebook” is also the original application that opens .ipynb files - but has since been replaced by Jupyter lab.\n\nI am using Jupyter lab, some things might not work with the Jupyter notebook application.\nYou can also open these files in Visual Studio Code."
  },
  {
    "objectID": "slides/slides-01.html#jupyter-notebooks",
    "href": "slides/slides-01.html#jupyter-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\n\nNotebooks contain a mix of code, code output, markdown-formatted text (including LaTeX equations), and more.\nWhen you open a Jupyter notebook in one of these apps, the document is “live”, meaning you can run the code.\n\nFor example:\n\n1 + 1\n\n2\n\n\n\nx = [1, 2, 3]\nx[0] = 9999\nx\n\n[9999, 2, 3]"
  },
  {
    "objectID": "slides/slides-01.html#jupyter",
    "href": "slides/slides-01.html#jupyter",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter",
    "text": "Jupyter\n\nBy default, Jupyter prints out the result of the last line of code, so you don’t need as many print statements.\nIn addition to the “live” notebooks, Jupyter notebooks can be statically rendered in the web browser.\n\nThis can be convenient for quick read-only access, without needing to launch the Jupyter notebook/lab application.\nBut you need to launch the app properly to interact with the notebooks."
  },
  {
    "objectID": "slides/slides-01.html#lecture-notes",
    "href": "slides/slides-01.html#lecture-notes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture notes",
    "text": "Lecture notes\n\nAll the lectures are available here.\nWhile the lecture notes are largely fixed, there may be small changes over the course of the term.\nA “finalized” version will be pushed to GitHub and the Jupyter book right before each class.\nEach instructor will have slightly adapted versions of notes to present slides during lectures.\n\nYou will find the link to these slides in our repository: https://github.com/UBC-CS/cpsc330-2024W2/tree/main/lectures."
  },
  {
    "objectID": "slides/slides-01.html#grades",
    "href": "slides/slides-01.html#grades",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Grades",
    "text": "Grades\n\nThe grading breakdown is here.\nThe policy on challenging grades is here."
  },
  {
    "objectID": "slides/slides-01.html#recommended-browser-and-tools",
    "href": "slides/slides-01.html#recommended-browser-and-tools",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Recommended browser and tools",
    "text": "Recommended browser and tools\n\nYou can install Chrome here.\nYou can install Firefox here.\n\nIn this course, we will primarily be using Python , git, GitHub, Canvas, Gradescope, Piazza, and PrairieLearn."
  },
  {
    "objectID": "slides/slides-01.html#course-conda-environment",
    "href": "slides/slides-01.html#course-conda-environment",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course conda environment",
    "text": "Course conda environment\n\nFollow the setup instructions here to create a course conda environment on your computer.\nIf you do not have your computer with you, you can partner up with someone and set up your own computer later."
  },
  {
    "objectID": "slides/slides-01.html#python-requirementsresources",
    "href": "slides/slides-01.html#python-requirementsresources",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Python requirements/resources",
    "text": "Python requirements/resources\nWe will primarily use Python in this course.\nHere is the basic Python knowledge you’ll need for the course:\n\nBasic Python programming\nNumpy\nPandas\nBasic matplotlib\nSparse matrices\n\nHomework 1 is all about Python.\n\n\n\n\n\n\nNote\n\n\nWe do not have time to teach all the Python we need but you can find some useful Python resources here."
  },
  {
    "objectID": "slides/slides-01.html#checklist-for-you-before-the-next-class",
    "href": "slides/slides-01.html#checklist-for-you-before-the-next-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Checklist for you before the next class",
    "text": "Checklist for you before the next class\n\nAre you able to access course Canvas shell?\nAre you able to access course Piazza?\nAre you able to access Gradescope? (If not, refer to the Gradescope Student Guide.)\n\nPlease join Gradescope through the Canvas.\n\nAre you able to access iClicker Cloud for this course?\nDid you follow the setup instructions here to create a course conda environment on your computer?\nDid you complete the syllabus quiz on Prairilearn? (Due date: Friday, January 24th at 11:59pm)\nAre you almost finished or at least started with homework 1? (Due: Tuesday, January 14th at 11:59pm)"
  },
  {
    "objectID": "slides/slides-01.html#summary",
    "href": "slides/slides-01.html#summary",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Summary",
    "text": "Summary\n\nMachine learning is increasingly being applied across various fields.\nIn supervised learning, we are given a set of observations (X) and their corresponding targets (y) and we wish to find a model function that relates X to y.\nMachine learning is a different paradigm for problem solving. Very often it reduces the time you spend programming and helps customizing and scaling your products.\nBefore applying machine learning to a problem, it’s always advisable to assess whether a given problem is suitable for a machine learning solution or not."
  }
]