[
  {
    "objectID": "slides/slides-03.html#announcements",
    "href": "slides/slides-03.html#announcements",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 2 (hw2) (Due: Jan 20, 11:59pm)\n\nYou are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.\nGroup submissions are not allowed for this assignment.\n\nAdvice on keeping up with the material\n\nPractice!\nMake sure you run the lecture notes on your laptop and experiment with the code.\nStart early on homework assignments.\n\nIf you are still on the waitlist, it‚Äôs your responsibility to keep up with the material and submit assignments.\nLast day to drop without a W standing: Jan 17"
  },
  {
    "objectID": "slides/slides-03.html#learning-outcomes",
    "href": "slides/slides-03.html#learning-outcomes",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nExplain how decision boundaries change with the max_depth hyperparameter and this relates to model complexity\nExplain the concept of generalization\nExplain how and why we split data for training\nDescribe the fundamental tradeoff between training score and the train-test gap\nState the golden rule"
  },
  {
    "objectID": "slides/slides-03.html#big-picture",
    "href": "slides/slides-03.html#big-picture",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Big picture",
    "text": "Big picture\nIn machine learning we want to learn a mapping function from labeled data so that we can predict labels of unlabeled data.\nFor example, suppose we want to build a spam filtering system. We will take a large number of spam/non-spam messages from the past, learn patterns associated with spam/non-spam from them, and predict whether a new incoming message in someone‚Äôs inbox is spam or non-spam based on these patterns.\nSo we want to learn from the past but ultimately we want to apply it on the future email messages."
  },
  {
    "objectID": "slides/slides-03.html#review-of-decision-boundaries",
    "href": "slides/slides-03.html#review-of-decision-boundaries",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Review of decision boundaries",
    "text": "Review of decision boundaries\nSelect the TRUE statement.\n\n\nThe decision boundary in the image below could come from a decision tree.\n\n\nThe decision boundary in the image below could not come from a decision tree.\n\n\nThere is not enough information to determine if a decision tree could create this boundary."
  },
  {
    "objectID": "slides/slides-03.html#running-example",
    "href": "slides/slides-03.html#running-example",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Running example",
    "text": "Running example\n\n\n\n\n\n\n\n\n\nml_experience\nclass_attendance\nlab1\nlab2\nlab3\nlab4\nquiz1\nquiz2\n\n\n\n\n0\n1\n1\n92\n93\n84\n91\n92\nA+\n\n\n1\n1\n0\n94\n90\n80\n83\n91\nnot A+\n\n\n2\n0\n0\n78\n85\n83\n80\n80\nnot A+\n\n\n3\n0\n1\n91\n94\n92\n91\n89\nA+"
  },
  {
    "objectID": "slides/slides-03.html#setup",
    "href": "slides/slides-03.html#setup",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Setup",
    "text": "Setup\n\nX = classification_df.drop([\"quiz2\"], axis=1)\ny = classification_df[\"quiz2\"]\nX_subset = X[[\"lab4\", \"quiz1\"]]  # Let's consider a subset of the data for visualization\nX_subset.head(4)\n\n\n\n\n\n\n\n\nlab4\nquiz1\n\n\n\n\n0\n91\n92\n\n\n1\n83\n91\n\n\n2\n80\n80\n\n\n3\n91\n89"
  },
  {
    "objectID": "slides/slides-03.html#depth-1",
    "href": "slides/slides-03.html#depth-1",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Depth = 1",
    "text": "Depth = 1"
  },
  {
    "objectID": "slides/slides-03.html#depth-2",
    "href": "slides/slides-03.html#depth-2",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Depth = 2",
    "text": "Depth = 2"
  },
  {
    "objectID": "slides/slides-03.html#depth-6",
    "href": "slides/slides-03.html#depth-6",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Depth = 6",
    "text": "Depth = 6"
  },
  {
    "objectID": "slides/slides-03.html#complex-models-decrease-training-error",
    "href": "slides/slides-03.html#complex-models-decrease-training-error",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Complex models decrease training error",
    "text": "Complex models decrease training error"
  },
  {
    "objectID": "slides/slides-03.html#question",
    "href": "slides/slides-03.html#question",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Question",
    "text": "Question\n\nHow to pick the best depth?\nHow can we make sure that the model we have built would do reasonably well on new data in the wild when it‚Äôs deployed?\nWhich of the following rules learned by the decision tree algorithm are likely to generalize better to new data?\n\n\nRule 1: If class_attendance == 1 then grade is A+.\n\n\nRule 2: If lab3 &gt; 83.5 and quiz1 &lt;= 83.5 and lab2 &lt;= 88 then quiz2 grade is A+\n\nThink about these questions on your own or discuss them with your friend/neighbour."
  },
  {
    "objectID": "slides/slides-03.html#generalization-fundamental-goal-of-ml",
    "href": "slides/slides-03.html#generalization-fundamental-goal-of-ml",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Generalization: Fundamental goal of ML",
    "text": "Generalization: Fundamental goal of ML\n\nTo generalize beyond what we see in the training examples\n\nWe only have access to limited amount of training data and we want to learn a mapping function which would predict targets reasonably well for examples beyond this training data."
  },
  {
    "objectID": "slides/slides-03.html#generalizing-to-unseen-data",
    "href": "slides/slides-03.html#generalizing-to-unseen-data",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Generalizing to unseen data",
    "text": "Generalizing to unseen data\n\nWhat prediction would you expect for each image?"
  },
  {
    "objectID": "slides/slides-03.html#training-error-vs.-generalization-error",
    "href": "slides/slides-03.html#training-error-vs.-generalization-error",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Training error vs.¬†Generalization error",
    "text": "Training error vs.¬†Generalization error\n\nGiven a model \\(M\\), in ML, people usually talk about two kinds of errors of \\(M\\).\n\nError on the training data: \\(error_{training}(M)\\)\nError on the entire distribution \\(D\\) of data: \\(error_{D}(M)\\)\n\nWe are interested in the error on the entire distribution\n\n\n‚Ä¶ But we do not have access to the entire distribution üòû"
  },
  {
    "objectID": "slides/slides-03.html#how-to-approximate-generalization-error",
    "href": "slides/slides-03.html#how-to-approximate-generalization-error",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "How to approximate generalization error?",
    "text": "How to approximate generalization error?\nA common way is data splitting.\n\n\nKeep aside some randomly selected portion from the training data.\nfit (train) a model on the training portion only.\nscore (assess) the trained model on this set aside data to get a sense of how well the model would be able to generalize.\nPretend that the kept aside data is representative of the real distribution \\(D\\) of data."
  },
  {
    "objectID": "slides/slides-03.html#traintest-split",
    "href": "slides/slides-03.html#traintest-split",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Train/test split",
    "text": "Train/test split"
  },
  {
    "objectID": "slides/slides-03.html#train_test_split",
    "href": "slides/slides-03.html#train_test_split",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "train_test_split",
    "text": "train_test_split\n\nfrom sklearn.model_selection import train_test_split\n# 80%-20% train test split on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=99\n) \n\n\n\n\n\n\nData portion\nShape\n\n\n\n\nX\n(21, 7)\n\n\ny\n(21,)\n\n\nX_train\n(16, 7)\n\n\ny_train\n(16,)\n\n\nX_test\n(5, 7)\n\n\ny_test\n(5,)"
  },
  {
    "objectID": "slides/slides-03.html#training-vs-test-error-max_depth2",
    "href": "slides/slides-03.html#training-vs-test-error-max_depth2",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Training vs test error (max_depth=2)",
    "text": "Training vs test error (max_depth=2)\n\n\n\n\n\n\n\n\n\n\n\nTrain error:   0.125\nTest error:   0.400"
  },
  {
    "objectID": "slides/slides-03.html#training-vs-test-error-max_depth6",
    "href": "slides/slides-03.html#training-vs-test-error-max_depth6",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Training vs test error (max_depth=6)",
    "text": "Training vs test error (max_depth=6)\n\n\n\n\n\n\n\n\n\n\n\nTrain error:   0.000\nTest error:   0.600"
  },
  {
    "objectID": "slides/slides-03.html#trainvalidationtest-split",
    "href": "slides/slides-03.html#trainvalidationtest-split",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Train/validation/test split",
    "text": "Train/validation/test split\n\nSometimes it‚Äôs a good idea to have a separate data for hyperparameter tuning."
  },
  {
    "objectID": "slides/slides-03.html#summary-of-train-validation-test-and-deployment-data",
    "href": "slides/slides-03.html#summary-of-train-validation-test-and-deployment-data",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Summary of train, validation, test, and deployment data",
    "text": "Summary of train, validation, test, and deployment data\n\n\n\n\nfit\nscore\npredict\n\n\n\n\nTrain\n‚úîÔ∏è\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nValidation\n\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nTest\n\nonce\nonce\n\n\nDeployment\n\n\n‚úîÔ∏è\n\n\n\n\nYou can typically expect \\(E_{train} &lt; E_{validation} &lt; E_{test} &lt; E_{deployment}\\)."
  },
  {
    "objectID": "slides/slides-03.html#iclicker-3.1",
    "href": "slides/slides-03.html#iclicker-3.1",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "iClicker 3.1",
    "text": "iClicker 3.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nA decision tree model with no depth (the default max_depth in sklearn) is likely to perform very well on the deployment data.\n\n\nData splitting helps us assess how well our model would generalize.\n\n\nDeployment data is scored only once.\n\n\nValidation data could be used for hyperparameter optimization.\n\n\nIt‚Äôs recommended that data be shuffled before splitting it into train and test sets."
  },
  {
    "objectID": "slides/slides-03.html#problems-with-single-trainvalidation-split",
    "href": "slides/slides-03.html#problems-with-single-trainvalidation-split",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Problems with single train/validation split",
    "text": "Problems with single train/validation split\n\nIf your dataset is small you might end up with a tiny training and/or validation set.\nYou might be unlucky with your splits such that they don‚Äôt align well or don‚Äôt well represent your test data."
  },
  {
    "objectID": "slides/slides-03.html#cross-validation-to-the-rescue",
    "href": "slides/slides-03.html#cross-validation-to-the-rescue",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Cross-validation to the rescue!!",
    "text": "Cross-validation to the rescue!!\n\nSplit the data into \\(k\\) folds (\\(k&gt;2\\), often \\(k=10\\)). In the picture below \\(k=4\\).\nEach ‚Äúfold‚Äù gets a turn at being the validation set."
  },
  {
    "objectID": "slides/slides-03.html#cross-validation-using-scikit-learn",
    "href": "slides/slides-03.html#cross-validation-using-scikit-learn",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Cross-validation using scikit-learn",
    "text": "Cross-validation using scikit-learn\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\nmodel = DecisionTreeClassifier(max_depth=4)\ncv_scores = cross_val_score(model, X_train, y_train, cv=4)\ncv_scores\n\narray([0.5 , 0.75, 0.5 , 0.75])\n\n\n\n\n\nAverage cross-validation score = 0.62\nStandard deviation of cross-validation score = 0.12\n\n\n\n\n\ncv_errors = 1 - cv_scores\n\n\n\n\nAverage cross-validation error = 0.38\nStandard deviation of cross-validation error = 0.12"
  },
  {
    "objectID": "slides/slides-03.html#under-the-hood",
    "href": "slides/slides-03.html#under-the-hood",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Under the hood",
    "text": "Under the hood\n\nCross-validation doesn‚Äôt shuffle the data; it‚Äôs done in train_test_split."
  },
  {
    "objectID": "slides/slides-03.html#our-typical-supervised-learning-set-up-is-as-follows",
    "href": "slides/slides-03.html#our-typical-supervised-learning-set-up-is-as-follows",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Our typical supervised learning set up is as follows:",
    "text": "Our typical supervised learning set up is as follows:\n\n\nWe are given training data with features X and target y\nWe split the data into train and test portions: X_train, y_train, X_test, y_test\nWe carry out hyperparameter optimization using cross-validation on the train portion: X_train and y_train.\nWe assess our best performing model on the test portion: X_test and y_test.\n\nWhat we care about is the test error, which tells us how well our model can be generalized."
  },
  {
    "objectID": "slides/slides-03.html#types-of-errors",
    "href": "slides/slides-03.html#types-of-errors",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Types of errors",
    "text": "Types of errors\nImagine that your train and validation errors do not align with each other. How do you diagnose the problem?\nWe‚Äôre going to think about 4 types of errors:\n\n\\(E_\\textrm{train}\\) is your training error (or mean train error from cross-validation).\n\\(E_\\textrm{valid}\\) is your validation error (or mean validation error from cross-validation).\n\\(E_\\textrm{test}\\) is your test error.\n\\(E_\\textrm{best}\\) is the best possible error you could get for a given problem."
  },
  {
    "objectID": "slides/slides-03.html#underfitting",
    "href": "slides/slides-03.html#underfitting",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Underfitting",
    "text": "Underfitting\n\nmodel = DecisionTreeClassifier(max_depth=1)  # decision stump\nscores = cross_validate(model, X_train, y_train, cv=4, return_train_score=True)\n\n\n\nTrain error:   0.229\nValidation error:   0.438"
  },
  {
    "objectID": "slides/slides-03.html#overfitting",
    "href": "slides/slides-03.html#overfitting",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Overfitting",
    "text": "Overfitting\n\nmodel = DecisionTreeClassifier(max_depth=6)\nscores = cross_validate(model, X_train, y_train, cv=4, return_train_score=True)\n\n\n\nTrain error:   0.000\nValidation error:   0.438"
  },
  {
    "objectID": "slides/slides-03.html#the-fundamental-tradeoff-of-supervised-learning",
    "href": "slides/slides-03.html#the-fundamental-tradeoff-of-supervised-learning",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "The ‚Äúfundamental tradeoff‚Äù of supervised learning:",
    "text": "The ‚Äúfundamental tradeoff‚Äù of supervised learning:\nAs you increase model complexity, \\(E_\\textrm{train}\\) tends to go down but \\(E_\\textrm{valid}-E_\\textrm{train}\\) tends to go up."
  },
  {
    "objectID": "slides/slides-03.html#bias-vs-variance-tradeoff",
    "href": "slides/slides-03.html#bias-vs-variance-tradeoff",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Bias vs variance tradeoff",
    "text": "Bias vs variance tradeoff\n\nThe fundamental trade-off is also called the bias/variance tradeoff in supervised machine learning.\n\n\nBias\n\nthe tendency to consistently learn the same wrong thing (high bias corresponds to underfitting)\n\nVariance\n\nthe tendency to learn random things irrespective of the real signal (high variance corresponds to overfitting)"
  },
  {
    "objectID": "slides/slides-03.html#how-to-pick-a-model-that-would-generalize-better",
    "href": "slides/slides-03.html#how-to-pick-a-model-that-would-generalize-better",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "How to pick a model that would generalize better?",
    "text": "How to pick a model that would generalize better?\n\nWe want to avoid both underfitting and overfitting.\nWe want to be consistent with the training data but we don‚Äôt to rely too much on it.\n\n\nThere are many subtleties here and there is no perfect answer but a common practice is to pick the model with minimum cross-validation error.\nsource"
  },
  {
    "objectID": "slides/slides-03.html#the-golden-rule-1",
    "href": "slides/slides-03.html#the-golden-rule-1",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "The golden rule ",
    "text": "The golden rule \n\nEven though we care the most about test error THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY.\nWe have to be very careful not to violate it while developing our ML pipeline.\nEven experts end up breaking it sometimes which leads to misleading results and lack of generalization on the real data."
  },
  {
    "objectID": "slides/slides-03.html#here-is-the-workflow-well-generally-follow.",
    "href": "slides/slides-03.html#here-is-the-workflow-well-generally-follow.",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Here is the workflow we‚Äôll generally follow.",
    "text": "Here is the workflow we‚Äôll generally follow.\n\n\nSplitting: Before doing anything, split the data X and y into X_train, X_test, y_train, y_test or train_df and test_df using train_test_split.\nSelect the best model using cross-validation: Use cross_validate with return_train_score = True so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.)\nScoring on test data: Finally score on the test data with the chosen hyperparameters to examine the generalization performance.\n\n\n\nAgain, there are many subtleties here we‚Äôll discuss the golden rule multiple times throughout the course."
  },
  {
    "objectID": "slides/slides-03.html#iclicker-3.2",
    "href": "slides/slides-03.html#iclicker-3.2",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "iClicker 3.2",
    "text": "iClicker 3.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-fold cross-validation calls fit \\(k\\) times\n\n\nWe use cross-validation to get a more robust estimate of model performance.\n\n\nIf the mean train accuracy is much higher than the mean cross-validation accuracy it‚Äôs likely to be a case of overfitting.\n\n\nThe fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n\n\nA decision stump on a complicated classification problem is likely to underfit."
  },
  {
    "objectID": "slides/slides-03.html#what-we-learned-today",
    "href": "slides/slides-03.html#what-we-learned-today",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "What we learned today?",
    "text": "What we learned today?\n\nImportance of generalization in supervised machine learning\nData splitting as a way to approximate generalization error\nTrain, test, validation, deployment data\nCross-validation\nOverfitting, underfitting, the fundamental tradeoff, and the golden rule."
  },
  {
    "objectID": "slides/slides-03.html#class-demo-time-permitting",
    "href": "slides/slides-03.html#class-demo-time-permitting",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Class demo (Time permitting)",
    "text": "Class demo (Time permitting)\nCopy this notebook to your working directory and follow along.\nhttps://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_03-ml-fundamentals.ipynb"
  },
  {
    "objectID": "slides/slides-04.html#announcements",
    "href": "slides/slides-04.html#announcements",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Announcements",
    "text": "Announcements\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nHomework 2 due Jan 20\nSyllabus quiz due date Jan 24\nThe lecture notes within these notebooks align with the content presented in the videos. Even though we do not cover all the content from these notebooks during lectures, it‚Äôs your responsibility to go through them on your own."
  },
  {
    "objectID": "slides/slides-04.html#learning-outcomes",
    "href": "slides/slides-04.html#learning-outcomes",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nDescribe the curse of dimensionality\nExplain the notion of similarity-based algorithms\nDescribe how \\(k\\)-NNs and SVMs with RBF kernel work\nDescribe the effects of hyper-parameters for \\(k\\)-NNs and SVMs"
  },
  {
    "objectID": "slides/slides-04.html#recap",
    "href": "slides/slides-04.html#recap",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap",
    "text": "Recap\nWhich of the following scenarios do NOT necessarily imply overfitting?\n\n\nTraining accuracy is 0.98 while validation accuracy is 0.60.\n\n\nThe model is too specific to the training data.\n\n\nThe decision boundary of a classifier is wiggly and highly irregular.\n\n\nTraining and validation accuracies are both approximately 0.88."
  },
  {
    "objectID": "slides/slides-04.html#recap-1",
    "href": "slides/slides-04.html#recap-1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap",
    "text": "Recap\nWhich of the following statements about overfitting is true?\n\n\nOverfitting is always beneficial for model performance on unseen data.\n\n\nSome degree of overfitting is common in most real-world problems.\n\n\nOverfitting ensures the model will perform well in real-world scenarios.\n\n\nOverfitting occurs when the model learns the training data too closely, including its noise and outliers."
  },
  {
    "objectID": "slides/slides-04.html#recap-2",
    "href": "slides/slides-04.html#recap-2",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap",
    "text": "Recap\nHow might one address the issue of underfitting in a machine learning model.\n\n\nIntroduce more noise to the training data.\n\n\nRemove features that might be relevant to the prediction.\n\n\nIncrease the model‚Äôs complexity, possibly by adding more parameter or features\n\n\nUse a smaller dataset for training."
  },
  {
    "objectID": "slides/slides-04.html#overfitting-and-underfitting",
    "href": "slides/slides-04.html#overfitting-and-underfitting",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nAn overfit model matches the training set so closely that it fails to make correct predictions on new unseen data.\n\nAn underfit model is too simple and does not even make good predictions on the training data\n\n\nSource"
  },
  {
    "objectID": "slides/slides-04.html#recap-3",
    "href": "slides/slides-04.html#recap-3",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap",
    "text": "Recap\n\nWhy do we split the data? What are train/valid/test splits?\nWhat are the benefits of cross-validation?\nWhat‚Äôs the fundamental trade-off in supervised machine learning?\nWhat is the golden rule of machine learning?"
  },
  {
    "objectID": "slides/slides-04.html#cross-validation",
    "href": "slides/slides-04.html#cross-validation",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/slides-04.html#summary-of-train-validation-test-and-deployment-data",
    "href": "slides/slides-04.html#summary-of-train-validation-test-and-deployment-data",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Summary of train, validation, test, and deployment data",
    "text": "Summary of train, validation, test, and deployment data\n\n\n\n\nfit\nscore\npredict\n\n\n\n\nTrain\n‚úîÔ∏è\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nValidation\n\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nTest\n\nonce\nonce\n\n\nDeployment\n\n\n‚úîÔ∏è"
  },
  {
    "objectID": "slides/slides-04.html#recap-the-fundamental-tradeoff",
    "href": "slides/slides-04.html#recap-the-fundamental-tradeoff",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: The fundamental tradeoff",
    "text": "Recap: The fundamental tradeoff\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up."
  },
  {
    "objectID": "slides/slides-04.html#motivation",
    "href": "slides/slides-04.html#motivation",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/slides-04.html#general-idea-of-k-nearest-neighbours-algorithm",
    "href": "slides/slides-04.html#general-idea-of-k-nearest-neighbours-algorithm",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "General idea of \\(k\\)-nearest neighbours algorithm",
    "text": "General idea of \\(k\\)-nearest neighbours algorithm"
  },
  {
    "objectID": "slides/slides-04.html#geometric-view-of-tabular-data-and-dimensions",
    "href": "slides/slides-04.html#geometric-view-of-tabular-data-and-dimensions",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Geometric view of tabular data and dimensions",
    "text": "Geometric view of tabular data and dimensions\n\nX_spotify.shape\n\n(2017, 13)\n\n\n\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nduration_ms\nenergy\ninstrumentalness\nkey\nliveness\nloudness\nmode\nspeechiness\ntempo\ntime_signature\nvalence\n\n\n\n\n0\n0.0102\n0.833\n204600\n0.434\n0.021900\n2\n0.1650\n-8.795\n1\n0.4310\n150.062\n4.0\n0.286\n\n\n1\n0.1990\n0.743\n326933\n0.359\n0.006110\n1\n0.1370\n-10.401\n1\n0.0794\n160.083\n4.0\n0.588\n\n\n2\n0.0344\n0.838\n185707\n0.412\n0.000234\n2\n0.1590\n-7.148\n1\n0.2890\n75.044\n4.0\n0.173\n\n\n3\n0.6040\n0.494\n199413\n0.338\n0.510000\n5\n0.0922\n-15.236\n1\n0.0261\n86.468\n4.0\n0.230\n\n\n4\n0.1800\n0.678\n392893\n0.561\n0.512000\n5\n0.4390\n-11.648\n0\n0.0694\n174.004\n4.0\n0.904"
  },
  {
    "objectID": "slides/slides-04.html#distance-between-feature-vectors",
    "href": "slides/slides-04.html#distance-between-feature-vectors",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Distance between feature vectors",
    "text": "Distance between feature vectors\n\nA common way to calculate the distance between vectors is calculating the Euclidean distance.\nThe euclidean distance between vectors \\(u = &lt;u_1, u_2, \\dots, u_n&gt;\\) and \\(v = &lt;v_1, v_2, \\dots, v_n&gt;\\) is defined as:\n\n\\[distance(u, v) = \\sqrt{\\sum_{i =1}^{n} (u_i - v_i)^2}\\]"
  },
  {
    "objectID": "slides/slides-04.html#k-nns-example",
    "href": "slides/slides-04.html#k-nns-example",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-NNs example",
    "text": "\\(k\\)-NNs example"
  },
  {
    "objectID": "slides/slides-04.html#k-nns-n1",
    "href": "slides/slides-04.html#k-nns-n1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-NNs (n=1)",
    "text": "\\(k\\)-NNs (n=1)\n\n\nn_neighbors 1"
  },
  {
    "objectID": "slides/slides-04.html#k-nns-n3",
    "href": "slides/slides-04.html#k-nns-n3",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-NNs (n=3)",
    "text": "\\(k\\)-NNs (n=3)\n\n\nn_neighbors 3"
  },
  {
    "objectID": "slides/slides-04.html#kneighborsclassifier",
    "href": "slides/slides-04.html#kneighborsclassifier",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "KNeighborsClassifier",
    "text": "KNeighborsClassifier\n\nfor k in [1, 3]:\n    neigh = KNeighborsClassifier(n_neighbors=k)\n    neigh.fit(X_small_cities, y_small_cities)\n    print(\n        \"Prediction of the black dot with %d neighbours: %s\"\n        % (k, neigh.predict(test_point))\n    )\n\nPrediction of the black dot with 1 neighbours: ['USA']\nPrediction of the black dot with 3 neighbours: ['Canada']"
  },
  {
    "objectID": "slides/slides-04.html#effect-of-k",
    "href": "slides/slides-04.html#effect-of-k",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Effect of \\(k\\)",
    "text": "Effect of \\(k\\)"
  },
  {
    "objectID": "slides/slides-04.html#iclicker-4.1",
    "href": "slides/slides-04.html#iclicker-4.1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "iClicker 4.1",
    "text": "iClicker 4.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nAnalogy-based models find examples from the test set that are most similar to the query example we are predicting.\n\n\nEuclidean distance will always have a non-negative value.\n\n\nWith \\(k\\)-NN, setting the hyperparameter \\(k\\) to larger values typically reduces training error.\n\n\nSimilar to decision trees, \\(k\\)-NNs finds a small set of good features.\n\n\nIn \\(k\\)-NN, with \\(k &gt; 1\\), the classification of the closest neighbour to the test example always contributes the most to the prediction."
  },
  {
    "objectID": "slides/slides-04.html#regression-with-k-nearest-neighbours-k-nns",
    "href": "slides/slides-04.html#regression-with-k-nearest-neighbours-k-nns",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Regression with \\(k\\)-nearest neighbours (\\(k\\)-NNs)",
    "text": "Regression with \\(k\\)-nearest neighbours (\\(k\\)-NNs)"
  },
  {
    "objectID": "slides/slides-04.html#curse-of-dimensionality",
    "href": "slides/slides-04.html#curse-of-dimensionality",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\n\nAffects all learners but especially bad for nearest-neighbour.\n\\(k\\)-NN usually works well when the number of dimensions \\(d\\) is small but things fall apart quickly as \\(d\\) goes up.\nIf there are many irrelevant attributes, \\(k\\)-NN is hopelessly confused because all of them contribute to finding similarity between examples.\nWith enough irrelevant attributes the accidental similarity swamps out meaningful similarity and \\(k\\)-NN is no better than random guessing."
  },
  {
    "objectID": "slides/slides-04.html#overview",
    "href": "slides/slides-04.html#overview",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Overview",
    "text": "Overview\n\nSVM RBFs are more like weighted \\(k\\)-NNs.\n\nThe decision boundary is defined by a set of positive and negative examples and their weights together with their similarity measure.\nA test example is labeled positive if on average it looks more like positive examples than the negative examples.\n\nDifference between \\(k\\)-NNs and SVM RBFs\n\nSVM RBFs only remember the key examples (support vectors).\nSVMs use a different similarity metric which is called a ‚Äúkernel‚Äù. A popular kernel is Radial Basis Functions (RBFs)\nThey usually perform better than \\(k\\)-NNs!"
  },
  {
    "objectID": "slides/slides-04.html#decision-boundary-of-svms",
    "href": "slides/slides-04.html#decision-boundary-of-svms",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Decision boundary of SVMs",
    "text": "Decision boundary of SVMs"
  },
  {
    "objectID": "slides/slides-04.html#support-vectors",
    "href": "slides/slides-04.html#support-vectors",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Support vectors",
    "text": "Support vectors\nThe decision boundary only depends on the support vectors."
  },
  {
    "objectID": "slides/slides-04.html#relation-of-gamma-and-the-fundamental-trade-off",
    "href": "slides/slides-04.html#relation-of-gamma-and-the-fundamental-trade-off",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Relation of gamma and the fundamental trade-off",
    "text": "Relation of gamma and the fundamental trade-off"
  },
  {
    "objectID": "slides/slides-04.html#relation-of-c-and-the-fundamental-trade-off",
    "href": "slides/slides-04.html#relation-of-c-and-the-fundamental-trade-off",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Relation of C and the fundamental trade-off",
    "text": "Relation of C and the fundamental trade-off"
  },
  {
    "objectID": "slides/slides-04.html#iclicker-4.2",
    "href": "slides/slides-04.html#iclicker-4.2",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "iClicker 4.2",
    "text": "iClicker 4.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-NN may perform poorly in high-dimensional space (say, d &gt; 1000).\n\n\nIn sklearn‚Äôs SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score.\n\n\nIf we increase both gamma and C, we can‚Äôt be certain if the model becomes more complex or less complex."
  },
  {
    "objectID": "slides/slides-04.html#similarity-based-algorithms",
    "href": "slides/slides-04.html#similarity-based-algorithms",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Similarity-based algorithms",
    "text": "Similarity-based algorithms\n\nUse similarity or distance metrics to predict targets.\nExamples: \\(k\\)-nearest neighbors, Support Vector Machines (SVMs) with RBF Kernel."
  },
  {
    "objectID": "slides/slides-04.html#k-nearest-neighbours",
    "href": "slides/slides-04.html#k-nearest-neighbours",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-nearest neighbours",
    "text": "\\(k\\)-nearest neighbours\n\nClassifies an object based on the majority label among its \\(k\\) closest neighbors.\nMain hyperparameter: \\(k\\) or n_neighbors in sklearn\nDistance Metrics: Euclidean\nStrengths: Simple and intuitive, can learn complex decision boundaries\nChallenges: Sensitive to the choice of distance metric and scaling (coming up)."
  },
  {
    "objectID": "slides/slides-04.html#curse-of-dimensionality-1",
    "href": "slides/slides-04.html#curse-of-dimensionality-1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\n\nAs dimensionality increases, the volume of the space increases exponentially, making the data sparse.\nDistance metrics lose meaning\n\nAccidental similarity swamps out meaningful similarity\nAll points become almost equidistant.\n\nOverfitting becomes likely: Harder to generalize with high-dimensional data.\nHow to deal with this?\n\nDimensionality reduction (PCA) (not covered in this course)\nFeature selection techniques."
  },
  {
    "objectID": "slides/slides-04.html#svms-with-rbf-kernel",
    "href": "slides/slides-04.html#svms-with-rbf-kernel",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "SVMs with RBF kernel",
    "text": "SVMs with RBF kernel\n\nRBF Kernel: Radial Basis Function, a way to transform data into higher dimensions implicitly.\nStrengths\n\nEffective in high-dimensional and sparse data\nGood performance on non-linear problems.\n\nHyperparameters:\n\nC$: Regularization parameter (trade-off between correct classification of training examples and maximization of the decision margin).\n\\(\\gamma\\): Defines how far the influence of a single training example reaches."
  },
  {
    "objectID": "slides/slides-04.html#intuition-of-c-and-gamma-in-svm-rbf",
    "href": "slides/slides-04.html#intuition-of-c-and-gamma-in-svm-rbf",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Intuition of C and gamma in SVM RBF",
    "text": "Intuition of C and gamma in SVM RBF\n\nC (Regularization): Controls the trade-off between perfect training accuracy and having a simpler decision boundary.\n\nHigh C: Strict, complex boundary (overfitting risk).\nLow C: More errors allowed, smoother boundary (generalizes better).\n\nGamma (Kernel Width): Controls the influence of individual data points.\n\nHigh Gamma: Points have local impact, complex boundary.\nLow Gamma: Points affect broader areas, smoother boundary.\n\nKey trade-off: Proper balance between C and gamma is crucial for avoiding overfitting or underfitting."
  },
  {
    "objectID": "slides/slides-06.html#announcements",
    "href": "slides/slides-06.html#announcements",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Announcements",
    "text": "Announcements\n\nHW3 is due next week Monday, Feb 3rd, 11:59 pm.\n\nYou can work in pairs for this assignment."
  },
  {
    "objectID": "slides/slides-06.html#learning-outcomes",
    "href": "slides/slides-06.html#learning-outcomes",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand how to use multiple types of features in a pipeline\nIncorporate text features in a machine learning pipeline\nIdentify when it‚Äôs appropriate to apply ordinal encoding vs one-hot encoding\nExplain strategies to deal with categorical variables with too many categories\nExplain why text data needs a different treatment than categorical variables"
  },
  {
    "objectID": "slides/slides-06.html#data",
    "href": "slides/slides-06.html#data",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Data",
    "text": "Data\n\nX, y = make_blobs(n_samples=100, centers=3, random_state=12, cluster_std=5) # make synthetic data\nX_train_toy, X_test_toy, y_train_toy, y_test_toy = train_test_split(\n    X, y, random_state=5, test_size=0.4) # split it into training and test sets\n# Visualize the training data\nplt.scatter(X_train_toy[:, 0], X_train_toy[:, 1], label=\"Training set\", s=60)\nplt.scatter(\n    X_test_toy[:, 0], X_test_toy[:, 1], color=mglearn.cm2(1), label=\"Test set\", s=60\n)\nplt.legend(loc=\"upper right\")"
  },
  {
    "objectID": "slides/slides-06.html#bad-methodology-1",
    "href": "slides/slides-06.html#bad-methodology-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 1",
    "text": "Bad methodology 1\n\nWhat‚Äôs wrong with scaling data separately?\n\n\nscaler = StandardScaler() # Creating a scalert object \nscaler.fit(X_train_toy) # Calling fit on the training data \ntrain_scaled = scaler.transform(\n    X_train_toy\n)  # Transforming the training data using the scaler fit on training data\n\nscaler = StandardScaler()  # Creating a separate object for scaling test data\nscaler.fit(X_test_toy)  # Calling fit on the test data\ntest_scaled = scaler.transform(\n    X_test_toy\n)  # Transforming the test data using the scaler fit on test data\n\nknn = KNeighborsClassifier()\nknn.fit(train_scaled, y_train_toy)\nprint(f\"Training score: {knn.score(train_scaled, y_train_toy):.2f}\")\nprint(f\"Test score: {knn.score(test_scaled, y_test_toy):.2f}\") # misleading scores\n\nTraining score: 0.63\nTest score: 0.60"
  },
  {
    "objectID": "slides/slides-06.html#scaling-train-and-test-data-separately",
    "href": "slides/slides-06.html#scaling-train-and-test-data-separately",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Scaling train and test data separately",
    "text": "Scaling train and test data separately"
  },
  {
    "objectID": "slides/slides-06.html#bad-methodology-2",
    "href": "slides/slides-06.html#bad-methodology-2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 2",
    "text": "Bad methodology 2\n\nWhat‚Äôs wrong with scaling the data together\n\n\n# join the train and test sets back together\nXX = np.vstack((X_train_toy, X_test_toy))\n\nscaler = StandardScaler()\nscaler.fit(XX)\nXX_scaled = scaler.transform(XX)\n\nXX_train = XX_scaled[:X_train_toy.shape[0]]\nXX_test = XX_scaled[X_train_toy.shape[0]:]\n\nknn = KNeighborsClassifier()\nknn.fit(XX_train, y_train_toy)\nprint(f\"Training score: {knn.score(XX_train, y_train_toy):.2f}\")  # Misleading score\nprint(f\"Test score: {knn.score(XX_test, y_test_toy):.2f}\")  # Misleading score\n\nTraining score: 0.63\nTest score: 0.55"
  },
  {
    "objectID": "slides/slides-06.html#bad-methodology-3",
    "href": "slides/slides-06.html#bad-methodology-3",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 3",
    "text": "Bad methodology 3\n\nWhat‚Äôs wrong here?\n\n\nknn = KNeighborsClassifier()\n\nscaler = StandardScaler()\nscaler.fit(X_train_toy)\nX_train_scaled = scaler.transform(X_train_toy)\nX_test_scaled = scaler.transform(X_test_toy)\ncross_val_score(knn, X_train_scaled, y_train_toy)\n\narray([0.25      , 0.5       , 0.58333333, 0.58333333, 0.41666667])"
  },
  {
    "objectID": "slides/slides-06.html#improper-preprocessing",
    "href": "slides/slides-06.html#improper-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Improper preprocessing",
    "text": "Improper preprocessing"
  },
  {
    "objectID": "slides/slides-06.html#proper-preprocessing",
    "href": "slides/slides-06.html#proper-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Proper preprocessing",
    "text": "Proper preprocessing"
  },
  {
    "objectID": "slides/slides-06.html#recap-sklearn-pipelines",
    "href": "slides/slides-06.html#recap-sklearn-pipelines",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Recap: sklearn Pipelines",
    "text": "Recap: sklearn Pipelines\n\nPipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\nSimplify the code and improves readability.\nReduce the risk of data leakage by ensuring proper transformation of the training and test sets.\nAutomatically apply transformations in sequence.\nExample:\n\nChaining a StandardScaler with a KNeighborsClassifier model.\n\n\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\n# Correct way to do cross validation without breaking the golden rule. \ncross_val_score(pipe_knn, X_train_toy, y_train_toy) \n\narray([0.25      , 0.5       , 0.5       , 0.58333333, 0.41666667])"
  },
  {
    "objectID": "slides/slides-06.html#sklearns-columntransformer",
    "href": "slides/slides-06.html#sklearns-columntransformer",
    "title": "Lecture 6: Column transformer and text features",
    "section": "sklearn‚Äôs ColumnTransformer",
    "text": "sklearn‚Äôs ColumnTransformer\n\nUse ColumnTransformer to build all our transformations together into one object\n\n\n\nUse a column transformer with sklearn pipelines."
  },
  {
    "objectID": "slides/slides-06.html#iclicker-exercise-6.1",
    "href": "slides/slides-06.html#iclicker-exercise-6.1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "(iClicker) Exercise 6.1",
    "text": "(iClicker) Exercise 6.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nYou could carry out cross-validation by passing a ColumnTransformer object to cross_validate.\n\n\nAfter applying column transformer, the order of the columns in the transformed data has to be the same as the order of the columns in the original data.\n\n\nAfter applying a column transformer, the transformed data is always going to be of different shape than the original data.\n\n\nWhen you call fit_transform on a ColumnTransformer object, you get a numpy ndarray."
  },
  {
    "objectID": "slides/slides-06.html#dealing-with-text-features",
    "href": "slides/slides-06.html#dealing-with-text-features",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Dealing with text features",
    "text": "Dealing with text features\n\nPreprocessing text to fit into machine learning models using text vectorization.\nBag of words representation\nIgnores the syntax and word order!"
  },
  {
    "objectID": "slides/slides-06.html#sklearn-countvectorizer",
    "href": "slides/slides-06.html#sklearn-countvectorizer",
    "title": "Lecture 6: Column transformer and text features",
    "section": "sklearn CountVectorizer",
    "text": "sklearn CountVectorizer\n\nUse scikit-learn‚Äôs CountVectorizer to encode text data\nCountVectorizer: Transforms text into a matrix of token counts\nImportant parameters:\n\nmax_features: Control the number of features used in the model\nmax_df, min_df: Control document frequency thresholds\nngram_range: Defines the range of n-grams to be extracted\nstop_words: Enables the removal of common words that are typically uninformative in most applications, such as ‚Äúand‚Äù, ‚Äúthe‚Äù, etc."
  },
  {
    "objectID": "slides/slides-06.html#incorporating-text-features-in-a-machine-learning-pipeline",
    "href": "slides/slides-06.html#incorporating-text-features-in-a-machine-learning-pipeline",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Incorporating text features in a machine learning pipeline",
    "text": "Incorporating text features in a machine learning pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipeline = make_pipeline(\n    CountVectorizer(),\n    SVC()\n)"
  },
  {
    "objectID": "slides/slides-06.html#remarks-on-preprocessing",
    "href": "slides/slides-06.html#remarks-on-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Remarks on preprocessing",
    "text": "Remarks on preprocessing\n\nThere is no one-size-fits-all solution in data preprocessing, and decisions often involve a degree of subjectivity.\n\nExploratory data analysis and domain knowledge inform these decisions\n\nAlways consider the specific goals of your project when deciding how to encode features."
  },
  {
    "objectID": "slides/slides-06.html#alternative-methods-for-scaling",
    "href": "slides/slides-06.html#alternative-methods-for-scaling",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Alternative methods for scaling",
    "text": "Alternative methods for scaling\n\nStandardScaler\n\nGood choice when the column follows a normal distribution or a distribution somewhat like a normal distribution.\n\nMinMaxScaler: Transform each feature to a desired range. Appropriate when\n\nGood choice for features such as human age, where there is a fixed range of values and the feature is uniformly distributed across the range\n\nNormalizer: Works on rows rather than columns. Normalize examples individually to unit norm.\n\nGood choice for frequency-type data\n\nLog scaling\n\nGood choice for features such as ratings per movies (power law distribution; a few movies have lots of ratings but most movies have very few ratings)\n\n‚Ä¶"
  },
  {
    "objectID": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding",
    "href": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nOrdinal Encoding: Encodes categorical features as an integer array.\nOne-hot Encoding: Creates binary columns for each category‚Äôs presence.\nSometimes how we encode a specific feature depends upon the context."
  },
  {
    "objectID": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding-1",
    "href": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nConsider weather feature and its four categories: Sunny (‚òÄÔ∏è), Cloudy (üå•Ô∏è), Rainy (‚õàÔ∏è), Snowy (‚ùÑÔ∏è)\nWhich encoding would you use in each of the following scenarios?\n\nPredicting traffic volume\nPredicting severity of weather-related road incidents"
  },
  {
    "objectID": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding-2",
    "href": "slides/slides-06.html#ordinal-encoding-vs.-one-hot-encoding-2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nConsider weather feature and its four categories: Sunny (‚òÄÔ∏è), Cloudy (üå•Ô∏è), Rainy (‚õàÔ∏è), Snowy (‚ùÑÔ∏è)\nPredicting traffic volume: Using one-hot encoding would make sense here because the impact of different weather conditions on traffic volume does not necessarily follow a clear order and different weather conditions could have very distinct effects.\nPredicting severity of weather-related road incidents: An ordinal encoding might be more appropriate if you define your weather categories from least to most severe as this could correlate directly with the likelihood or severity of incidents."
  },
  {
    "objectID": "slides/slides-06.html#handle_unknown-ignore-of-onehotencoder",
    "href": "slides/slides-06.html#handle_unknown-ignore-of-onehotencoder",
    "title": "Lecture 6: Column transformer and text features",
    "section": "handle_unknown = \"ignore\" of OneHotEncoder",
    "text": "handle_unknown = \"ignore\" of OneHotEncoder\n\nUse handle_unknown='ignore' with OneHotEncoder to safely ignore unseen categories during transform.\nIn each of the following scenarios, identify whether it‚Äôs a reasonable strategy or not.\n\nExample 1: Suppose you are building a model to predict customer behavior (e.g., purchase likelihood) based on features like location, device_type, and product_category. During training, you have observed a set of categories for product_category, but in the future, new product categories might be added.\nExample 2: You‚Äôre building a model to predict disease diagnosis based on symptoms, where each symptom is categorized (e.g., fever, headache, nausea)."
  },
  {
    "objectID": "slides/slides-06.html#handle_unknown-ignore-of-onehotencoder-1",
    "href": "slides/slides-06.html#handle_unknown-ignore-of-onehotencoder-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "handle_unknown = \"ignore\" of OneHotEncoder",
    "text": "handle_unknown = \"ignore\" of OneHotEncoder\n\nReasonable use: When unseen categories are less likely to impact the model‚Äôs prediction accuracy (e.g., product categories in e-commerce), and you prefer to avoid breaking the model.\nNot-so-reasonable use: When unseen categories could provide critical new information that could significantly alter predictions (e.g., in medical diagnostics), ignoring them could result in a poor or dangerous outcome."
  },
  {
    "objectID": "slides/slides-06.html#dropif_binary-argument-of-onehotencoder",
    "href": "slides/slides-06.html#dropif_binary-argument-of-onehotencoder",
    "title": "Lecture 6: Column transformer and text features",
    "section": "drop=\"if_binary\" argument of OneHotEncoder",
    "text": "drop=\"if_binary\" argument of OneHotEncoder\n\ndrop=‚Äòif_binary‚Äô argument in OneHotEncoder:\nReduces redundancy by dropping one of the columns if the feature is binary."
  },
  {
    "objectID": "slides/slides-06.html#categorical-variables-with-too-many-categories",
    "href": "slides/slides-06.html#categorical-variables-with-too-many-categories",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Categorical variables with too many categories",
    "text": "Categorical variables with too many categories\n\nStrategies for categorical variables with too many categories:\n\nDimensionality reduction techniques\nBucketing categories into ‚Äòothers‚Äô\nClustering or grouping categories manually\nOnly considering top-N categories\n‚Ä¶"
  },
  {
    "objectID": "slides/slides-06.html#iclicker-exercise-6.2",
    "href": "slides/slides-06.html#iclicker-exercise-6.2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "(iClicker) Exercise 6.2",
    "text": "(iClicker) Exercise 6.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nhandle_unknown=\"ignore\" would treat all unknown categories equally.\n\n\nAs you increase the value for max_features hyperparameter of CountVectorizer the training score is likely to go up.\n\n\nSuppose you are encoding text data using CountVectorizer. If you encounter a word in the validation or the test split that‚Äôs not available in the training data, we‚Äôll get an error.\n\n\nIn the code below, inside cross_validate, each fold might have slightly different number of features (columns) in the fold.\n\n\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)"
  },
  {
    "objectID": "slides/slides-06.html#summary-of-lectures-5-and-6",
    "href": "slides/slides-06.html#summary-of-lectures-5-and-6",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Summary of lectures 5 and 6",
    "text": "Summary of lectures 5 and 6\n\nMotivation for preprocessing\nCommon preprocessing steps\n\nImputation\nScaling\nOne-hot encoding\n\nGolden rule in the context of preprocessing\nBuilding simple supervised machine learning pipelines using sklearn.pipeline.make_pipeline.\nDefining transformers with multiple transformations ColumnTransformer\nDealing with text features\n\nBag of words representation: CountVectorizer"
  },
  {
    "objectID": "slides/slides-07.html#announcements",
    "href": "slides/slides-07.html#announcements",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/m4ujp0s4xgm5o5/post/204\n\nHW3 is due next week Monday, Feb 3rd, 11:59 pm.\n\nYou can work in pairs for this assignment.\n\n\nWhere to find slides?\n\nhttps://aroth85.github.io/cpsc330-slides/lecture.html"
  },
  {
    "objectID": "slides/slides-07.html#learning-outcomes",
    "href": "slides/slides-07.html#learning-outcomes",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, students are expected to be able to:\n\nExplain the advantages of getting probability scores instead of hard predictions during classification\nExplain the general intuition behind linear models\nExplain the difference between linear regression and logistic regression\n\nExplain how predict works for linear regression\nExplain how can you interpret model predictions using coefficients learned by a linear model\nExplain the advantages and limitations of linear classifiers"
  },
  {
    "objectID": "slides/slides-07.html#learning-outcomes-contd",
    "href": "slides/slides-07.html#learning-outcomes-contd",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Learning outcomes (contd)",
    "text": "Learning outcomes (contd)\n\nBroadly describe linear SVMs\nDemonstrate how the alpha hyperparameter of Ridge is related to the fundamental tradeoff\nUse scikit-learn‚Äôs Ridge model\nUse scikit-learn‚Äôs LogisticRegression model and predict_proba to get probability scores"
  },
  {
    "objectID": "slides/slides-07.html#recap-dealing-with-text-features",
    "href": "slides/slides-07.html#recap-dealing-with-text-features",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Recap: Dealing with text features",
    "text": "Recap: Dealing with text features\n\nPreprocessing text to fit into machine learning models using text vectorization.\nBag of words representation"
  },
  {
    "objectID": "slides/slides-07.html#recap-sklearn-countvectorizer",
    "href": "slides/slides-07.html#recap-sklearn-countvectorizer",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Recap: sklearn CountVectorizer",
    "text": "Recap: sklearn CountVectorizer\n\nUse scikit-learn‚Äôs CountVectorizer to encode text data\nCountVectorizer: Transforms text into a matrix of token counts\nImportant parameters:\n\nmax_features: Control the number of features used in the model\nmax_df, min_df: Control document frequency thresholds\nngram_range: Defines the range of n-grams to be extracted\nstop_words: Enables the removal of common words that are typically uninformative in most applications, such as ‚Äúand‚Äù, ‚Äúthe‚Äù, etc."
  },
  {
    "objectID": "slides/slides-07.html#recap-incorporating-text-features-in-a-machine-learning-pipeline",
    "href": "slides/slides-07.html#recap-incorporating-text-features-in-a-machine-learning-pipeline",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Recap: Incorporating text features in a machine learning pipeline",
    "text": "Recap: Incorporating text features in a machine learning pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipeline = make_pipeline(\n    CountVectorizer(),\n    SVC()\n)"
  },
  {
    "objectID": "slides/slides-07.html#iclicker-exercise-6.1",
    "href": "slides/slides-07.html#iclicker-exercise-6.1",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "(iClicker) Exercise 6.1",
    "text": "(iClicker) Exercise 6.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nYou could carry out cross-validation by passing a ColumnTransformer object to cross_validate.\n\n\nAfter applying column transformer, the order of the columns in the transformed data has to be the same as the order of the columns in the original data.\n\n\nAfter applying a column transformer, the transformed data is always going to be of different shape than the original data.\n\n\nWhen you call fit_transform on a ColumnTransformer object, you get a numpy ndarray."
  },
  {
    "objectID": "slides/slides-07.html#iclicker-exercise-6.2",
    "href": "slides/slides-07.html#iclicker-exercise-6.2",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "(iClicker) Exercise 6.2",
    "text": "(iClicker) Exercise 6.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nhandle_unknown=\"ignore\" would treat all unknown categories equally.\n\n\nAs you increase the value for max_features hyperparameter of CountVectorizer the training score is likely to go up.\n\n\nSuppose you are encoding text data using CountVectorizer. If you encounter a word in the validation or the test split that‚Äôs not available in the training data, we‚Äôll get an error.\n\n\nIn the code below, inside cross_validate, each fold might have slightly different number of features (columns) in the fold.\n\n\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)"
  },
  {
    "objectID": "slides/slides-07.html#linear-models-1",
    "href": "slides/slides-07.html#linear-models-1",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Linear models",
    "text": "Linear models\n\n\n\nLinear models make an assumption that the relationship between X and y is linear.\nIn this case, with only one feature, our model is a straight line.\nWhat do we need to represent a line?\n\nSlope (\\(w_1\\)): Determines the angle of the line.\nY-intercept (\\(w_0\\)): Where the line crosses the y-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking predictions\n\n\\[ y_{hat} = w_1 \\times \\text{# hours studied} + w_0\\]"
  },
  {
    "objectID": "slides/slides-07.html#ridge-vs.-linearregression",
    "href": "slides/slides-07.html#ridge-vs.-linearregression",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Ridge vs.¬†LinearRegression",
    "text": "Ridge vs.¬†LinearRegression\n\nOrdinary linear regression is sensitive to multicolinearity and overfitting\nMulticolinearity: Overlapping and redundant features. Most of the real-world datasets have colinear features.\n\nLinear regression may produce large and unstable coefficients in such cases.\nRidge adds a parameter to control the complexity of a model. Finds a line that balances fit and prevents overly large coefficients."
  },
  {
    "objectID": "slides/slides-07.html#when-to-use-what",
    "href": "slides/slides-07.html#when-to-use-what",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "When to use what?",
    "text": "When to use what?\n\nLinearRegression\n\nWhen interpretability is key, and no multicollinearity exists\n\nRidge\n\nWhen you have multicollinearity (highly correlated features).\nWhen you want to prevent overfitting in linear models.\n\nIn this course, we‚Äôll use Ridge."
  },
  {
    "objectID": "slides/slides-07.html#iclicker-exercise-7.1",
    "href": "slides/slides-07.html#iclicker-exercise-7.1",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "(iClicker) Exercise 7.1",
    "text": "(iClicker) Exercise 7.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nIncreasing the hyperparameter alpha of Ridge is likely to decrease model complexity.\n\n\nRidge can be used with datasets that have multiple features.\n\n\nWith Ridge, we learn one coefficient per training example.\n\n\nIf you train a linear regression model on a 2-dimensional problem (2 features), the model will learn 3 parameters: one for each feature and one for the bias term."
  },
  {
    "objectID": "slides/slides-07.html#logistic-regression",
    "href": "slides/slides-07.html#logistic-regression",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nSuppose your target is binary: pass or fail\nLogistic regression is used for such binary classification tasks.\n\nLogistic regression predicts a probability that the given example belongs to a particular class.\nIt uses Sigmoid function to map any real-valued input into a value between 0 and 1, representing the probability of a specific outcome.\nA threshold (usually 0.5) is applied to the predicted probability to decide the final class label."
  },
  {
    "objectID": "slides/slides-07.html#logistic-regression-decision-boundary",
    "href": "slides/slides-07.html#logistic-regression-decision-boundary",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Logistic regression: Decision boundary",
    "text": "Logistic regression: Decision boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe decision boundary is the point on the x-axis where the corresponding predicted probability on the y-axis is 0.5."
  },
  {
    "objectID": "slides/slides-07.html#decision-boundary-of-logistic-regression",
    "href": "slides/slides-07.html#decision-boundary-of-logistic-regression",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Decision boundary of logistic regression",
    "text": "Decision boundary of logistic regression"
  },
  {
    "objectID": "slides/slides-07.html#parametric-vs.-non-parametric-models-high-level",
    "href": "slides/slides-07.html#parametric-vs.-non-parametric-models-high-level",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Parametric vs.¬†non-Parametric models (high-level)",
    "text": "Parametric vs.¬†non-Parametric models (high-level)\n\nImagine you are training a logistic regression model. For each of the following scenarios, identify how many parameters (weights and biases) will be learned.\nScenario 1: 100 features and 1,000 examples\nScenario 2: 100 features and 1 million examples"
  },
  {
    "objectID": "slides/slides-07.html#parametric-vs.-non-parametric-models-high-level-1",
    "href": "slides/slides-07.html#parametric-vs.-non-parametric-models-high-level-1",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Parametric vs.¬†non-Parametric models (high-level)",
    "text": "Parametric vs.¬†non-Parametric models (high-level)\n\n\nParametric\n\nExamples: Logistic regression, linear regression, linear SVM\n\nModels with a fixed number of parameters, regardless of the dataset size\nSimple, computationally efficient, less prone to overfitting\nLess flexible, may not capture complex relationships\n\n\nNon parametric\n\nExamples: KNN, SVM RBF, Decision tree with no specific depth specified\nModels where the number of parameters grows with the dataset size. They do not assume a fixed form for the functions being learned.\nFlexible, can adapt to complex patterns\nComputationally expensive, risk of overfitting with noisy data"
  },
  {
    "objectID": "slides/slides-07.html#iclicker-exercise-7.2",
    "href": "slides/slides-07.html#iclicker-exercise-7.2",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "(iClicker) Exercise 7.2",
    "text": "(iClicker) Exercise 7.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nIncreasing logistic regression‚Äôs C hyperparameter increases model complexity.\n\n\nThe raw output score can be used to calculate the probability score for a given prediction.\n\n\nFor linear classifier trained on \\(d\\) features, the decision boundary is a \\(d-1\\)-dimensional hyperparlane.\n\n\nA linear model is likely to be uncertain about the data points close to the decision boundary."
  },
  {
    "objectID": "slides/slides-07.html#linear-svm",
    "href": "slides/slides-07.html#linear-svm",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Linear SVM",
    "text": "Linear SVM\n\nWe have seen non-linear SVM with RBF kernel before. This is the default SVC model in sklearn because it tends to work better in many cases.\nThere is also a linear SVM. You can pass kernel=\"linear\" to create a linear SVM.\npredict method of linear SVM and logistic regression works the same way.\nWe can get coef_ associated with the features and intercept_ using a Linear SVM model"
  },
  {
    "objectID": "slides/slides-07.html#summary",
    "href": "slides/slides-07.html#summary",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/slides-07.html#summary-of-linear-models",
    "href": "slides/slides-07.html#summary-of-linear-models",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Summary of linear models",
    "text": "Summary of linear models\n\nLinear regression is a linear model for regression whereas logistic regression is a linear model for classification.\nBoth these models learn one coefficient per feature, plus an intercept."
  },
  {
    "objectID": "slides/slides-07.html#interpretation-of-coefficients-in-linear-models",
    "href": "slides/slides-07.html#interpretation-of-coefficients-in-linear-models",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Interpretation of coefficients in linear models",
    "text": "Interpretation of coefficients in linear models\n\nThe \\(j\\)th coefficient tells us how feature \\(j\\) affects the prediction\n\nif \\(w_j &gt; 0\\) then increasing \\(x_{ij}\\) moves us toward predicting \\(+1\\)\nif \\(w_j &lt; 0\\) then increasing \\(x_{ij}\\) moves us toward prediction \\(-1\\)\nif \\(w_j = 0\\) then the feature is not used in making a prediction"
  },
  {
    "objectID": "slides/slides-07.html#importance-of-scaling",
    "href": "slides/slides-07.html#importance-of-scaling",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Importance of scaling",
    "text": "Importance of scaling\n\nWhen you are interpreting the model coefficients, scaling is crucial.\nIf you do not scale the data, features with smaller magnitude are going to get coefficients with bigger magnitude whereas features with bigger scale are going to get coefficients with smaller magnitude.\nThat said, when you scale the data, feature values become hard to interpret for humans!"
  },
  {
    "objectID": "slides/slides-07.html#limitations-of-linear-models",
    "href": "slides/slides-07.html#limitations-of-linear-models",
    "title": "CPSC 330 Lecture 7: Linear models",
    "section": "Limitations of linear models",
    "text": "Limitations of linear models\n\nIs your data ‚Äúlinearly separable‚Äù? Can you draw a hyperplane between these datapoints that separates them with 0 error.\nIf the training examples can be separated by a linear decision rule, they are linearly separable."
  },
  {
    "objectID": "slides/slides-09.html#announcements",
    "href": "slides/slides-09.html#announcements",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/m4ujp0s4xgm5o5/post/204\n\nHW4 has been released. Due next week Monday.\nRemember to attend tutorials!\n\nThis weeks tutorial will cover data imbalance and fairness"
  },
  {
    "objectID": "slides/slides-09.html#learning-outcomes",
    "href": "slides/slides-09.html#learning-outcomes",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand why different classification metrics are necessary\nUnderstand when to use different metrics"
  },
  {
    "objectID": "slides/slides-09.html#ml-workflow",
    "href": "slides/slides-09.html#ml-workflow",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ML workflow",
    "text": "ML workflow"
  },
  {
    "objectID": "slides/slides-09.html#accuracy",
    "href": "slides/slides-09.html#accuracy",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Accuracy",
    "text": "Accuracy\n\nSo far we have been measuring classification model performance using Accuracy.\nAccuracy is the proportion of all classifications that were correct, whether positive or negative. \\[Accuracy = \\frac{\\text{correct classifications}}{\\text{total classifications}}\\]\nHowever, in many real-world applications, the dataset is imbalanced or one kind of mistake is more costly than the other\nIn such cases, it‚Äôs better to optimize for one of the other metrics instead."
  },
  {
    "objectID": "slides/slides-09.html#fraud-confusion-matrix",
    "href": "slides/slides-09.html#fraud-confusion-matrix",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Fraud Confusion matrix",
    "text": "Fraud Confusion matrix\n\nWhich types of errors would be most critical for the bank to address?"
  },
  {
    "objectID": "slides/slides-09.html#fraud-confusion-matrix-1",
    "href": "slides/slides-09.html#fraud-confusion-matrix-1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Fraud Confusion matrix",
    "text": "Fraud Confusion matrix\n\n\n\n\n\nTN \\(\\rightarrow\\) True negatives\nFP \\(\\rightarrow\\) False positives\nFN \\(\\rightarrow\\) False negatives\nTP \\(\\rightarrow\\) True positives"
  },
  {
    "objectID": "slides/slides-09.html#confusion-matrix-questions",
    "href": "slides/slides-09.html#confusion-matrix-questions",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nImagine a spam filter model where emails classified as spam are labeled 1 and non-spam emails are labeled 0. If a spam email is incorrectly classified as non-spam, what is this error called?\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09.html#confusion-matrix-questions-1",
    "href": "slides/slides-09.html#confusion-matrix-questions-1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nIn an intrusion detection system, intrusions are identified as 1 and non-intrusive activities as 0. If the system fails to identify an actual intrusion, wrongly categorizing it as non-intrusive, what is this type of error called?\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09.html#confusion-matrix-questions-2",
    "href": "slides/slides-09.html#confusion-matrix-questions-2",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nIn a medical test for a disease, diseased states are labeled as 1 and healthy states as 0. If a healthy patient is incorrectly diagnosed with the disease, what is this error known as?\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09.html#precision-and-recall",
    "href": "slides/slides-09.html#precision-and-recall",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Precision and Recall",
    "text": "Precision and Recall"
  },
  {
    "objectID": "slides/slides-09.html#f1-score",
    "href": "slides/slides-09.html#f1-score",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "F1-score",
    "text": "F1-score\n\nF1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance.\nF1-score is a harmonic mean of precision and recall.\n\n\\[ F1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}\\]"
  },
  {
    "objectID": "slides/slides-09.html#iclicker-exercise-9.1",
    "href": "slides/slides-09.html#iclicker-exercise-9.1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "iClicker Exercise 9.1",
    "text": "iClicker Exercise 9.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nIn medical diagnosis, false positives are more damaging than false negatives (assume ‚Äúpositive‚Äù means the person has a disease, ‚Äúnegative‚Äù means they don‚Äôt).\n\n\nIn spam classification, false positives are more damaging than false negatives (assume ‚Äúpositive‚Äù means the email is spam, ‚Äúnegative‚Äù means they it‚Äôs not).\n\n\nIf method A gets a higher accuracy than method B, that means its precision is also higher.\n\n\nIf method A gets a higher accuracy than method B, that means its recall is also higher."
  },
  {
    "objectID": "slides/slides-09.html#counter-examples",
    "href": "slides/slides-09.html#counter-examples",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Counter examples",
    "text": "Counter examples\nMethod A - higher accuracy but lower precision\n\n\n\nNegative\nPositive\n\n\n\n\n90\n5\n\n\n5\n0\n\n\n\nMethod B - lower accuracy but higher precision\n\n\n\nNegative\nPositive\n\n\n\n\n80\n15\n\n\n0\n5"
  },
  {
    "objectID": "slides/slides-09.html#thresholding",
    "href": "slides/slides-09.html#thresholding",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Thresholding",
    "text": "Thresholding\n\nThe above metrics assume a fixed threshold.\nWe use thresholding to get the binary prediction.\nA typical threshold is 0.5.\n\nA prediction of 0.90 \\(\\rightarrow\\) a high likelihood that the transaction is fraudulent and we predict fraud\nA prediction of 0.20 \\(\\rightarrow\\) a low likelihood that the transaction is non-fraudulent and we predict Non fraud\n\nWhat happens if the predicted score is equal to the chosen threshold?\nPlay with classification thresholds"
  },
  {
    "objectID": "slides/slides-09.html#iclicker-exercise-9.2",
    "href": "slides/slides-09.html#iclicker-exercise-9.2",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "iClicker Exercise 9.2",
    "text": "iClicker Exercise 9.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nIf we increase the classification threshold, both true and false positives are likely to decrease.\n\n\nIf we increase the classification threshold, both true and false negatives are likely to decrease.\n\n\nLowering the classification threshold generally increases the model‚Äôs recall.\n\n\n\nRaising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives."
  },
  {
    "objectID": "slides/slides-09.html#pr-curve",
    "href": "slides/slides-09.html#pr-curve",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "PR curve",
    "text": "PR curve\n\nCalculate precision and recall (TPR) at every possible threshold and graph them.\nBetter choice for highly imbalanced datasets"
  },
  {
    "objectID": "slides/slides-09.html#roc-curve",
    "href": "slides/slides-09.html#roc-curve",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ROC curve",
    "text": "ROC curve\n\nCalculate the true positive rate (TPR) and false positive rate (FPR) at every possible thresholding and graph TPR over FPR.\nGood choice when the datasets are roughly balanced."
  },
  {
    "objectID": "slides/slides-09.html#auc",
    "href": "slides/slides-09.html#auc",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "AUC",
    "text": "AUC\n\nThe area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative."
  },
  {
    "objectID": "slides/slides-09.html#roc-auc-questions",
    "href": "slides/slides-09.html#roc-auc-questions",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ROC AUC questions",
    "text": "ROC AUC questions\nConsider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?\n\n\n\n\n\n\nIf false positives (false alarms) are highly costly\n\n\nIf false positives are cheap and false negatives (missed true positives) highly costly\n\n\nIf the costs are roughly equivalent"
  },
  {
    "objectID": "slides/slides-09.html#questions-for-you",
    "href": "slides/slides-09.html#questions-for-you",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\n\n\n\n\nWhat‚Äôs the AUC of a baseline model?\n\n\nSource"
  },
  {
    "objectID": "slides/slides-09.html#questions-for-you-1",
    "href": "slides/slides-09.html#questions-for-you-1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\nWhat‚Äôs the difference between the average precision (AP) score and F1-score?\nWhich model would you pick?\n\n\nSource"
  },
  {
    "objectID": "slides/slides-09.html#questions-for-you-2",
    "href": "slides/slides-09.html#questions-for-you-2",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\n\n\n\n\nWhich model would you pick?"
  },
  {
    "objectID": "slides/slides-09.html#dealing-with-class-imbalance",
    "href": "slides/slides-09.html#dealing-with-class-imbalance",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Dealing with class imbalance",
    "text": "Dealing with class imbalance\n\nUnder sampling\nOversampling\nclass weight=\"balanced\" (preferred method for this course)\nSMOTE"
  },
  {
    "objectID": "slides/slides-09.html#what-did-we-learn-today",
    "href": "slides/slides-09.html#what-did-we-learn-today",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "What did we learn today?",
    "text": "What did we learn today?\n\nA number of possible ways to evaluate machine learning classification models\nWhat precision, recall, confusion matrix etc. are\nHow thresholds can change performance\nThe difference between metrics at a single threshold (f1) and metrics for all thresholds (AP)"
  },
  {
    "objectID": "lecture-10.html",
    "href": "lecture-10.html",
    "title": "Lecture 10: Regression metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-10.html#slides",
    "href": "lecture-10.html#slides",
    "title": "Lecture 10: Regression metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-10.html#outline",
    "href": "lecture-10.html#outline",
    "title": "Lecture 10: Regression metrics",
    "section": "Outline",
    "text": "Outline\n\nRecap: Classification metrics\nRidge and RidgeCV.\nalpha hyperparameter of Ridge\nMSE, RMSE, MAPE\nApply log-transform on the target values in a regression problem with TransformedTargetRegressor",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-08.html",
    "href": "lecture-08.html",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-08.html#slides",
    "href": "lecture-08.html#slides",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-08.html#outline",
    "href": "lecture-08.html#outline",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "Outline",
    "text": "Outline\n\nWhy hyperparameter optimization?\nHyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV\nOptimization bias",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-06.html",
    "href": "lecture-06.html",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-06.html#slides",
    "href": "lecture-06.html#slides",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-06.html#outline",
    "href": "lecture-06.html#outline",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "Outline",
    "text": "Outline\n\nRecap\nColumn transformer\nArguments of one-hot encoder\nsklearn CountVectorizer\nincorporating text features in the pipeline",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-04.html",
    "href": "lecture-04.html",
    "title": "Lecture 4: k-nearest neighbours and SVM RBFs",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 4: k-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-04.html#slides",
    "href": "lecture-04.html#slides",
    "title": "Lecture 4: k-nearest neighbours and SVM RBFs",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 4: k-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-04.html#outline",
    "href": "lecture-04.html#outline",
    "title": "Lecture 4: k-nearest neighbours and SVM RBFs",
    "section": "Outline",
    "text": "Outline\n\nRecap\nSimilarity-based algorithms\n\\(k\\)-nearest neighbours intuitions\nCurse of dimensionality\nSVM with RBF kernel intuition\nInterpretation of C and gamma hyperparameters of SVM RBF",
    "crumbs": [
      "Lectures",
      "Lecture 4: k-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-02.html",
    "href": "lecture-02.html",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "lecture-02.html#slides",
    "href": "lecture-02.html#slides",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "lecture-02.html#outline",
    "href": "lecture-02.html#outline",
    "title": "Lecture 2: Terminology, baselines, decision trees",
    "section": "Outline",
    "text": "Outline\n\nFeatures, target, examples, training\nParameters and hyperparameters\nDecision boundary\nClassification vs.¬†regression\nInference vs.¬†prediction\nAccuracy vs.¬†error, baselines\nIntuition of decision trees",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision trees"
    ]
  },
  {
    "objectID": "slides/slides-11.html#announcements",
    "href": "slides/slides-11.html#announcements",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Announcements",
    "text": "Announcements\n\nMidterm 1 this week!\nImportant information about midterm 1\n\nhttps://piazza.com/class/m4ujp0s4xgm5o5/post/204\nGood news for you: You‚Äôll have access to our course notes in the midterm!\n\nPractice midterm questions available on PL\n\nYou can see an example of all the info you will have access to in the last question."
  },
  {
    "objectID": "slides/slides-11.html#iclicker-exercise-10.1",
    "href": "slides/slides-11.html#iclicker-exercise-10.1",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "iClicker Exercise 10.1",
    "text": "iClicker Exercise 10.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nPrice per square foot would be a good feature to add in our X.\n\n\nThe alpha hyperparameter of Ridge has similar interpretation of C hyperparameter of LogisticRegression; higher alpha means more complex model.\n\n\nIn Ridge, smaller alpha means bigger coefficients whereas bigger alpha means smaller coefficients."
  },
  {
    "objectID": "slides/slides-11.html#iclicker-exercise-10.2",
    "href": "slides/slides-11.html#iclicker-exercise-10.2",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "iClicker Exercise 10.2",
    "text": "iClicker Exercise 10.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nWe can use still use precision and recall for regression problems but now we have other metrics we can use as well.\n\n\nIn sklearn for regression problems, using r2_score() and .score() (with default values) will produce the same results.\n\n\nRMSE is always going to be non-negative.\n\n\nMSE does not directly provide the information about whether the model is underpredicting or overpredicting.\n\n\nWe can pass multiple scoring metrics to GridSearchCV or RandomizedSearchCV for regression as well as classification problems."
  },
  {
    "objectID": "slides/slides-11.html#ridge-and-ridgecv",
    "href": "slides/slides-11.html#ridge-and-ridgecv",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Ridge and RidgeCV",
    "text": "Ridge and RidgeCV\n\nRidge Regression: alpha hyperparameter controls model complexity.\nRidgeCV: Ridge regression with built-in cross-validation to find the optimal alpha."
  },
  {
    "objectID": "slides/slides-11.html#alpha-hyperparameter",
    "href": "slides/slides-11.html#alpha-hyperparameter",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "alpha hyperparameter",
    "text": "alpha hyperparameter\n\nRole of alpha:\n\nControls model complexity\nHigher alpha: Simpler model, smaller coefficients.\nLower alpha: Complex model, larger coefficients."
  },
  {
    "objectID": "slides/slides-11.html#regression-metrics-mse-rmse-mape",
    "href": "slides/slides-11.html#regression-metrics-mse-rmse-mape",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Regression metrics: MSE, RMSE, MAPE",
    "text": "Regression metrics: MSE, RMSE, MAPE\n\nMean Squared Error (MSE): Average of the squares of the errors.\nRoot Mean Squared Error (RMSE): Square root of MSE, same units as the target variable.\nMean Absolute Percentage Error (MAPE): Average of the absolute percentage errors."
  },
  {
    "objectID": "slides/slides-11.html#applying-log-transformation-to-the-targets",
    "href": "slides/slides-11.html#applying-log-transformation-to-the-targets",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Applying log transformation to the targets",
    "text": "Applying log transformation to the targets\n\nSuitable when the target has a wide range and spans several orders of magnitude\n\nExample: counts data such as social media likes or price data\n\nHelps manage skewed data, making patterns more apparent and regression models more effective.\nTransformedTargetRegressor\n\nWraps a regression model and applies a transformation to the target values."
  },
  {
    "objectID": "slides/slides-11.html#what-is-machine-learning",
    "href": "slides/slides-11.html#what-is-machine-learning",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "What is machine learning",
    "text": "What is machine learning\n\nML uses data to build models that identify patterns, make predictions, or generate content.\nIt enables computers to learn from data.\nNo single model is suitable for all situations."
  },
  {
    "objectID": "slides/slides-11.html#when-is-ml-suitable",
    "href": "slides/slides-11.html#when-is-ml-suitable",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "When is ML suitable?",
    "text": "When is ML suitable?\n\nML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\nRule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making.\nHuman experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence."
  },
  {
    "objectID": "slides/slides-11.html#terminology",
    "href": "slides/slides-11.html#terminology",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Terminology",
    "text": "Terminology\n\nFeatures (X) and target (y)\nExamples\nPredictions\nAccuracy, error\nParameters and hyperparameters\nDecision boundaries"
  },
  {
    "objectID": "slides/slides-11.html#important-concepts",
    "href": "slides/slides-11.html#important-concepts",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Important concepts",
    "text": "Important concepts\n\nWhat is a baseline? Why do we use them?\nWhy do we split the data? What are train/valid/test splits?\nWhat are the benefits of cross-validation?\nWhat is underfitting and overfitting?\nWhat‚Äôs the fundamental trade-off in supervised machine learning?\nWhat is the golden rule of machine learning?"
  },
  {
    "objectID": "slides/slides-11.html#overfitting-and-underfitting",
    "href": "slides/slides-11.html#overfitting-and-underfitting",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\n\n\nSource\n\n\nAn overfit model matches the training set so closely that it fails to make correct predictions on new unseen data.\n\nAn underfit model is too simple and does not even make good predictions on the training data"
  },
  {
    "objectID": "slides/slides-11.html#the-fundamental-tradeoff",
    "href": "slides/slides-11.html#the-fundamental-tradeoff",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "The fundamental tradeoff",
    "text": "The fundamental tradeoff\n\n\n\n\n\n\n\n\n\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.\n\nHow to pick a model?"
  },
  {
    "objectID": "slides/slides-11.html#supervised-models-we-have-seen",
    "href": "slides/slides-11.html#supervised-models-we-have-seen",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Supervised models we have seen",
    "text": "Supervised models we have seen\n\nDecision trees: Split data into subsets based on feature values to create decision rules\nk-NNs: Classify based on the majority vote from k nearest neighbors\nSVM RBFs: Create a boundary using an RBF kernel to separate classes\nLinear models: Assumption that the relationship between X and y is linear"
  },
  {
    "objectID": "slides/slides-11.html#comparison-of-models",
    "href": "slides/slides-11.html#comparison-of-models",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Comparison of models",
    "text": "Comparison of models\n\n\n\n\n\n\n\n\n\nModel\nParameters and hyperparameters\nStrengths\nWeaknesses\n\n\n\n\nDecision Trees\n\n\n\n\n\nKNNs\n\n\n\n\n\nSVM RBF\n\n\n\n\n\nLinear models"
  },
  {
    "objectID": "slides/slides-11.html#sklearn-transformers",
    "href": "slides/slides-11.html#sklearn-transformers",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "sklearn Transformers",
    "text": "sklearn Transformers\n\n\n\n\n\n\n\n\nTransformer\nHyperparameters\nWhen to use?\n\n\n\n\nSimpleImputer\n\n\n\n\nStandardScaler\n\n\n\n\nOneHotEncoder\n\n\n\n\nOrdinalEncoder\n\n\n\n\nCountVectorizer\n\n\n\n\nTransformedTargetRegressor"
  },
  {
    "objectID": "slides/slides-11.html#features",
    "href": "slides/slides-11.html#features",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Features",
    "text": "Features\n\nUsing features that are directly related to the target can cause data leakage.\n\nExample: If you‚Äôre building a model to predict the churn rate of customers in a subscription service (churned or not churned) and you are using a feature like ‚Äúaccount deactivation date‚Äù.\n\n\nIf a feature essentially gives away the answer, the model might perform exceptionally well during training but fail to generalize to new, unseen data."
  },
  {
    "objectID": "slides/slides-11.html#preprocessing",
    "href": "slides/slides-11.html#preprocessing",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nIncorporating information from the validation or test data during the preprocessing phase (e.g., scaling based on the distribution of the entire dataset instead of just the training set) can also lead to leakage. This can happen if the transformations applied to the training data are influenced by the whole dataset, thus indirectly feeding information about the test set into the model during training."
  },
  {
    "objectID": "slides/slides-11.html#pipelines",
    "href": "slides/slides-11.html#pipelines",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Pipelines",
    "text": "Pipelines\n\nPipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\nSimplify the code and improves readability.\nReduce the risk of data leakage by ensuring proper transformation of the training and test sets.\nAutomatically apply transformations in sequence."
  },
  {
    "objectID": "slides/slides-11.html#column-transformers",
    "href": "slides/slides-11.html#column-transformers",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Column Transformers",
    "text": "Column Transformers\n\nIn what scenarios do we use column transformers?\n\nDifferent transformations for different types of columns (e.g., scaling numerical data, encoding categorical data).\n\nHandle datasets with heterogeneous data types effectively."
  },
  {
    "objectID": "slides/slides-11.html#linear-models",
    "href": "slides/slides-11.html#linear-models",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Linear models",
    "text": "Linear models\n\nHow prediction is done\nCoefficient interpretation\nImpact of scaling\nEffect of hyperparameters\nAdvantages of getting probability scores instead of hard predictions during classification (Logistic)"
  },
  {
    "objectID": "slides/slides-11.html#hyperparameter-optimization",
    "href": "slides/slides-11.html#hyperparameter-optimization",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\n\n\n\n\n\n\n\n\nMethod\nStrengths/Weaknesses\nWhen to use?\n\n\n\n\nNested for loops\n\n\n\n\nGrid search\n\n\n\n\nRandom search"
  },
  {
    "objectID": "slides/slides-11.html#classification-metrics",
    "href": "slides/slides-11.html#classification-metrics",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nConfusion matrix\nFP, FN, TP, TN\n\n\n\n\n\n\n\n\n\nMetric\nHow to generate/calculate?\nWhen to use?\n\n\n\n\nAccuracy\n\n\n\n\nPrecision\n\n\n\n\nRecall\n\n\n\n\nF1-score\n\n\n\n\nAP score\n\n\n\n\nAUC"
  },
  {
    "objectID": "slides/slides-11.html#regression-metrics",
    "href": "slides/slides-11.html#regression-metrics",
    "title": "CPSC 330 Lecture 11: Midterm review",
    "section": "Regression Metrics",
    "text": "Regression Metrics\n\n\n\n\n\n\n\n\nMetric\nHow to generate/calculate?\nWhen to use?\n\n\n\n\nMSE\n\n\n\n\nRMSE\n\n\n\n\nr2 score\n\n\n\n\nMAPE"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs.¬†regression, inference vs.¬†prediction, accuracy vs.¬†error, baselines, intuition of decision trees\n\n\n\n\nLecture 3: ML fundamentals\n\n\nGeneralization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule\n\n\n\n\nLecture 4: k-nearest neighbours and SVM RBFs\n\n\nIntroduction to KNNs, hyperparameter n_neighbours or \\(k\\), C and gamma hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.\n\n\n\n\nLecture 5: Preprocessing and sklearn pipelines\n\n\nPreprocessing motivation, Common transformations in sklearn, sklearn transformers vs.¬†Estimators, The golden rule in the feature transformations, sklearn pipelines\n\n\n\n\nLecture 6: sklearn column transformer and text fearutres\n\n\nColumn transformer, arguments of OHE, encoding text features, incorporating text features in an ML pipeline\n\n\n\n\nLecture 7: Linear models\n\n\nIntuition behind linear models, linear regression and logistic regression, scikit-learn‚Äôs Ridge model, prediction probabilities, interpret model predictions using coefficients learned by a linear model, parametric vs.¬†non-parametric models\n\n\n\n\nLecture 8: Hyperparameter Optimization\n\n\nMotivation for hyperparameter optimization, hyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV, optimization bias\n\n\n\n\nLecture 9: Classification metrics\n\n\nconfusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance\n\n\n\n\nLecture 10: Regression metrics\n\n\nClassification metrics recap, ridge and RidgeCV, alpha hyperparameter of ridge, MSE, RMSE, MAPE, log transformations on the target\n\n\n\n\nLecture 11: Midterm review\n\n\n¬†\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "lectures.html#schedule",
    "href": "lectures.html#schedule",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs.¬†regression, inference vs.¬†prediction, accuracy vs.¬†error, baselines, intuition of decision trees\n\n\n\n\nLecture 3: ML fundamentals\n\n\nGeneralization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule\n\n\n\n\nLecture 4: k-nearest neighbours and SVM RBFs\n\n\nIntroduction to KNNs, hyperparameter n_neighbours or \\(k\\), C and gamma hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.\n\n\n\n\nLecture 5: Preprocessing and sklearn pipelines\n\n\nPreprocessing motivation, Common transformations in sklearn, sklearn transformers vs.¬†Estimators, The golden rule in the feature transformations, sklearn pipelines\n\n\n\n\nLecture 6: sklearn column transformer and text fearutres\n\n\nColumn transformer, arguments of OHE, encoding text features, incorporating text features in an ML pipeline\n\n\n\n\nLecture 7: Linear models\n\n\nIntuition behind linear models, linear regression and logistic regression, scikit-learn‚Äôs Ridge model, prediction probabilities, interpret model predictions using coefficients learned by a linear model, parametric vs.¬†non-parametric models\n\n\n\n\nLecture 8: Hyperparameter Optimization\n\n\nMotivation for hyperparameter optimization, hyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV, optimization bias\n\n\n\n\nLecture 9: Classification metrics\n\n\nconfusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance\n\n\n\n\nLecture 10: Regression metrics\n\n\nClassification metrics recap, ridge and RidgeCV, alpha hyperparameter of ridge, MSE, RMSE, MAPE, log transformations on the target\n\n\n\n\nLecture 11: Midterm review\n\n\n¬†\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to CPSC330! Here, you‚Äôll find slides for CPSC 330 Section 204. These slides are based on the notes present here.\n\nClass times üïò 11 am to 12:20 pm\nWhere? üìç Geography Building (GEOG) - 212, 1984 West Mall, Vancouver, BC V6T 1Z2"
  },
  {
    "objectID": "slides/slides-10.html#announcements",
    "href": "slides/slides-10.html#announcements",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/m4ujp0s4xgm5o5/post/204\nGood news for you: You‚Äôll have access to our course notes in the midterm!\n\nPractice midterm questions available on PL\n\nYou can see an example of all the info you will have access to in the last question.\n\nHW4 due Monday\nRemember to attend tutorials!\n\nThis weeks tutorial will cover data imbalance and fairness"
  },
  {
    "objectID": "slides/slides-10.html#recap-confusion-matrix",
    "href": "slides/slides-10.html#recap-confusion-matrix",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: Confusion matrix",
    "text": "Recap: Confusion matrix\n\n\n\n\n\nTN \\(\\rightarrow\\) True negatives\nFP \\(\\rightarrow\\) False positives\nFN \\(\\rightarrow\\) False negatives\nTP \\(\\rightarrow\\) True positives"
  },
  {
    "objectID": "slides/slides-10.html#recap-precision-recall-f1-score",
    "href": "slides/slides-10.html#recap-precision-recall-f1-score",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: Precision, Recall, F1-Score",
    "text": "Recap: Precision, Recall, F1-Score\n\n\n\n\n\\[ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}\\]"
  },
  {
    "objectID": "slides/slides-10.html#recap-iclicker-9.2-c",
    "href": "slides/slides-10.html#recap-iclicker-9.2-c",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: iClicker 9.2-C",
    "text": "Recap: iClicker 9.2-C\nLowering the classification threshold generally increases the model‚Äôs recall.\n\\[ recall = \\frac{TP}{TP + FN} \\]"
  },
  {
    "objectID": "slides/slides-10.html#recap-iclicker-9.2-d",
    "href": "slides/slides-10.html#recap-iclicker-9.2-d",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: iClicker 9.2-D",
    "text": "Recap: iClicker 9.2-D\nRaising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positive.\n\\[ precision = \\frac{TP}{TP + FP} \\]"
  },
  {
    "objectID": "slides/slides-10.html#recap-pr-curve",
    "href": "slides/slides-10.html#recap-pr-curve",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: PR curve",
    "text": "Recap: PR curve\n\nCalculate precision and recall (TPR) at every possible threshold and graph them.\nBetter choice for highly imbalanced datasets because it focuses on the performance of the positive class."
  },
  {
    "objectID": "slides/slides-10.html#recap-point-vs-curve-metrics",
    "href": "slides/slides-10.html#recap-point-vs-curve-metrics",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: Point vs curve metrics",
    "text": "Recap: Point vs curve metrics\n\nWhat‚Äôs the difference between the average precision (AP) score and F1-score?\nWhich model would you pick?"
  },
  {
    "objectID": "slides/slides-10.html#recap-roc-curve",
    "href": "slides/slides-10.html#recap-roc-curve",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: ROC curve",
    "text": "Recap: ROC curve\n\nCalculate the true positive rate (TPR aka recall) (\\(\\frac{TP}{TP + FN}\\)) and false positive rate (FPR) (\\(\\frac{FP}{FP + TN}\\)) at every possible thresholding and graph TPR over FPR.\nGood choice when the datasets are roughly balanced."
  },
  {
    "objectID": "slides/slides-10.html#recap-roc-curve-1",
    "href": "slides/slides-10.html#recap-roc-curve-1",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: ROC Curve",
    "text": "Recap: ROC Curve\n\nNot a great choice when there is an extreme imbalance because FPR can remain relatively low even if the number of false positives is high, simply because the number of negatives is very large.\n\\[ FPR  = \\frac{FP}{FP + TN}\\]\nThe area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative."
  },
  {
    "objectID": "slides/slides-10.html#recap-roc-curve-2",
    "href": "slides/slides-10.html#recap-roc-curve-2",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Recap: ROC Curve",
    "text": "Recap: ROC Curve\n\n\n\n\n\nWhich model would you pick?"
  },
  {
    "objectID": "slides/slides-10.html#questions-for-you",
    "href": "slides/slides-10.html#questions-for-you",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\n\n\n\n\nWhat‚Äôs the AUC of a baseline model?\n\n\nSource"
  },
  {
    "objectID": "slides/slides-10.html#dealing-with-class-imbalance",
    "href": "slides/slides-10.html#dealing-with-class-imbalance",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Dealing with class imbalance",
    "text": "Dealing with class imbalance\n\nUnder sampling\nOversampling\nclass weight=\"balanced\" (preferred method for this course)\nSMOTE"
  },
  {
    "objectID": "slides/slides-10.html#ridge-and-ridgecv",
    "href": "slides/slides-10.html#ridge-and-ridgecv",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Ridge and RidgeCV",
    "text": "Ridge and RidgeCV\n\nRidge Regression: alpha hyperparameter controls model complexity.\nRidgeCV: Ridge regression with built-in cross-validation to find the optimal alpha."
  },
  {
    "objectID": "slides/slides-10.html#alpha-hyperparameter",
    "href": "slides/slides-10.html#alpha-hyperparameter",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "alpha hyperparameter",
    "text": "alpha hyperparameter\n\nRole of alpha:\n\nControls model complexity\nHigher alpha: Simpler model, smaller coefficients.\nLower alpha: Complex model, larger coefficients."
  },
  {
    "objectID": "slides/slides-10.html#regression-metrics-mse-rmse-mape",
    "href": "slides/slides-10.html#regression-metrics-mse-rmse-mape",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Regression metrics: MSE, RMSE, MAPE",
    "text": "Regression metrics: MSE, RMSE, MAPE\n\nMean Squared Error (MSE): Average of the squares of the errors.\nRoot Mean Squared Error (RMSE): Square root of MSE, same units as the target variable.\nMean Absolute Percentage Error (MAPE): Average of the absolute percentage errors."
  },
  {
    "objectID": "slides/slides-10.html#applying-log-transformation-to-the-targets",
    "href": "slides/slides-10.html#applying-log-transformation-to-the-targets",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Applying log transformation to the targets",
    "text": "Applying log transformation to the targets\n\nSuitable when the target has a wide range and spans several orders of magnitude\n\nExample: counts data such as social media likes or price data\n\nHelps manage skewed data, making patterns more apparent and regression models more effective.\nTransformedTargetRegressor\n\nWraps a regression model and applies a transformation to the target values."
  },
  {
    "objectID": "slides/slides-10.html#iclicker-exercise-10.1",
    "href": "slides/slides-10.html#iclicker-exercise-10.1",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "iClicker Exercise 10.1",
    "text": "iClicker Exercise 10.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nPrice per square foot would be a good feature to add in our X.\n\n\nThe alpha hyperparameter of Ridge has similar interpretation of C hyperparameter of LogisticRegression; higher alpha means more complex model.\n\n\nIn Ridge, smaller alpha means bigger coefficients whereas bigger alpha means smaller coefficients."
  },
  {
    "objectID": "slides/slides-10.html#iclicker-exercise-10.2",
    "href": "slides/slides-10.html#iclicker-exercise-10.2",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "iClicker Exercise 10.2",
    "text": "iClicker Exercise 10.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nWe can use still use precision and recall for regression problems but now we have other metrics we can use as well.\n\n\nIn sklearn for regression problems, using r2_score() and .score() (with default values) will produce the same results.\n\n\nRMSE is always going to be non-negative.\n\n\nMSE does not directly provide the information about whether the model is underpredicting or overpredicting.\n\n\nWe can pass multiple scoring metrics to GridSearchCV or RandomizedSearchCV for regression as well as classification problems."
  },
  {
    "objectID": "lecture-01.html",
    "href": "lecture-01.html",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#slides",
    "href": "lecture-01.html#slides",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#outline",
    "href": "lecture-01.html#outline",
    "title": "Lecture 1: Course introduction",
    "section": "Outline",
    "text": "Outline\n\nWhat is machine learning\nTypes of machine learning\nLearning to navigate through the course materials\nGetting familiar with the course policies",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-03.html",
    "href": "lecture-03.html",
    "title": "Lecture 3: ML fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-03.html#slides",
    "href": "lecture-03.html#slides",
    "title": "Lecture 3: ML fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-03.html#outline",
    "href": "lecture-03.html#outline",
    "title": "Lecture 3: ML fundamentals",
    "section": "Outline",
    "text": "Outline\n\nGeneralization, data splitting\nCross-validation\nOverfitting, underfitting, the fundamental tradeoff\nThe golden rule",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-05.html",
    "href": "lecture-05.html",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-05.html#slides",
    "href": "lecture-05.html#slides",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-05.html#outline",
    "href": "lecture-05.html#outline",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Outline",
    "text": "Outline\n\nRecap\nPreprocessing motivation\nCommon transformations in sklearn\nsklearn transformers vs.¬†Estimators\nThe golden rule in the feature transformations\nsklearn pipelines",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-07.html",
    "href": "lecture-07.html",
    "title": "Lecture 7: Linear models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-07.html#slides",
    "href": "lecture-07.html#slides",
    "title": "Lecture 7: Linear models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-07.html#outline",
    "href": "lecture-07.html#outline",
    "title": "Lecture 7: Linear models",
    "section": "Outline",
    "text": "Outline\n\nIntuition behind linear models\nLinear regression and logistic regression\nscikit-learn‚Äôs Ridge model\nPrediction probabilities\nInterpret model predictions using coefficients learned by a linear model\nParametric vs.¬†non-parametric models",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-09.html",
    "href": "lecture-09.html",
    "title": "Lecture 9: Classification metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-09.html#slides",
    "href": "lecture-09.html#slides",
    "title": "Lecture 9: Classification metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-09.html#outline",
    "href": "lecture-09.html#outline",
    "title": "Lecture 9: Classification metrics",
    "section": "Outline",
    "text": "Outline\n\nIssues with using accuracy\nComponents of a confusion matrix\nPrecision, recall, and f1-score and use them to evaluate different classifiers\nPrecision-recall curves\nAverage precision score\nROC curves and ROC AUC using scikit-learn\nDealing with class imbalance\nModel performance on specific groups in a dataset.",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-11.html",
    "href": "lecture-11.html",
    "title": "Lecture 11: Midterm review",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 11: Midterm review"
    ]
  },
  {
    "objectID": "lecture-11.html#slides",
    "href": "lecture-11.html#slides",
    "title": "Lecture 11: Midterm review",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 11: Midterm review"
    ]
  },
  {
    "objectID": "lecture-11.html#outline",
    "href": "lecture-11.html#outline",
    "title": "Lecture 11: Midterm review",
    "section": "Outline",
    "text": "Outline",
    "crumbs": [
      "Lectures",
      "Lecture 11: Midterm review"
    ]
  },
  {
    "objectID": "slides/slides-05.html#announcements",
    "href": "slides/slides-05.html#announcements",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Announcements",
    "text": "Announcements\n\nHW1 grades have been posted.\n\nSee syllabus about regrade request etiquette.\n\nHomework 1 solutions have been posted on Canvas under Files tab. Please do not share them with anyone or do not post them anywhere.\nSyllabus quiz is due Jan 24.\nHW3 should be available."
  },
  {
    "objectID": "slides/slides-05.html#learning-outcomes",
    "href": "slides/slides-05.html#learning-outcomes",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nExplain motivation for preprocessing in supervised machine learning;\nDiscuss golden rule in the context of feature transformations;\nIdentify when to implement feature transformations such as imputation, scaling, and one-hot encoding in a machine learning model development pipeline;\nUse sklearn transformers for applying feature transformations on your dataset;\nUse sklearn.pipeline.Pipeline and sklearn.pipeline.make_pipeline to build a preliminary machine learning pipeline."
  },
  {
    "objectID": "slides/slides-05.html#recap",
    "href": "slides/slides-05.html#recap",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Recap",
    "text": "Recap\n\nDecision trees: Split data into subsets based on feature values to create decision rules\n\\(k\\)-NNs: Classify based on the majority vote from k nearest neighbors\nSVM RBFs: Create a boundary using an RBF kernel to separate classes"
  },
  {
    "objectID": "slides/slides-05.html#motivation",
    "href": "slides/slides-05.html#motivation",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Motivation",
    "text": "Motivation\n\n\nSo far we have seen\n\nThree ML models (decision trees, \\(k\\)-NNs, SVMs with RBF kernel)\nML fundamentals (train-validation-test split, cross-validation, the fundamental tradeoff, the golden rule)\n\nAre we ready to do machine learning on real-world datasets?\n\nVery often real-world datasets need preprocessing before we use them to build ML models."
  },
  {
    "objectID": "slides/slides-05.html#iclicker-exercise-5.1",
    "href": "slides/slides-05.html#iclicker-exercise-5.1",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.1",
    "text": "(iClicker) Exercise 5.1\niClicker cloud join link: https://join.iclicker.com/HTRZ\nTake a guess: In your machine learning project, how much time will you typically spend on data preparation and transformation?\n\n\n~80% of the project time\n\n\n~20% of the project time\n\n\n~50% of the project time\n\n\nNone. Most of the time will be spent on model building\n\n\nThe question is adapted from here."
  },
  {
    "objectID": "slides/slides-05.html#preprocessing-motivation-example",
    "href": "slides/slides-05.html#preprocessing-motivation-example",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Preprocessing motivation: example",
    "text": "Preprocessing motivation: example\nYou‚Äôre trying to find a suitable date based on:\n\nAge (closer to yours is better).\nNumber of Facebook Friends (how should we interpret?)."
  },
  {
    "objectID": "slides/slides-05.html#preprocessing-motivation-example-1",
    "href": "slides/slides-05.html#preprocessing-motivation-example-1",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Preprocessing motivation: example",
    "text": "Preprocessing motivation: example\n\nYou are 30 years old and have 250 Facebook friends.\n\n\n\n\n\n\n\n\n\n\n\nPerson\nAge\n#FB Friends\nEuclidean Distance Calculation\nDistance\n\n\n\n\nA\n25\n400\n‚àö(5¬≤ + 150¬≤)\n150.08\n\n\nB\n27\n300\n‚àö(3¬≤ + 50¬≤)\n50.09\n\n\nC\n30\n500\n‚àö(0¬≤ + 250¬≤)\n250.00\n\n\nD\n60\n250\n‚àö(30¬≤ + 0¬≤)\n30.00\n\n\n\nBased on the distances, the two nearest neighbors (2-NN) are:\n\nPerson D (Distance: 30.00)\nPerson B (Distance: 50.09)\n\nWhat‚Äôs the problem here?"
  },
  {
    "objectID": "slides/slides-05.html#imputation-fill-the-gaps",
    "href": "slides/slides-05.html#imputation-fill-the-gaps",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Imputation: Fill the gaps! (üü© üüß üü¶)",
    "text": "Imputation: Fill the gaps! (üü© üüß üü¶)\nFill in missing data using a chosen strategy:\n\nMean: Replace missing values with the average of the available data.\nMedian: Use the middle value.\nMost Frequent: Use the most common value (mode).\nKNN Imputation: Fill based on similar neighbors.\n\nExample:\nImputation is like filling in your average or median or most frequent grade for an assessment you missed.\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05.html#scaling-everything-to-the-same-range",
    "href": "slides/slides-05.html#scaling-everything-to-the-same-range",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Scaling: Everything to the same range! (üìâ üìà)",
    "text": "Scaling: Everything to the same range! (üìâ üìà)\nEnsure all features have a comparable range.\n\nStandardScaler: Mean = 0, Standard Deviation = 1.\n\nExample:\nScaling is like adjusting the number of everyone‚Äôs Facebook friends so that both the number of friends and their age are on a comparable scale. This way, one feature doesn‚Äôt dominate the other when making comparisons.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05.html#iclicker-exercise-5.2",
    "href": "slides/slides-05.html#iclicker-exercise-5.2",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.2",
    "text": "(iClicker) Exercise 5.2\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nStandardScaler ensures a fixed range (i.e., minimum and maximum values) for the features.\n\n\nStandardScaler calculates mean and standard deviation for each feature separately.\n\n\nIn general, it‚Äôs a good idea to apply scaling on numeric features before training \\(k\\)-NN or SVM RBF models.\n\n\nThe transformed feature values might be hard to interpret for humans.\n\n\nAfter applying SimpleImputer the transformed data has a different shape than the original data."
  },
  {
    "objectID": "slides/slides-05.html#one-hot-encoding-1-0-0",
    "href": "slides/slides-05.html#one-hot-encoding-1-0-0",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "One-Hot encoding: üçé ‚Üí 1Ô∏è‚É£ 0Ô∏è‚É£ 0Ô∏è‚É£",
    "text": "One-Hot encoding: üçé ‚Üí 1Ô∏è‚É£ 0Ô∏è‚É£ 0Ô∏è‚É£\nConvert categorical features into binary columns.\n\nCreates new binary columns for each category.\nUseful for handling categorical data in machine learning models.\n\nExample:\nTurn ‚ÄúApple, Banana, Orange‚Äù into binary columns:\n\n\n\nFruit\nüçé\nüçå\nüçä\n\n\n\n\nApple üçé\n1\n0\n0\n\n\nBanana üçå\n0\n1\n0\n\n\nOrange üçä\n0\n0\n1\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05.html#ordinal-encoding-ranking-matters-3",
    "href": "slides/slides-05.html#ordinal-encoding-ranking-matters-3",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Ordinal encoding: Ranking matters! (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ‚Üí 3Ô∏è‚É£)",
    "text": "Ordinal encoding: Ranking matters! (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ‚Üí 3Ô∏è‚É£)\nConvert categories into integer values that have a meaningful order.\n\nAssign integers based on order or rank.\nUseful when there is an inherent ranking in the data.\n\nExample:\nTurn ‚ÄúPoor, Average, Good‚Äù into 1, 2, 3:\n\n\n\nRating\nOrdinal\n\n\n\n\nPoor\n1\n\n\nAverage\n2\n\n\nGood\n3\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nX_ordinal = encoder.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05.html#transformers",
    "href": "slides/slides-05.html#transformers",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Transformers",
    "text": "Transformers\n\nAre used to transform or preprocess data.\nImplement the fit and transform methods.\n\nfit(X): Learns parameters from the data.\ntransform(X): Applies the learned transformation to the data.\n\nExamples:\n\nImputation (SimpleImputer): Fills missing values.\nScaling (StandardScaler): Standardizes features.\n\n\n\n\nfit_transform(X): Convenience method for calling fit and then transform on the same data."
  },
  {
    "objectID": "slides/slides-05.html#estimators",
    "href": "slides/slides-05.html#estimators",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Estimators",
    "text": "Estimators\n\nUsed to make predictions.\nImplement fit and predict methods.\n\nfit(X, y): Learns from labeled data.\npredict(X): Makes predictions on new data.\n\nExamples: DecisionTreeClassifier, SVC, KNeighborsClassifier\n\n\n\nRegression models are also estimators"
  },
  {
    "objectID": "slides/slides-05.html#how-to-carry-out-cross-validation-improper",
    "href": "slides/slides-05.html#how-to-carry-out-cross-validation-improper",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "How to carry out cross-validation? (improper)",
    "text": "How to carry out cross-validation? (improper)"
  },
  {
    "objectID": "slides/slides-05.html#how-to-carry-out-cross-validation-proper",
    "href": "slides/slides-05.html#how-to-carry-out-cross-validation-proper",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "How to carry out cross-validation? (proper)",
    "text": "How to carry out cross-validation? (proper)"
  },
  {
    "objectID": "slides/slides-05.html#the-golden-rule-in-feature-transformations",
    "href": "slides/slides-05.html#the-golden-rule-in-feature-transformations",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "The golden rule in feature transformations",
    "text": "The golden rule in feature transformations\n\nNever transform the entire dataset at once!\nWhy? It leads to data leakage ‚Äî using information from the test set in your training process, which can artificially inflate model performance.\nFit transformers like scalers and imputers on the training set only.\nApply the transformations to both the training and test sets separately.\n\nExample:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "slides/slides-05.html#sklearn-pipelines",
    "href": "slides/slides-05.html#sklearn-pipelines",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "sklearn Pipelines",
    "text": "sklearn Pipelines\n\nPipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\nSimplify the code and improves readability.\nReduce the risk of data leakage by ensuring proper transformation of the training and test sets.\nAutomatically apply transformations in sequence.\n\nExample:\nChaining a StandardScaler with a KNeighborsClassifier model.\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)"
  },
  {
    "objectID": "slides/slides-05.html#iclicker-exercise-5.3",
    "href": "slides/slides-05.html#iclicker-exercise-5.3",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.3",
    "text": "(iClicker) Exercise 5.3\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nYou can have scaling of numeric features, one-hot encoding of categorical features, and scikit-learn estimator within a single pipeline.\n\n\nOnce you have a scikit-learn pipeline object with an estimator as the last step, you can call fit, predict, and score on it.\n\n\nYou can carry out data splitting within scikit-learn pipeline.\n\n\nWe have to be careful of the order we put each transformation and model in a pipeline."
  },
  {
    "objectID": "slides/slides-08.html#announcements",
    "href": "slides/slides-08.html#announcements",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/m4ujp0s4xgm5o5/post/204\n\nHW3 is due next week Monday, Feb 3rd, 11:59 pm.\nReminder my office hours\n\nTuesday from 12:30 to 1:30 in my office ICCS 353"
  },
  {
    "objectID": "slides/slides-08.html#learning-outcomes",
    "href": "slides/slides-08.html#learning-outcomes",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nExplain the need for hyperparameter optimization\n\nCarry out hyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV\nExplain different hyperparameters of GridSearchCV\nExplain the importance of selecting a good range for the values.\nExplain optimization bias\nIdentify and reason when to trust and not trust reported accuracies"
  },
  {
    "objectID": "slides/slides-08.html#recap-logistic-regression",
    "href": "slides/slides-08.html#recap-logistic-regression",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nA linear model used for binary classification tasks.\n\n(Optional) There is am extension of logistic regression called multinomial logistic regression for multiclass classification.\n\nParameters:\n\nCoefficients (Weights): The model learns a coefficient or a weight associated with each feature that represents its importance.\nBias (Intercept): A constant term added to the linear combination of features and their coefficients."
  },
  {
    "objectID": "slides/slides-08.html#recap-logistic-regression-1",
    "href": "slides/slides-08.html#recap-logistic-regression-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nThe model computes a weighted sum of the input features‚Äô values, adjusted by their respective coefficients and the bias term.\nThis weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the ‚Äúpositive‚Äù class.\n\n\\[\\begin{equation}\n\\hat{p} = \\sigma\\left(\\sum_{j=1}^d w_j x_j + b\\right)\n\\end{equation}\\]\n\n\\(\\hat{p}\\) is the predicted probability of the example belonging to the positive class.\n\\(w_j\\) is the learned weight associated with feature \\(j\\)\n\\(x_j\\) is the value of the input feature \\(j\\)\n\\(b\\) is the bias term"
  },
  {
    "objectID": "slides/slides-08.html#recap-logistic-regression-2",
    "href": "slides/slides-08.html#recap-logistic-regression-2",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nFor a dataset with \\(d\\) features, the decision boundary that separates the classes is a \\(d-1\\) dimensional hyperplane.\n\nComplexity hyperparameter: C in sklearn.\n\nHigher C \\(\\rightarrow\\) more complex model meaning larger coefficients\nLower C \\(\\rightarrow\\) less complex model meaning smaller coefficients"
  },
  {
    "objectID": "slides/slides-08.html#interpretation-of-coefficients-in-linear-models",
    "href": "slides/slides-08.html#interpretation-of-coefficients-in-linear-models",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Interpretation of coefficients in linear models",
    "text": "Interpretation of coefficients in linear models\n\nThe \\(j\\)th coefficient tells us how feature \\(j\\) affects the prediction\n\nif \\(w_j &gt; 0\\) then increasing \\(x_{ij}\\) moves us toward predicting \\(+1\\)\nif \\(w_j &lt; 0\\) then increasing \\(x_{ij}\\) moves us toward prediction \\(-1\\)\nif \\(w_j = 0\\) then the feature is not used in making a prediction"
  },
  {
    "objectID": "slides/slides-08.html#importance-of-scaling",
    "href": "slides/slides-08.html#importance-of-scaling",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Importance of scaling",
    "text": "Importance of scaling\n\nWhen you are interpreting the model coefficients, scaling is crucial.\nIf you do not scale the data, features with smaller magnitude are going to get coefficients with bigger magnitude whereas features with bigger scale are going to get coefficients with smaller magnitude.\nThat said, when you scale the data, feature values become hard to interpret for humans!"
  },
  {
    "objectID": "slides/slides-08.html#limitations-of-linear-models",
    "href": "slides/slides-08.html#limitations-of-linear-models",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Limitations of linear models",
    "text": "Limitations of linear models\n\nIs your data ‚Äúlinearly separable‚Äù? Can you draw a hyperplane between these datapoints that separates them with 0 error.\nIf the training examples can be separated by a linear decision rule, they are linearly separable."
  },
  {
    "objectID": "slides/slides-08.html#recap-countvectorizer-input",
    "href": "slides/slides-08.html#recap-countvectorizer-input",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: CountVectorizer input",
    "text": "Recap: CountVectorizer input\n\nPrimarily designed to accept either a pandas.Series of text data or a 1D numpy array. It can also process a list of string data directly.\nUnlike many transformers that handle multiple features (DataFrame or 2D numpy array), CountVectorizer a single text column at a time.\nIf your dataset contains multiple text columns, you will need to instantiate separate CountVectorizer objects for each text feature.\nThis approach ensures that the unique vocabulary and tokenization processes are correctly applied to each specific text column without interference."
  },
  {
    "objectID": "slides/slides-08.html#hyperparameter-optimization",
    "href": "slides/slides-08.html#hyperparameter-optimization",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Hyperparameter optimization",
    "text": "Hyperparameter optimization"
  },
  {
    "objectID": "slides/slides-08.html#data",
    "href": "slides/slides-08.html#data",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Data",
    "text": "Data\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\n3130\nspam\nLookAtMe!: Thanks for your purchase of a video...\n\n\n106\nham\nAight, I'll hit you up when I get some cash\n\n\n4697\nham\nDon no da:)whats you plan?\n\n\n856\nham\nGoing to take your babe out ?"
  },
  {
    "objectID": "slides/slides-08.html#model-building",
    "href": "slides/slides-08.html#model-building",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Model building",
    "text": "Model building\n\nLet‚Äôs define a pipeline\n\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\n\nWhat are some hyperparameters for this pipeline?\n\n\n\nSuppose we want to try out different hyperparameter values.\n\n\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}"
  },
  {
    "objectID": "slides/slides-08.html#hyperparameters-the-problem",
    "href": "slides/slides-08.html#hyperparameters-the-problem",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Hyperparameters: the problem",
    "text": "Hyperparameters: the problem\n\nIn order to improve the generalization performance, finding the best values for the important hyperparameters of a model is necessary for almost all models and datasets.\nPicking good hyperparameters is important because if we don‚Äôt do it, we might end up with an underfit or overfit model."
  },
  {
    "objectID": "slides/slides-08.html#manual-hyperparameter-optimization-procedure",
    "href": "slides/slides-08.html#manual-hyperparameter-optimization-procedure",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Manual hyperparameter optimization procedure",
    "text": "Manual hyperparameter optimization procedure\n\nDefine a parameter space.\nIterate through possible combinations.\nEvaluate model performance.\n\n\n\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "slides/slides-08.html#manual-hyperparameter-optimization",
    "href": "slides/slides-08.html#manual-hyperparameter-optimization",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Manual hyperparameter optimization",
    "text": "Manual hyperparameter optimization\n\nAdvantage: we may have some intuition about what might work.\n\nE.g. if I‚Äôm massively overfitting, try decreasing max_depth or C.\n\nDisadvantages\n\nIt takes a lot of work\nNot reproducible\nIn very complicated cases, our intuition might be worse than a data-driven approach"
  },
  {
    "objectID": "slides/slides-08.html#automated-hyperparameter-optimization",
    "href": "slides/slides-08.html#automated-hyperparameter-optimization",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Automated hyperparameter optimization",
    "text": "Automated hyperparameter optimization\n\nFormulate the hyperparamter optimization as a one big search problem.\nOften we have many hyperparameters of different types: categorical, integer, and continuous.\nOften, the search space is quite big and systematic search for optimal values is infeasible."
  },
  {
    "objectID": "slides/slides-08.html#sklearn-methods",
    "href": "slides/slides-08.html#sklearn-methods",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "sklearn methods",
    "text": "sklearn methods\n\nsklearn provides two main methods for hyperparameter optimization\n\nGrid Search\nRandom Search"
  },
  {
    "objectID": "slides/slides-08.html#grid-search-overview",
    "href": "slides/slides-08.html#grid-search-overview",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Grid search overview",
    "text": "Grid search overview\n\nCovers all possible combinations from the provided grid.\nCan be parallelized easily.\nIntegrates cross-validation."
  },
  {
    "objectID": "slides/slides-08.html#grid-search-in-practice",
    "href": "slides/slides-08.html#grid-search-in-practice",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Grid search in practice",
    "text": "Grid search in practice\n\nFor GridSearchCV we need\n\nAn instantiated model or a pipeline\nA parameter grid: A user specifies a set of values for each hyperparameter.\nOther optional arguments\n\n\nThe method considers product of the sets and evaluates each combination one by one."
  },
  {
    "objectID": "slides/slides-08.html#grid-search-example",
    "href": "slides/slides-08.html#grid-search-example",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Grid search example",
    "text": "Grid search example\n\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(\n  pipe_svm, \n  param_grid=param_grid, \n  n_jobs=-1, \n  return_train_score=True\n)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n\nnp.float64(0.9782606272997375)\n\n\n\nnjobs=-1 will use all available cores"
  },
  {
    "objectID": "slides/slides-08.html#problems-with-exhaustive-grid-search",
    "href": "slides/slides-08.html#problems-with-exhaustive-grid-search",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Problems with exhaustive grid search",
    "text": "Problems with exhaustive grid search\n\nRequired number of models to evaluate grows exponentially with the dimensionally of the configuration space.\nExample: Suppose you have\n\n5 hyperparameters\n10 different values for each hyperparameter\nYou‚Äôll be evaluating \\(10^5=100,000\\) models! That is you‚Äôll be calling cross_validate 100,000 times!\nExhaustive search may become infeasible fairly quickly.\n\nOther options?"
  },
  {
    "objectID": "slides/slides-08.html#random-search-overview",
    "href": "slides/slides-08.html#random-search-overview",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Random search overview",
    "text": "Random search overview\n\nMore efficient than grid search when dealing with large hyperparameter spaces.\nSamples a given number of parameter settings from distributions."
  },
  {
    "objectID": "slides/slides-08.html#random-search-example",
    "href": "slides/slides-08.html#random-search-example",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Random search example",
    "text": "Random search example\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform, randint, uniform\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(\n  pipe_svm,                                    \n  param_distributions=param_dist, \n  n_iter=10, \n  n_jobs=-1, \n  return_train_score=True\n)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n\nnp.float64(0.9828474655872702)"
  },
  {
    "objectID": "slides/slides-08.html#n_iter",
    "href": "slides/slides-08.html#n_iter",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "n_iter",
    "text": "n_iter\n\nNote the n_iter, we didn‚Äôt need this for GridSearchCV.\nLarger n_iter will take longer but it‚Äôll do more searching.\n\nRemember you still need to multiply by number of folds!\n\nYou can set random_state for reproducibility but you don‚Äôt have to do it."
  },
  {
    "objectID": "slides/slides-08.html#advantages-of-randomizedsearchcv",
    "href": "slides/slides-08.html#advantages-of-randomizedsearchcv",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Advantages of RandomizedSearchCV",
    "text": "Advantages of RandomizedSearchCV\n\nFaster compared to GridSearchCV.\nAdding parameters that do not influence the performance does not affect efficiency.\nWorks better when some parameters are more important than others.\nIn general, I recommend using RandomizedSearchCV rather than GridSearchCV."
  },
  {
    "objectID": "slides/slides-08.html#advantages-of-randomizedsearchcv-1",
    "href": "slides/slides-08.html#advantages-of-randomizedsearchcv-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Advantages of RandomizedSearchCV",
    "text": "Advantages of RandomizedSearchCV"
  },
  {
    "objectID": "slides/slides-08.html#questions-for-class-discussion",
    "href": "slides/slides-08.html#questions-for-class-discussion",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Questions for class discussion",
    "text": "Questions for class discussion\n\nSuppose you have 10 hyperparameters, each with 4 possible values. If you run GridSearchCV with this parameter grid, how many cross-validation experiments will be carried out?\nSuppose you have 10 hyperparameters and each takes 4 values. If you run RandomizedSearchCV with this parameter grid with n_iter=20, how many cross-validation experiments will be carried out?"
  },
  {
    "objectID": "slides/slides-08.html#iclicker-exercise-8.1",
    "href": "slides/slides-08.html#iclicker-exercise-8.1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "(iClicker) Exercise 8.1",
    "text": "(iClicker) Exercise 8.1\niClicker cloud join link: https://join.iclicker.com/VYFJ\nSelect all of the following statements which are TRUE.\n\n\nIf you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n\n\nGrid search is guaranteed to find the best hyperparameter values.\n\n\nIt is possible to get different hyperparameters in different runs of RandomizedSearchCV."
  },
  {
    "objectID": "slides/slides-08.html#optimization-bias-motivation",
    "href": "slides/slides-08.html#optimization-bias-motivation",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Optimization bias (motivation)",
    "text": "Optimization bias (motivation)\n\nWhy do we need to evaluate the model on the test set in the end?\nWhy not just use cross-validation on the whole dataset?\nWhile carrying out hyperparameter optimization, we usually try over many possibilities.\n\nIf our dataset is small and if your validation set is hit too many times, we suffer from optimization bias or overfitting the validation set."
  },
  {
    "objectID": "slides/slides-08.html#optimization-bias-of-parameter-learning",
    "href": "slides/slides-08.html#optimization-bias-of-parameter-learning",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Optimization bias of parameter learning",
    "text": "Optimization bias of parameter learning\n\nOverfitting of the training error\nAn example:\n\nDuring training, we could search over tons of different decision trees.\n\nSo we can get ‚Äúlucky‚Äù and find a tree with low training error by chance."
  },
  {
    "objectID": "slides/slides-08.html#optimization-bias-of-hyper-parameter-learning",
    "href": "slides/slides-08.html#optimization-bias-of-hyper-parameter-learning",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Optimization bias of hyper-parameter learning",
    "text": "Optimization bias of hyper-parameter learning\n\nOverfitting of the validation error\nAn example:\n\nHere, we might optimize the validation error over 1000 values of max_depth.\nOne of the 1000 trees might have low validation error by chance."
  },
  {
    "objectID": "slides/slides-08.html#optional-example-1-optimization-bias",
    "href": "slides/slides-08.html#optional-example-1-optimization-bias",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "(Optional) Example 1: Optimization bias",
    "text": "(Optional) Example 1: Optimization bias\nConsider a multiple-choice (a,b,c,d) ‚Äútest‚Äù with 10 questions:\n\nIf you choose answers randomly, expected grade is 25% (no bias).\nIf you fill out two tests randomly and pick the best, expected grade is 33%.\n\nOptimization bias of ~8%.\n\nIf you take the best among 10 random tests, expected grade is ~47%.\nIf you take the best among 100, expected grade is ~62%.\nIf you take the best among 1000, expected grade is ~73%.\nIf you take the best among 10000, expected grade is ~82%.\n\nYou have so many ‚Äúchances‚Äù that you expect to do well.\n\n\n\nBut on new questions the ‚Äúrandom choice‚Äù accuracy is still 25%."
  },
  {
    "objectID": "slides/slides-08.html#optional-example-2-optimization-bias",
    "href": "slides/slides-08.html#optional-example-2-optimization-bias",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "(Optional) Example 2: Optimization bias",
    "text": "(Optional) Example 2: Optimization bias\nIf we instead used a 100-question test then:\n\nExpected grade from best over 1 randomly-filled test is 25%.\nExpected grade from best over 2 randomly-filled test is ~27%.\nExpected grade from best over 10 randomly-filled test is ~32%.\nExpected grade from best over 100 randomly-filled test is ~36%.\nExpected grade from best over 1000 randomly-filled test is ~40%.\nExpected grade from best over 10000 randomly-filled test is ~43%.\n\n\n\nThe optimization bias grows with the number of things we try.\n\n‚ÄúComplexity‚Äù of the set of models we search over.\n\n\n\n\n\nBut, optimization bias shrinks quickly with the number of examples.\n\nBut it‚Äôs still non-zero and growing if you over-use your validation set!"
  },
  {
    "objectID": "slides/slides-08.html#optimization-bias-overview",
    "href": "slides/slides-08.html#optimization-bias-overview",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Optimization bias overview",
    "text": "Optimization bias overview\n\nWhy do we need separate validation and test datasets?"
  },
  {
    "objectID": "slides/slides-08.html#this-is-why-we-need-a-test-set",
    "href": "slides/slides-08.html#this-is-why-we-need-a-test-set",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "This is why we need a test set",
    "text": "This is why we need a test set\n\nThe frustrating part is that if our dataset is small then our test set is also small üòî.\nBut we don‚Äôt have a lot of better alternatives, unfortunately, if we have a small dataset."
  },
  {
    "objectID": "slides/slides-08.html#when-test-score-is-much-lower-than-cv-score",
    "href": "slides/slides-08.html#when-test-score-is-much-lower-than-cv-score",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "When test score is much lower than CV score",
    "text": "When test score is much lower than CV score\n\nWhat to do if your test score is much lower than your cross-validation score:\n\nTry simpler models and use the test set a couple of times; it‚Äôs not the end of the world.\nCommunicate this clearly when you report the results."
  },
  {
    "objectID": "slides/slides-08.html#mitigating-optimization-bias.",
    "href": "slides/slides-08.html#mitigating-optimization-bias.",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Mitigating optimization bias.",
    "text": "Mitigating optimization bias.\n\nCross-validation\nEnsembles\nRegularization and choosing a simpler model"
  },
  {
    "objectID": "slides/slides-08.html#questions-for-you",
    "href": "slides/slides-08.html#questions-for-you",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Questions for you",
    "text": "Questions for you\n\nYou have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n\nProbably\nProbably not"
  },
  {
    "objectID": "slides/slides-08.html#automated-hyperparameter-optimization-1",
    "href": "slides/slides-08.html#automated-hyperparameter-optimization-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Automated hyperparameter optimization",
    "text": "Automated hyperparameter optimization\n\nAdvantages\n\nReduce human effort\nLess prone to error and improve reproducibility\nData-driven approaches may be effective\n\nDisadvantages\n\nMay be hard to incorporate intuition\nBe careful about overfitting on the validation set"
  },
  {
    "objectID": "slides/slides-08.html#discussion",
    "href": "slides/slides-08.html#discussion",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Discussion",
    "text": "Discussion\nLet‚Äôs say that, for a particular feature, the histograms of that feature are identical for the two target classes. Does that mean the feature is not useful for predicting the target class?"
  },
  {
    "objectID": "slides/slides-01.html#learning-outcomes",
    "href": "slides/slides-01.html#learning-outcomes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nExplain the motivation behind study machine learning.\nBriefly describe supervised learning.\nDifferentiate between traditional programming and machine learning.\nAssess whether a given problem is suitable for a machine learning solution.\nNavigate through the course material.\nBe familiar with the policies and how the class is going to run."
  },
  {
    "objectID": "slides/slides-01.html#cpsc-330-website",
    "href": "slides/slides-01.html#cpsc-330-website",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 website",
    "text": "CPSC 330 website\n\n\n\nCourse Jupyter book: https://ubc-cs.github.io/cpsc330-2024W2/README.html\nCourse GitHub repository: https://github.com/UBC-CS/cpsc330-2024W2"
  },
  {
    "objectID": "slides/slides-01.html#meet-your-instructor",
    "href": "slides/slides-01.html#meet-your-instructor",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet your instructor",
    "text": "Meet your instructor\n\n\n\n\n\nAndrew Roth\nI am an Assistant Professor Departments of Computer Science and Pathology & Laboratory Medicine.\nI received my Ph.D.¬†in Bioinformatics at UBC.\nMy research uses statistical machine learning methods to study cancer.\nContact information\n\nEmail: aroth@cs.ubc.ca\nOffice: ICCS 359"
  },
  {
    "objectID": "slides/slides-01.html#meet-eva-a-fictitious-persona",
    "href": "slides/slides-01.html#meet-eva-a-fictitious-persona",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet Eva (a fictitious persona)!",
    "text": "Meet Eva (a fictitious persona)!\n\n\n\n\nEva is among one of you. She has some experience in Python programming. She knows machine learning as a buzz word. During her recent internship, she has developed some interest and curiosity in the field. She wants to learn what is it and how to use it. She is a curious person and usually has a lot of questions!"
  },
  {
    "objectID": "slides/slides-01.html#you-all",
    "href": "slides/slides-01.html#you-all",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "You all",
    "text": "You all\n\nIntroduce yourself to your neighbour.\nSince we‚Äôre going to spend the semester with each other, I would like to know you a bit better."
  },
  {
    "objectID": "slides/slides-01.html#asking-questions-during-class",
    "href": "slides/slides-01.html#asking-questions-during-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Asking questions during class",
    "text": "Asking questions during class\nYou are welcome to ask questions by raising your hand. There is also a reflection Google Document for this course for your questions/comments/reflections. It will be great if you can write about your takeaways, struggle points, and general comments in this document so that I‚Äôll try to address those points in the next lecture."
  },
  {
    "objectID": "slides/slides-01.html#activity-1-httpsshorturl.atd8v0w",
    "href": "slides/slides-01.html#activity-1-httpsshorturl.atd8v0w",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 1: https://shorturl.at/D8v0w",
    "text": "Activity 1: https://shorturl.at/D8v0w\n\n\n\nWrite your answers to the questions below in this Google doc: https://shorturl.at/D8v0w\nWhat do you know about machine learning?\nWhat would you like to get out of this course?\nAre there any particular topics or aspects of this course that you are especially excited or anxious about? Why?"
  },
  {
    "objectID": "slides/slides-01.html#spam-prediction",
    "href": "slides/slides-01.html#spam-prediction",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Spam prediction",
    "text": "Spam prediction\n\nSuppose you are given some data with labeled spam and non-spam messages\n\n\nCodeOutput\n\n\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(\n    sms_df, test_size=0.10, random_state=42\n)\n\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\nspam\nLookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\n\n\nham\nAight, I'll hit you up when I get some cash\n\n\nham\nDon no da:)whats you plan?\n\n\nham\nGoing to take your babe out ?\n\n\nham\nNo need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room."
  },
  {
    "objectID": "slides/slides-01.html#traditional-programming-vs.-ml",
    "href": "slides/slides-01.html#traditional-programming-vs.-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Traditional programming vs.¬†ML",
    "text": "Traditional programming vs.¬†ML\n\nImagine writing a Python program for spam identification, i.e., whether a text message or an email is spam or non-spam.\nTraditional programming\n\nCome up with rules using human understanding of spam messages.\nTime consuming and hard to come up with robust set of rules.\n\nMachine learning\n\nCollect large amount of data of spam and non-spam emails and let the machine learning algorithm figure out rules."
  },
  {
    "objectID": "slides/slides-01.html#lets-train-a-model",
    "href": "slides/slides-01.html#lets-train-a-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs train a model",
    "text": "Let‚Äôs train a model\n\nThere are several packages that help us perform machine learning.\n\n\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\nclf = make_pipeline(\n    CountVectorizer(max_features=5000), \n    LogisticRegression(max_iter=5000)\n)\nclf.fit(X_train, y_train); # Training the model"
  },
  {
    "objectID": "slides/slides-01.html#unseen-messages",
    "href": "slides/slides-01.html#unseen-messages",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Unseen messages",
    "text": "Unseen messages\n\nNow use the trained model to predict targets of unseen messages:\n\n\n\n\n\n\n\n\n¬†\nsms\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok."
  },
  {
    "objectID": "slides/slides-01.html#predicting-on-unseen-data",
    "href": "slides/slides-01.html#predicting-on-unseen-data",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting on unseen data",
    "text": "Predicting on unseen data\nThe model is accurately predicting labels for the unseen text messages above!\n\npred_dict = {\n    \"sms\": X_test[0:4],\n    \"spam_predictions\": clf.predict(X_test[0:4]),\n}\npd.DataFrame(pred_dict)\n\n\n\n\n\n\n\n\nsms\nspam_predictions\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\nham\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one m...\nham\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\nspam\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\nham"
  },
  {
    "objectID": "slides/slides-01.html#a-different-way-to-solve-problems",
    "href": "slides/slides-01.html#a-different-way-to-solve-problems",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "A different way to solve problems",
    "text": "A different way to solve problems\nMachine learning uses computer programs to model data. It can be used to extract hidden patterns, make predictions in new situation, or generate novel content.\n\nA field of study that gives computers the ability to learn without being explicitly programmed.  ‚Äì Arthur Samuel (1959)"
  },
  {
    "objectID": "slides/slides-01.html#ml-vs.-traditional-programming",
    "href": "slides/slides-01.html#ml-vs.-traditional-programming",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ML vs.¬†traditional programming",
    "text": "ML vs.¬†traditional programming\n\nWith machine learning, you‚Äôre likely to\n\nSave time\nCustomize and scale products"
  },
  {
    "objectID": "slides/slides-01.html#prevalence-of-ml",
    "href": "slides/slides-01.html#prevalence-of-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Prevalence of ML",
    "text": "Prevalence of ML\nLet‚Äôs look at some examples."
  },
  {
    "objectID": "slides/slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "href": "slides/slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity: For what type of problems ML is appropriate? (~5 mins)",
    "text": "Activity: For what type of problems ML is appropriate? (~5 mins)\nDiscuss with your neighbour for which of the following problems you would use machine learning\n\nFinding a list of prime numbers up to a limit\nGiven an image, automatically identifying and labeling objects in the image\nFinding the distance between two nodes in a graph"
  },
  {
    "objectID": "slides/slides-01.html#types-of-machine-learning",
    "href": "slides/slides-01.html#types-of-machine-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nHere are some typical learning problems.\n\nSupervised learning (Gmail spam filtering)\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nUnsupervised learning (Google News)\n\nTraining a model to find patterns in a dataset, typically an unlabeled dataset.\n\nReinforcement learning (AlphaGo)\n\nA family of algorithms for finding suitable actions to take in a given situation in order to maximize a reward.\n\nRecommendation systems (Amazon item recommendation system)\n\nPredict the ‚Äúrating‚Äù or ‚Äúpreference‚Äù a user would give to an item."
  },
  {
    "objectID": "slides/slides-01.html#what-is-supervised-learning",
    "href": "slides/slides-01.html#what-is-supervised-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is supervised learning?",
    "text": "What is supervised learning?\n\nTraining data comprises a set of observations (X) and their corresponding targets (y).\nWe wish to find a model function f that relates X to y.\nWe use the model function to predict targets of new examples."
  },
  {
    "objectID": "slides/slides-01.html#evas-questions",
    "href": "slides/slides-01.html#evas-questions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ü§î Eva‚Äôs questions",
    "text": "ü§î Eva‚Äôs questions\nAt this point, Eva is wondering about many questions.\n\nHow are we exactly ‚Äúlearning‚Äù whether a message is spam and ham?\nAre we expected to get correct predictions for all possible messages? How does it predict the label for a message it has not seen before?\n\nWhat if the model mis-labels an unseen example? For instance, what if the model incorrectly predicts a non-spam as a spam? What would be the consequences?\nHow do we measure the success or failure of spam identification?\nIf you want to use this model in the wild, how do you know how reliable it is?\n\nWould it be useful to know how confident the model is about the predictions rather than just a yes or a no?\n\n\nIt‚Äôs great to think about these questions right now. But Eva has to be patient. By the end of this course you‚Äôll know answers to many of these questions!"
  },
  {
    "objectID": "slides/slides-01.html#predicting-labels-of-a-given-image",
    "href": "slides/slides-01.html#predicting-labels-of-a-given-image",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting labels of a given image",
    "text": "Predicting labels of a given image\n\nWe can also use machine learning to predict labels of given images using a technique called transfer learning.\n\n\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\n                     tiger cat              0.636\n              tabby, tabby cat              0.174\nPembroke, Pembroke Welsh corgi              0.081\n               lynx, catamount              0.011\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.994\n                  leopard, Panthera pardus              0.005\njaguar, panther, Panthera onca, Felis onca              0.001\n       snow leopard, ounce, Panthera uncia              0.000\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                   Class  Probability score\n                                 macaque              0.885\npatas, hussar monkey, Erythrocebus patas              0.062\n      proboscis monkey, Nasalis larvatus              0.015\n                       titi, titi monkey              0.010\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                        Class  Probability score\nWalker hound, Walker foxhound              0.582\n             English foxhound              0.144\n                       beagle              0.068\n                  EntleBucher              0.059\n--------------------------------------------------------------"
  },
  {
    "objectID": "slides/slides-01.html#predicting-housing-prices",
    "href": "slides/slides-01.html#predicting-housing-prices",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting housing prices",
    "text": "Predicting housing prices\nSuppose we want to predict housing prices given a number of attributes associated with houses. The target here is continuous and not discrete.\n\n\n\n\n\ntarget\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n509000.0\n2\n1.50\n1930\n3521\n2.0\n0\n0\n3\n8\n1930\n0\n1989\n0\n98007\n47.6092\n-122.146\n1840\n3576\n\n\n675000.0\n5\n2.75\n2570\n12906\n2.0\n0\n0\n3\n8\n2570\n0\n1987\n0\n98075\n47.5814\n-122.050\n2580\n12927\n\n\n420000.0\n3\n1.00\n1150\n5120\n1.0\n0\n0\n4\n6\n800\n350\n1946\n0\n98116\n47.5588\n-122.392\n1220\n5120\n\n\n680000.0\n8\n2.75\n2530\n4800\n2.0\n0\n0\n4\n7\n1390\n1140\n1901\n0\n98112\n47.6241\n-122.305\n1540\n4800\n\n\n357823.0\n3\n1.50\n1240\n9196\n1.0\n0\n0\n3\n8\n1240\n0\n1968\n0\n98072\n47.7562\n-122.094\n1690\n10800"
  },
  {
    "objectID": "slides/slides-01.html#building-a-regression-model",
    "href": "slides/slides-01.html#building-a-regression-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Building a regression model",
    "text": "Building a regression model\n\nfrom lightgbm.sklearn import LGBMRegressor\n\nX_train, y_train = train_df.drop(columns= [\"target\"]), train_df[\"target\"]\nX_test, y_test = test_df.drop(columns= [\"target\"]), train_df[\"target\"]\n\nmodel = LGBMRegressor()\nmodel.fit(X_train, y_train);\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2333\n[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 18\n[LightGBM] [Info] Start training from score 539762.702545"
  },
  {
    "objectID": "slides/slides-01.html#predicting-prices-of-unseen-houses",
    "href": "slides/slides-01.html#predicting-prices-of-unseen-houses",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting prices of unseen houses",
    "text": "Predicting prices of unseen houses\n\n\n\n\n\nPredicted_target\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n345831.740542\n4\n2.25\n2130\n8078\n1.0\n0\n0\n4\n7\n1380\n750\n1977\n0\n98055\n47.4482\n-122.209\n2300\n8112\n\n\n601042.018745\n3\n2.50\n2210\n7620\n2.0\n0\n0\n3\n8\n2210\n0\n1994\n0\n98052\n47.6938\n-122.130\n1920\n7440\n\n\n311310.186024\n4\n1.50\n1800\n9576\n1.0\n0\n0\n4\n7\n1800\n0\n1977\n0\n98045\n47.4664\n-121.747\n1370\n9576\n\n\n597555.592401\n3\n2.50\n1580\n1321\n2.0\n0\n2\n3\n8\n1080\n500\n2014\n0\n98107\n47.6688\n-122.402\n1530\n1357\n\n\n\n\n\nWe are predicting continuous values here as apposed to discrete values in spam vs.¬†ham example."
  },
  {
    "objectID": "slides/slides-01.html#machine-learning-workflow",
    "href": "slides/slides-01.html#machine-learning-workflow",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Machine learning workflow",
    "text": "Machine learning workflow\nSupervised machine learning is quite flexible; it can be used on a variety of problems and different kinds of data. Here is a typical workflow of a supervised machine learning systems.\n\n\n\n\n\n\nWe will build machine learning pipelines in this course, focusing on some of the steps above."
  },
  {
    "objectID": "slides/slides-01.html#questions-for-you",
    "href": "slides/slides-01.html#questions-for-you",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "‚ùì‚ùì Questions for you",
    "text": "‚ùì‚ùì Questions for you\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are True (iClicker)\n\n\nPredicting spam is an example of machine learning.\n\n\nPredicting housing prices is not an example of machine learning.\n\n\nFor problems such as spelling correction, translation, face recognition, spam identification, if you are a domain expert, it‚Äôs usually faster and scalable to come up with a robust set of rules manually rather than building a machine learning model.\n\n\nIf you are asked to write a program to find all prime numbers up to a limit, it is better to implement one of the algorithms for doing so rather than using machine learning.\n\n\nGoogle News is likely be using machine learning to organize news."
  },
  {
    "objectID": "slides/slides-01.html#about-this-course",
    "href": "slides/slides-01.html#about-this-course",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "About this course",
    "text": "About this course\n\n\n\n\n\n\nImportant\n\n\nCourse website: https://github.com/UBC-CS/cpsc330-2024W2 is the most important link. Please read everything on this GitHub page!\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMake sure you go through the syllabus thoroughly and complete the syllabus quiz before Friday, Jan 24th at 11:59pm."
  },
  {
    "objectID": "slides/slides-01.html#cpsc-330-vs.-340",
    "href": "slides/slides-01.html#cpsc-330-vs.-340",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 vs.¬†340",
    "text": "CPSC 330 vs.¬†340\nRead https://github.com/UBC-CS/cpsc330-2024W2/blob/main/docs/330_vs_340.md which explains the difference between two courses.\nTLDR:\n\n340: how do ML models work?\n330: how do I use ML models?\nCPSC 340 has many prerequisites.\nCPSC 340 goes deeper but has a more narrow scope.\nI think CPSC 330 will be more useful if you just plan to apply basic ML."
  },
  {
    "objectID": "slides/slides-01.html#registration-waitlist-and-prerequisites",
    "href": "slides/slides-01.html#registration-waitlist-and-prerequisites",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Registration, waitlist and prerequisites",
    "text": "Registration, waitlist and prerequisites\n\n\n\n\n\n\nImportant\n\n\nPlease go through this document carefully before contacting your instructors about these issues. Even then, we are very unlikely to be able to help with registration, waitlist or prerequisite issues.\n\n\n\n\nIf you are on waitlist and if you‚Äôd like to try your chances, you should be able to access Canvas and Piazza.\n\nIf you‚Äôre unable to make it this time, this course will be offered in the summer."
  },
  {
    "objectID": "slides/slides-01.html#lecture-format",
    "href": "slides/slides-01.html#lecture-format",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture format",
    "text": "Lecture format\n\nIn person lectures Tu/Th.\nSometimes there will be videos to watch before lecture. You will find the list of pre-watch videos in the schedule on the course webpage.\nWe will also try to work on some questions and exercises together during the class.\nAll materials will be posted in this GitHub repository.\nWeekly Tu tutorial be office hours format run by the TAs.\nWeekly Th/Fr tutorials will be worksheet format run by the TAs.\nTutorials are completely optional.\n\nYou do not need to be registered in a tutorial.\nYou can attend whatever tutorials or office hours your want, regardless of in which/whether you‚Äôre registered."
  },
  {
    "objectID": "slides/slides-01.html#home-work-assignments",
    "href": "slides/slides-01.html#home-work-assignments",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Home work assignments",
    "text": "Home work assignments\n\nFirst homework assignment is due this coming Tuesday, January 14, midnight.\n\nThis is a relatively straightforward assignment on Python.\nIf you struggle with this assignment then that could be a sign that you will struggle later on in the course.\n\n\nYou must do the first two homework assignments on your own."
  },
  {
    "objectID": "slides/slides-01.html#exams",
    "href": "slides/slides-01.html#exams",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Exams",
    "text": "Exams\n\nWe‚Äôll have two self-scheduled midterms and one final in Computer-based Testing Facility (CBTF)."
  },
  {
    "objectID": "slides/slides-01.html#course-structure",
    "href": "slides/slides-01.html#course-structure",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course structure",
    "text": "Course structure\n\nIntroduction\n\nWeek 1\n\nPart I: ML fundamentals, preprocessing, midterm 1\n\nWeeks 2, 3, 4, 5, 6, 7, 8\n\nPart II: Unsupervised learning, transfer learning, common special cases, midterm 2\n\nWeeks 8, 9, 10, 11, 12\n\nPart III: Communication and ethics\n\nML skills are not beneficial if you can‚Äôt use them responsibly and communicate your results. In this module we‚Äôll talk about these aspects.\nWeeks 13, 14"
  },
  {
    "objectID": "slides/slides-01.html#code-of-conduct",
    "href": "slides/slides-01.html#code-of-conduct",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Code of conduct",
    "text": "Code of conduct\n\nOur main forum for getting help will be Piazza.\n\n\n\n\n\n\n\nImportant\n\n\nPlease read this entire document about asking for help. TLDR: Be nice."
  },
  {
    "objectID": "slides/slides-01.html#homework-format-jupyter-notebooks",
    "href": "slides/slides-01.html#homework-format-jupyter-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Homework format: Jupyter notebooks",
    "text": "Homework format: Jupyter notebooks\n\nOur notes are created in a Jupyter notebook, with file extension .ipynb.\nAlso, you will complete your homework assignments using Jupyter notebooks.\nConfusingly, ‚ÄúJupyter notebook‚Äù is also the original application that opens .ipynb files - but has since been replaced by Jupyter lab.\n\nI am using Jupyter lab, some things might not work with the Jupyter notebook application.\nYou can also open these files in Visual Studio Code."
  },
  {
    "objectID": "slides/slides-01.html#jupyter-notebooks",
    "href": "slides/slides-01.html#jupyter-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\n\nNotebooks contain a mix of code, code output, markdown-formatted text (including LaTeX equations), and more.\nWhen you open a Jupyter notebook in one of these apps, the document is ‚Äúlive‚Äù, meaning you can run the code.\n\nFor example:\n\n1 + 1\n\n2\n\n\n\nx = [1, 2, 3]\nx[0] = 9999\nx\n\n[9999, 2, 3]"
  },
  {
    "objectID": "slides/slides-01.html#jupyter",
    "href": "slides/slides-01.html#jupyter",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter",
    "text": "Jupyter\n\nBy default, Jupyter prints out the result of the last line of code, so you don‚Äôt need as many print statements.\nIn addition to the ‚Äúlive‚Äù notebooks, Jupyter notebooks can be statically rendered in the web browser.\n\nThis can be convenient for quick read-only access, without needing to launch the Jupyter notebook/lab application.\nBut you need to launch the app properly to interact with the notebooks."
  },
  {
    "objectID": "slides/slides-01.html#lecture-notes",
    "href": "slides/slides-01.html#lecture-notes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture notes",
    "text": "Lecture notes\n\nAll the lectures are available here.\nWhile the lecture notes are largely fixed, there may be small changes over the course of the term.\nA ‚Äúfinalized‚Äù version will be pushed to GitHub and the Jupyter book right before each class.\nEach instructor will have slightly adapted versions of notes to present slides during lectures.\n\nYou will find the link to these slides in our repository: https://github.com/UBC-CS/cpsc330-2024W2/tree/main/lectures."
  },
  {
    "objectID": "slides/slides-01.html#grades",
    "href": "slides/slides-01.html#grades",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Grades",
    "text": "Grades\n\nThe grading breakdown is here.\nThe policy on challenging grades is here."
  },
  {
    "objectID": "slides/slides-01.html#recommended-browser-and-tools",
    "href": "slides/slides-01.html#recommended-browser-and-tools",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Recommended browser and tools",
    "text": "Recommended browser and tools\n\nYou can install Chrome here.\nYou can install Firefox here.\n\nIn this course, we will primarily be using Python , git, GitHub, Canvas, Gradescope, Piazza, and PrairieLearn."
  },
  {
    "objectID": "slides/slides-01.html#course-conda-environment",
    "href": "slides/slides-01.html#course-conda-environment",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course conda environment",
    "text": "Course conda environment\n\nFollow the setup instructions here to create a course conda environment on your computer.\nIf you do not have your computer with you, you can partner up with someone and set up your own computer later."
  },
  {
    "objectID": "slides/slides-01.html#python-requirementsresources",
    "href": "slides/slides-01.html#python-requirementsresources",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Python requirements/resources",
    "text": "Python requirements/resources\nWe will primarily use Python in this course.\nHere is the basic Python knowledge you‚Äôll need for the course:\n\nBasic Python programming\nNumpy\nPandas\nBasic matplotlib\nSparse matrices\n\nHomework 1 is all about Python.\n\n\n\n\n\n\nNote\n\n\nWe do not have time to teach all the Python we need but you can find some useful Python resources here."
  },
  {
    "objectID": "slides/slides-01.html#checklist-for-you-before-the-next-class",
    "href": "slides/slides-01.html#checklist-for-you-before-the-next-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Checklist for you before the next class",
    "text": "Checklist for you before the next class\n\nAre you able to access course Canvas shell?\nAre you able to access course Piazza?\nAre you able to access Gradescope? (If not, refer to the Gradescope Student Guide.)\n\nPlease join Gradescope through the Canvas.\n\nAre you able to access iClicker Cloud for this course?\nDid you follow the setup instructions here to create a course conda environment on your computer?\nDid you complete the syllabus quiz on Prairilearn? (Due date: Friday, January 24th at 11:59pm)\nAre you almost finished or at least started with homework 1? (Due: Tuesday, January 14th at 11:59pm)"
  },
  {
    "objectID": "slides/slides-01.html#summary",
    "href": "slides/slides-01.html#summary",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Summary",
    "text": "Summary\n\nMachine learning is increasingly being applied across various fields.\nIn supervised learning, we are given a set of observations (X) and their corresponding targets (y) and we wish to find a model function that relates X to y.\nMachine learning is a different paradigm for problem solving. Very often it reduces the time you spend programming and helps customizing and scaling your products.\nBefore applying machine learning to a problem, it‚Äôs always advisable to assess whether a given problem is suitable for a machine learning solution or not."
  },
  {
    "objectID": "slides/slides-02.html#announcements",
    "href": "slides/slides-02.html#announcements",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Announcements",
    "text": "Announcements\n\nSlide link https://aroth85.github.io/cpsc330-slides\nThings due this week\n\nHomework 1 (hw1): Due Jan 14 11:59pm\n\nHomework 2 (hw2) has been released (Due: Jan 20, 11:59pm)\n\nThere is some autograding in this homework.\n\nYou can find the tentative due dates for all deliverables here.\nPlease monitor Piazza (especially pinned posts and instructor posts) for announcements.\nI‚Äôll assume that you‚Äôve watched the pre-lecture videos."
  },
  {
    "objectID": "slides/slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "href": "slides/slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Suggested workflow for working with Jupyter Notebooks",
    "text": "Suggested workflow for working with Jupyter Notebooks\n\nCreate a folder on your computer that will have all the CPSC 330 repos:\n\n~/School/Year3/CPSC330/ &lt;‚Äì Consider this your CPSC parent folder\n\nCreate subfolders for: hw, class, practice\nIn the hw folder, you will then clone hw1, hw2, hw3, etc‚Ä¶\nIn the class folder, you will clone the CPSC330-2024W2 repo which contains all the class jupyter notebooks\n\nDo not make any changes to files in this directory/repo, you will have trouble when you pull stuff during each class.\nIf you did make changes, you can reset to the last commit and DESTROY any changes you made (be careful with this command) using: git reset --hard\n\nIn the practice folder, you can copy any notebooks (.ipynb) and files (like data/*.csv) you want to try running locally and experiment"
  },
  {
    "objectID": "slides/slides-02.html#learning-outcomes",
    "href": "slides/slides-02.html#learning-outcomes",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nFrom this lecture, you will be able to\n\nIdentify whether a given problem could be solved using supervised machine learning or not;\nDifferentiate between supervised and unsupervised machine learning;\nExplain machine learning terminology such as features, targets, predictions, training, and error;\nDifferentiate between classification and regression problems;"
  },
  {
    "objectID": "slides/slides-02.html#learning-outcomes-contd",
    "href": "slides/slides-02.html#learning-outcomes-contd",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Learning outcomes (contd)",
    "text": "Learning outcomes (contd)\n\nUse DummyClassifier and DummyRegressor as baselines for machine learning problems;\nExplain the fit and predict paradigm and use score method of ML models;\nBroadly describe how decision tree prediction works;\nUse DecisionTreeClassifier and DecisionTreeRegressor to build decision trees using scikit-learn;\nVisualize decision trees;\nExplain the difference between parameters and hyperparameters;\nExplain the concept of decision boundaries;\nExplain the relation between model complexity and decision boundaries."
  },
  {
    "objectID": "slides/slides-02.html#big-picture",
    "href": "slides/slides-02.html#big-picture",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Big picture",
    "text": "Big picture\nIn this lecture, we‚Äôll talk about our first machine learning model: Decision trees. We will also familiarize ourselves with some common terminology in supervised machine learning."
  },
  {
    "objectID": "slides/slides-02.html#recap-what-is-ml",
    "href": "slides/slides-02.html#recap-what-is-ml",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: What is ML?",
    "text": "Recap: What is ML?\n\nML uses data to build models that find patterns, make predictions, or generate content.\nIt helps computers learn from data to make decisions.\nNo one model works for every situation."
  },
  {
    "objectID": "slides/slides-02.html#recap-supervised-learning",
    "href": "slides/slides-02.html#recap-supervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: Supervised learning",
    "text": "Recap: Supervised learning\n\nWe wish to find a model function f that relates X to y.\nWe use the model function to predict targets of new examples.\n\n\n\n\n\n\nIn the first part of this course, we‚Äôll focus on supervised machine learning."
  },
  {
    "objectID": "slides/slides-02.html#unsupervised-learning",
    "href": "slides/slides-02.html#unsupervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nIn unsupervised learning training data consists of observations (X) without any corresponding targets.\nUnsupervised learning could be used to group similar things together in X or to provide concise summary of the data.\n\n\n\n\n\n\nWe‚Äôll learn more about this topic later."
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.1-supervised-vs-unsupervised",
    "href": "slides/slides-02.html#iclicker-2.1-supervised-vs-unsupervised",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.1: Supervised vs unsupervised",
    "text": "iClicker 2.1: Supervised vs unsupervised\nClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are examples of supervised machine learning\n\n\nFinding groups of similar properties in a real estate data set.\n\n\nPredicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n\n\nGrouping articles on different topics from different news sources (something like the Google News app).\n\n\nDetecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n\n\nGiven some measure of employee performance, identify the key factors which are likely to influence their performance."
  },
  {
    "objectID": "slides/slides-02.html#framework",
    "href": "slides/slides-02.html#framework",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Framework",
    "text": "Framework\n\nThere are many frameworks to do do machine learning.\nWe‚Äôll mainly be using scikit-learn framework."
  },
  {
    "objectID": "slides/slides-02.html#running-example",
    "href": "slides/slides-02.html#running-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Running example",
    "text": "Running example\nImagine you‚Äôre in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people. Here are the first few rows of this toy dataset.\n\n\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02.html#features-target-example",
    "href": "slides/slides-02.html#features-target-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Features, target, example",
    "text": "Features, target, example\n\nWhat are the features X?\n\nfeatures = inputs = predictors = explanatory variables = regressors = independent variables = covariates\n\nWhat‚Äôs the target y?\n\ntarget = output = outcome = response variable = dependent variable = labels\n\nCan you think of other relevant features for the job happiness problem?"
  },
  {
    "objectID": "slides/slides-02.html#classification-vs.-regression",
    "href": "slides/slides-02.html#classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs.¬†Regression",
    "text": "Classification vs.¬†Regression\nIn supervised machine learning, there are two main kinds of learning problems based on what they are trying to predict.\n\nClassification problem: predicting among two or more discrete classes\n\nExample1: Predict whether a patient has a liver disease or not\nExample2: Predict whether a student would get an A+ or not in quiz2.\n\nRegression problem: predicting a continuous value\n\nExample1: Predict housing prices\nExample2: Predict a student‚Äôs score in quiz2."
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.2-classification-vs.-regression",
    "href": "slides/slides-02.html#iclicker-2.2-classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.2: Classification vs.¬†Regression",
    "text": "iClicker 2.2: Classification vs.¬†Regression\nClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are examples of regression problems\n\n\nPredicting the price of a house based on features such as number of bedrooms and the year built.\n\n\nPredicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n\n\nPredicting percentage grade in CPSC 330 based on past grades.\n\n\nPredicting whether you should bicycle tomorrow or not based on the weather forecast.\n\n\nPredicting appropriate thermostat temperature based on the wind speed and the number of people in a room."
  },
  {
    "objectID": "slides/slides-02.html#classification-vs.-regression-1",
    "href": "slides/slides-02.html#classification-vs.-regression-1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs.¬†Regression",
    "text": "Classification vs.¬†Regression\n\nIs this a classification problem or a regression problem?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02.html#prediction-vs.-inference",
    "href": "slides/slides-02.html#prediction-vs.-inference",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction vs.¬†Inference",
    "text": "Prediction vs.¬†Inference\n\nInference is using the model to understand the relationship between the features and the target\n\nWhy certain factors influence happiness?\n\nPrediction is using the model to predict the target value for new examples based on learned patterns.\nOf course these goals are related, and in many situations we need both."
  },
  {
    "objectID": "slides/slides-02.html#training",
    "href": "slides/slides-02.html#training",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training",
    "text": "Training\n\nIn supervised ML, the goal is to learn a function that maps input features (X) to a target (y).\nThe relationship between X and y is often complex, making it difficult to define mathematically.\nWe use algorithms to approximate this complex relationship between X and y.\nTraining is the process of applying an algorithm to learn the best function (or model) that maps X to y.\nIn this course, I‚Äôll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline."
  },
  {
    "objectID": "slides/slides-02.html#separating-x-and-y",
    "href": "slides/slides-02.html#separating-x-and-y",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Separating X and y",
    "text": "Separating X and y\n\nIn order to train a model we need to separate X and y from the dataframe.\n\n\n# Extract the feature set by removing the target column \"happy?\"\nX = toy_happiness_df.drop(columns=[\"happy?\"])\n# Extract the target variable \"happy?\"\ny = toy_happiness_df[\"happy?\"]"
  },
  {
    "objectID": "slides/slides-02.html#baseline",
    "href": "slides/slides-02.html#baseline",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Baseline",
    "text": "Baseline\n\nLet‚Äôs try a simplest algorithm of predicting the most popular target!\n\n\nfrom sklearn.dummy import DummyClassifier\n# Initialize the DummyClassifier to always predict the most frequent class\nmodel = DummyClassifier(strategy=\"most_frequent\") \n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\n# Add the predicted values as a new column in the dataframe\ntoy_happiness_df['dummy_predictions'] = model.predict(X) \ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\ndummy_predictions\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\nHappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\nHappy\n\n\n2\n1\n80000\n1\n0\nHappy\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy\nHappy"
  },
  {
    "objectID": "slides/slides-02.html#score-your-model",
    "href": "slides/slides-02.html#score-your-model",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "score your model",
    "text": "score your model\n\nHow do you know how well your model is doing?\nFor classification problems, by default, score gives the accuracy of the model, i.e., proportion of correctly predicted targets.\n\n\nmodel.score(X, y)\n\n0.5714285714285714"
  },
  {
    "objectID": "slides/slides-02.html#steps-to-train-a-classifier-using-sklearn",
    "href": "slides/slides-02.html#steps-to-train-a-classifier-using-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Steps to train a classifier using sklearn",
    "text": "Steps to train a classifier using sklearn\n\nRead the data\nCreate X and y\nCreate a classifier object\nfit the classifier\npredict on new examples\nscore the model"
  },
  {
    "objectID": "slides/slides-02.html#dummyregressor",
    "href": "slides/slides-02.html#dummyregressor",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "DummyRegressor",
    "text": "DummyRegressor\nYou can also do the same thing for regression problems using DummyRegressor, which predicts mean, median, or constant value of the training set for all examples."
  },
  {
    "objectID": "slides/slides-02.html#pre-intuition",
    "href": "slides/slides-02.html#pre-intuition",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Pre-Intuition",
    "text": "Pre-Intuition\nLet‚Äôs play 20 questions! You can ask me up to 20 Yes/No questions to figure out the answer."
  },
  {
    "objectID": "slides/slides-02.html#intuition",
    "href": "slides/slides-02.html#intuition",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Intuition",
    "text": "Intuition\n\nDecision trees find the ‚Äúbest‚Äù way to split data to make predictions.\nEach split is based on a question, like ‚ÄòAre the colleagues supportive?‚Äô\nThe goal is to group data by similar outcomes at each step.\nNow, let‚Äôs see a decision tree using sklearn."
  },
  {
    "objectID": "slides/slides-02.html#decision-tree-with-sklearn",
    "href": "slides/slides-02.html#decision-tree-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree with sklearn",
    "text": "Decision tree with sklearn\nLet‚Äôs train a simple decision tree on our toy dataset.\n\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\n# Create a decision tree object\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1)\n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\nplot_tree(\n  model, filled=True, feature_names=X.columns, \n  class_names=[\"Happy\", \"Unhappy\"], impurity=False, fontsize=12\n);"
  },
  {
    "objectID": "slides/slides-02.html#prediction",
    "href": "slides/slides-02.html#prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction",
    "text": "Prediction\n\nGiven a new example, how does a decision tree predict the class of this example?\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,"
  },
  {
    "objectID": "slides/slides-02.html#prediction-with-sklearn",
    "href": "slides/slides-02.html#prediction-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction with sklearn",
    "text": "Prediction with sklearn\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,\n\n\n\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\n\nModel prediction:  ['Unhappy']"
  },
  {
    "objectID": "slides/slides-02.html#training-high-level",
    "href": "slides/slides-02.html#training-high-level",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training (high level)",
    "text": "Training (high level)\n\nHow many possible questions could we ask in this context?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\n\n\n\n\n0\n0\n70000\n0\n1\n\n\n1\n1\n60000\n0\n0\n\n\n2\n1\n80000\n1\n0\n\n\n3\n1\n110000\n0\n1\n\n\n4\n1\n120000\n1\n0\n\n\n5\n1\n150000\n1\n1\n\n\n6\n0\n150000\n1\n0"
  },
  {
    "objectID": "slides/slides-02.html#training-high-level-1",
    "href": "slides/slides-02.html#training-high-level-1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training (high level)",
    "text": "Training (high level)\n\nDecision tree learning is a search process to find the ‚Äúbest‚Äù tree among many possible ones.\nWe evaluate questions using measures like information gain or the Gini index to find the most effective split.\nAt each step, we aim to split the data into groups with more certainty in their outcomes."
  },
  {
    "objectID": "slides/slides-02.html#parameters-vs.-hyperparameters",
    "href": "slides/slides-02.html#parameters-vs.-hyperparameters",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Parameters vs.¬†Hyperparameters",
    "text": "Parameters vs.¬†Hyperparameters\n\nParameters\n\nThe questions (features and thresholds) used to split the data at each node.\nExample: salary &lt;= 75000 at the root node\n\n\nHyperparameters\n\nSettings that control tree growth, like max_depth, which limits how deep the tree can go."
  },
  {
    "objectID": "slides/slides-02.html#decision-boundary-with-max_depth1",
    "href": "slides/slides-02.html#decision-boundary-with-max_depth1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=1",
    "text": "Decision boundary with max_depth=1"
  },
  {
    "objectID": "slides/slides-02.html#decision-boundary-with-max_depth2",
    "href": "slides/slides-02.html#decision-boundary-with-max_depth2",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=2",
    "text": "Decision boundary with max_depth=2"
  },
  {
    "objectID": "slides/slides-02.html#iclicker-2.3-baselines-and-decision-trees",
    "href": "slides/slides-02.html#iclicker-2.3-baselines-and-decision-trees",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.3: Baselines and Decision trees",
    "text": "iClicker 2.3: Baselines and Decision trees\niClicker cloud join link: https://join.iclicker.com/HTRZ\nSelect all of the following statements which are TRUE.\n\n\nChange in features (i.e., binarizing features above) would change DummyClassifier predictions.\n\n\nPredict takes only X as argument whereas fit and score take both X and y as arguments.\n\n\nFor the decision tree algorithm to work, the feature values must be binary.\n\n\nThe prediction in a decision tree works by routing the example from the root to the leaf."
  },
  {
    "objectID": "slides/slides-02.html#question-for-next-lecture-decision-boundaries",
    "href": "slides/slides-02.html#question-for-next-lecture-decision-boundaries",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Question for next lecture: Decision boundaries",
    "text": "Question for next lecture: Decision boundaries\nSelect the TRUE statement.\n\n\nThe decision boundary in the image below could come from a decision tree.\n\n\nThe decision boundary in the image below could not come from a decision tree.\n\n\nThere is not enough information to determine if a decision tree could create this boundary."
  },
  {
    "objectID": "slides/slides-02.html#what-we-learned-today",
    "href": "slides/slides-02.html#what-we-learned-today",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "What we learned today?",
    "text": "What we learned today?\n\nThere is a lot of terminology and jargon used in ML.\nSteps to train a supervised machine learning model.\nWhat baselines are and why they are useful.\nWhat decision trees are and how they work intuitively.\nWhat decision boundaries are."
  }
]