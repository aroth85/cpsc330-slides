---
title: 'Lecture 4: $k$-nearest neighbours and SVM RBFs'
author: "Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)"
description: Supervised Machine Learning Fundamentals
description-short: 'introduction to KNNs, hyperparameter `n_neighbours` or $k$, `C` and `gamma` hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.'
format:
  revealjs:
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Announcements 

```{python}
from panel import widgets
from panel.interact import interact
from sklearn.model_selection import cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

import matplotlib
import matplotlib.pyplot as plt
import mglearn
import panel as pn

import os
import pathlib
import sys

sys.path.append(os.path.join("code"))
from plotting_functions import *
from utils import *

pn.extension()
DATA_DIR = pathlib.Path("data")
```

- Homework 2 due Jan 20
- Syllabus quiz due date Jan 24 
- The lecture notes within these notebooks align with the content presented in the videos. 
Even though we do not cover all the content from these notebooks during lectures, it’s your responsibility to go through them on your own.

## Learning outcomes

From this lecture, you will be able to 

- Describe the curse of dimensionality
- Explain the notion of similarity-based algorithms
- Describe how $k$-NNs and SVMs with RBF kernel work
- Describe the effects of hyper-parameters for $k$-NNs and SVMs

## Recap

Which of the following scenarios do **NOT necessarily imply overfitting**? 

- (A) Training accuracy is 0.98 while validation accuracy is 0.60.
- (B) The model is too specific to the training data. 
- (C) The decision boundary of a classifier is wiggly and highly irregular.
- (D) Training and validation accuracies are both approximately 0.88. 

## Recap

Which of the following statements about **overfitting** is true? 

- (A) Overfitting is always beneficial for model performance on unseen data.
- (B) Some degree of overfitting is common in most real-world problems. 
- (C) Overfitting ensures the model will perform well in real-world scenarios.
- (D) Overfitting occurs when the model learns the training data too closely, including its noise and outliers.   


## Recap

How might one address the issue of **underfitting** in a machine learning model. 

- (A) Introduce more noise to the training data. 
- (B) Remove features that might be relevant to the prediction. 
- (C) Increase the model's complexity, possibly by adding more parameter or features
- (D) Use a smaller dataset for training. 

## Overfitting and underfitting 

- An **overfit model** matches the training set so closely that it fails to make correct predictions on new unseen data.  
- An **underfit model** is too simple and does not even make good predictions on the training data 

![](img/underfit-overfit-google-developer.png){fig-align="center"}

[Source](https://developers.google.com/machine-learning/crash-course/overfitting/overfitting)


## Recap 
- Why do we split the data? What are train/valid/test splits? 
- What are the benefits of cross-validation?
- What’s the fundamental trade-off in supervised machine learning?
- What is the golden rule of machine learning?


## Cross validation

![](img/cross-validation.png){fig-align="center"}


## Summary of train, validation, test, and deployment data 

|         | `fit` | `score` | `predict` |
|----------|-------|---------|-----------|
| Train    | ✔️      | ✔️      | ✔️         |
| Validation |      | ✔️      | ✔️         |
| Test    |       |  once   | once         |
| Deployment    |       |       | ✔️         |


## Recap: The fundamental tradeoff

As you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.  

![](img/malp_0201.png){fig-align="center"}


# Analogy-based models

## Motivation

![](img/knn-motivation.png){fig-align="center"}

## General idea of $k$-nearest neighbours algorithm

```{python}
X, y = mglearn.datasets.make_forge()
X_test = np.array([[8.2, 3.66214339], [9.9, 3.2], [11.2, 0.5]])

plot_train_test_points(X, y, X_test)
```

## Geometric view of tabular data and dimensions {.smaller}

```{python}
spotify_df = pd.read_csv(DATA_DIR.joinpath("spotify.csv"), index_col=0)
X_spotify = spotify_df.drop(columns=["target", "song_title", "artist"])
```

```{python}
#| echo: true
X_spotify.shape
```

```{python}
X_spotify.head()
```

## Distance between feature vectors 

- A common way to calculate the distance between vectors is calculating the **Euclidean distance**. 
- The euclidean distance between vectors $u = <u_1, u_2, \dots, u_n>$ and $v = <v_1, v_2, \dots, v_n>$ is defined as: 

$$distance(u, v) = \sqrt{\sum_{i =1}^{n} (u_i - v_i)^2}$$ 

# $k$-Nearest Neighbours ($k$-NNs)

## $k$-NNs example

```{python}
cities_df = pd.read_csv(DATA_DIR.joinpath("canada_usa_cities.csv"))
X_cities = cities_df[["longitude", "latitude"]]
y_cities = cities_df["country"]

small_cities = cities_df.sample(30, random_state=90)
one_city = small_cities.sample(1, random_state=44)
small_train_df = pd.concat([small_cities, one_city]).drop_duplicates(keep=False)

X_small_cities = small_train_df.drop(columns=["country"]).to_numpy()
y_small_cities = small_train_df["country"].to_numpy()
test_point = one_city[["longitude", "latitude"]].to_numpy()
```

```{python}
plot_train_test_points(
    X_small_cities,
    y_small_cities,
    test_point,
    class_names=["Canada", "USA"],
    test_format="circle",
)
```

## $k$-NNs (n=1)

```{python}
plot_knn_clf(
    X_small_cities,
    y_small_cities,
    test_point,
    n_neighbors=1,
    class_names=["Canada", "USA"],
    test_format="circle",
)
```

## $k$-NNs (n=3)

```{python}
plot_knn_clf(
    X_small_cities,
    y_small_cities,
    test_point,
    n_neighbors=3,
    class_names=["Canada", "USA"],
    test_format="circle",
)
```

## `KNeighborsClassifier`

```{python}
#| echo: true
for k in [1, 3]:
    neigh = KNeighborsClassifier(n_neighbors=k)
    neigh.fit(X_small_cities, y_small_cities)
    print(
        "Prediction of the black dot with %d neighbours: %s"
        % (k, neigh.predict(test_point))
    )
```

## Effect of $k$

```{python}
X = cities_df.drop(columns=["country"])
y = cities_df["country"]

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=123
)

plot_knn_decision_boundaries(X_train, y_train, k_values=[1, 11, 100], verbose=False)
```

## iClicker 4.1

iClicker cloud join link: https://join.iclicker.com/HTRZ

**Select all of the following statements which are TRUE.**

- (A) Analogy-based models find examples from the test set that are most similar to the query example we are predicting.
- (B) Euclidean distance will always have a non-negative value.
- (C) With $k$-NN, setting the hyperparameter $k$ to larger values typically reduces training error. 
- (D) Similar to decision trees, $k$-NNs finds a small set of good features.
- (E) In $k$-NN, with $k > 1$, the classification of the closest neighbour to the test example always contributes the most to the prediction.

## Regression with $k$-nearest neighbours ($k$-NNs)

```{python}
mglearn.plots.plot_knn_regression(n_neighbors=3)
```

# SVMs

## Curse of dimensionality

- Affects all learners but especially bad for nearest-neighbour. 
- $k$-NN usually works well when the number of dimensions $d$ is small but things fall apart quickly as $d$ goes up.
- If there are many irrelevant attributes, $k$-NN is hopelessly confused because all of them contribute to finding similarity between examples. 
- With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and $k$-NN is no better than random guessing. 

## Overview {.smaller}

- SVM RBFs are more like weighted $k$-NNs.
    - The decision boundary is defined by **a set of positive and negative examples** and **their weights** together with **their similarity measure**. 
    - A test example is labeled positive if on average it looks more like positive examples than the negative examples. 

- Difference between $k$-NNs and SVM RBFs
    - SVM RBFs only remember the key examples (support vectors). 
    - SVMs use a different similarity metric which is called a "kernel". A popular kernel is Radial Basis Functions (RBFs)
    - They usually perform better than $k$-NNs! 

## Decision boundary of SVMs 

```{python}
cities_df = pd.read_csv(DATA_DIR.joinpath("canada_usa_cities.csv"))
X_cities = cities_df[["longitude", "latitude"]]
y_cities = cities_df["country"]
X_train, X_test, y_train, y_test = train_test_split(
    X_cities, y_cities, test_size=0.2, random_state=123
)

knn = KNeighborsClassifier(n_neighbors=11)
svm = SVC(gamma=0.01)

fig, axes = plt.subplots(1, 2, figsize=(16, 5))

for i, (clf, ax) in enumerate(zip([knn, svm], axes)):
    clf.fit(X_train, y_train)
    mglearn.plots.plot_2d_separator(
        clf, X_train.to_numpy(), fill=True, eps=0.5, ax=ax, alpha=0.4
    )
    mglearn.discrete_scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], y_train, ax=ax)
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    if i == 0:
        ax.set_title("kNN")
    if i == 1:
        ax.set_title("SVM")
```

## Support vectors

**The decision boundary only depends on the support vectors.**

```{python}
plot_support_vectors(svm, X_train.to_numpy(), y_train.to_numpy())
```

## Relation of `gamma` and the fundamental trade-off

```{python}
gamma = [0.001, 0.01, 0.1]
plot_svc_gamma(
    gamma,
    X_train.to_numpy(),
    y_train.to_numpy(),
    x_label="Longitude",
    y_label="Latitude",
    verbose=False,
)
```

## Relation of `C` and the fundamental trade-off

```{python}
C = [0.1, 1.0, 100.0]
plot_svc_C(
    C, 
    X_train.to_numpy(), 
    y_train.to_numpy(), 
    x_label="Longitude", 
    y_label="Latitude", 
    verbose=False,
)
```

## iClicker 4.2

iClicker cloud join link: https://join.iclicker.com/HTRZ

**Select all of the following statements which are TRUE.**

- (A) $k$-NN may perform poorly in high-dimensional space (say, *d* > 1000). 
- (B) In sklearn’s SVC classifier, large values of `gamma` tend to result in higher training score but probably lower validation score. 
- (C) If we increase both `gamma` and `C`, we can't be certain if the model becomes more complex or less complex.

# Summary

## Similarity-based algorithms 
- Use similarity or distance metrics to predict targets.
- Examples: $k$-nearest neighbors, Support Vector Machines (SVMs) with RBF Kernel.

## $k$-nearest neighbours
- Classifies an object based on the majority label among its $k$ closest neighbors. 
- Main hyperparameter: $k$ or `n_neighbors` in `sklearn`
- Distance Metrics: Euclidean
- Strengths: Simple and intuitive, can learn complex decision boundaries
- Challenges: Sensitive to the choice of distance metric and **scaling** (coming up).


## Curse of dimensionality 
- As dimensionality increases, the volume of the space increases exponentially, making the data sparse.
- Distance metrics lose meaning
    - Accidental similarity swamps out meaningful similarity
    - All points become almost equidistant.
- Overfitting becomes likely: Harder to generalize with high-dimensional data.
- How to deal with this? 
    - Dimensionality reduction (PCA) (not covered in this course)
    - Feature selection techniques.

## SVMs with RBF kernel 
- RBF Kernel: Radial Basis Function, a way to transform data into higher dimensions implicitly.
- Strengths 
    - Effective in high-dimensional and sparse data
    - Good performance on non-linear problems.
- Hyperparameters:
    - C$: Regularization parameter (trade-off between correct classification of training examples and maximization of the decision margin).
	- $\gamma$: Defines how far the influence of a single training example reaches.

## Intuition of `C` and `gamma` in SVM RBF
- `C` (Regularization): Controls the trade-off between perfect training accuracy and having a simpler decision boundary. 
    - High C: Strict, complex boundary (overfitting risk).
    - Low C: More errors allowed, smoother boundary (generalizes better).
- `Gamma` (Kernel Width): Controls the influence of individual data points.
	- High Gamma: Points have local impact, complex boundary.
	- Low Gamma: Points affect broader areas, smoother boundary.
- Key trade-off: Proper balance between `C` and `gamma` is crucial for avoiding overfitting or underfitting.


# [Class demo](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_04-kNNs-SVMs.ipynb)

(time permitting)