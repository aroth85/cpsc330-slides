---
title: "CPSC 330 Lecture 9: Classification Metrics" 
author: "Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)"
description: "Metrics for classification"
description-short: "confusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance" 
format:
  revealjs:
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Announcements 

- Important information about midterm 1
  - https://piazza.com/class/m4ujp0s4xgm5o5/post/204
- HW4 has been released. Due next week Monday. 
- Remember to attend tutorials!
  - This weeks tutorial will cover data imbalance and fairness


```{python}
import os
import sys
import pandas as pd 
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from scipy.stats import expon, lognorm, loguniform, randint, uniform, norm, randint
%matplotlib inline
import mglearn
DATA_DIR = 'data/' 
```

## Learning outcomes

- Understand why different classification metrics are necessary
- Understand when to use different metrics


## ML workflow 
![](img/ml-workflow.png)

## Accuracy
- So far we have been measuring classification model performance using **Accuracy**. 
- **Accuracy** is the proportion of all classifications that were correct, whether *positive* or *negative*. 
$$Accuracy = \frac{\text{correct classifications}}{\text{total classifications}}$$
- However, in many real-world applications, the dataset is imbalanced or one kind of mistake is more costly than the other
- In such cases, it's better to optimize for one of the other metrics instead.

## Fraud Confusion matrix

- Which types of errors would be most critical for the bank to address?

![](img/fraud-confusion-matrix.png)

## Fraud Confusion matrix
:::: {.columns}
:::{.column width="80%"}
![](img/tp-fp-tn-fn-fraud.png)
:::

:::{.column width="20%"}
- TN $\rightarrow$ True negatives 
- FP $\rightarrow$ False positives 
- FN $\rightarrow$ False negatives
- TP $\rightarrow$ True positives 
:::
::::


## Confusion matrix questions 

Imagine a spam filter model where emails classified as spam are labeled 1 and non-spam emails are labeled 0. If a spam email is incorrectly classified as non-spam, what is this error called?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In an intrusion detection system, intrusions are identified as 1 and non-intrusive activities as 0. If the system fails to identify an actual intrusion, wrongly categorizing it as non-intrusive, what is this type of error called?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In a medical test for a disease, diseased states are labeled as 1 and healthy states as 0. If a healthy patient is incorrectly diagnosed with the disease, what is this error known as?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative


## Precision and Recall
![](img/precision-recall.png)
![](img/fraud-precision-recall.png)

## F1-score

- F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. 
- F1-score is a harmonic mean of precision and recall. 

$$ F1 = 2 \times \frac{ precision \times recall}{precision + recall}$$

## iClicker Exercise 9.1

**iClicker cloud join link: https://join.iclicker.com/HTRZ**

**Select all of the following statements which are TRUE.**

- (A) In medical diagnosis, false positives are more damaging than false negatives (assume "positive" means the person has a disease, "negative" means they don't).
- (B) In spam classification, false positives are more damaging than false negatives (assume "positive" means the email is spam, "negative" means they it's not).
- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.
- (D) If method A gets a higher accuracy than method B, that means its recall is also higher.

## Counter examples

Method A - higher accuracy but lower precision

| Negative | Positive
| -------- |:-------------:|
| 90      | 5|
| 5      | 0|

Method B - lower accuracy but higher precision

| Negative | Positive
| -------- |:-------------:|
| 80      | 15|
| 0      | 5|

# [Classification metrics class demo](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_09-classification-metrics.ipynb)

## Thresholding 

- The above metrics assume a fixed threshold. 
- We use thresholding to get the binary prediction. 
- A typical threshold is 0.5.
    - A prediction of 0.90 $\rightarrow$ a high likelihood that the transaction is fraudulent and we predict **fraud**
    - A prediction of 0.20 $\rightarrow$ a low likelihood that the transaction is non-fraudulent and we predict **Non fraud**
- **What happens if the predicted score is equal to the chosen threshold?**

- [Play with classification thresholds](https://developers.google.com/machine-learning/crash-course/classification/thresholding)

## iClicker Exercise 9.2

**iClicker cloud join link: https://join.iclicker.com/HTRZ**

**Select all of the following statements which are TRUE.**

- (A) If we increase the classification threshold, both true and false positives are likely to decrease.
- (B) If we increase the classification threshold, both true and false negatives are likely to decrease.
- (C) Lowering the classification threshold generally increases the modelâ€™s recall.  
- (D) Raising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives.


## PR curve
- Calculate precision and recall (TPR) at every possible threshold and graph them. 
- Better choice for highly imbalanced datasets 

![](img/pr-curve-example.png)


## ROC curve 
- Calculate the true positive rate (TPR) and false positive rate (FPR) at every possible thresholding and graph TPR over FPR. 
- Good choice when the datasets are roughly balanced. 
![](img/roc-curve-example.png)

## AUC 
- The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.

# [Classification metrics class demo](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_09-classification-metrics.ipynb)

## ROC AUC questions

Consider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?

:::: {.columns}

:::{.column width="50%"}
![](img/auc_abc)
:::

:::{.column width="50%"}

- (1) If false positives (false alarms) are highly costly
- (2) If false positives are cheap and false negatives (missed true positives) highly costly
- (3) If the costs are roughly equivalent
:::
::::

## Questions for you {.smaller}
:::: {.columns}
:::{.column width="60%"}
![](img/roc-baseline.png) 
:::
:::{.column width="40%"}
- What's the AUC of a baseline model? 
:::
::::

[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

## Questions for you 

- What's the difference between the average precision (AP) score and F1-score? 
- Which model would you pick? 

![](img/pr-curve-which-model.png)


[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

## Questions for you {.smaller}

:::: {.columns}
:::{.column width="60%"}
![](img/roc-curve-which-model.png)
:::
:::{.column width="40%"}
- Which model would you pick? 
:::
::::

## Dealing with class imbalance
- Under sampling 
- Oversampling 
- `class weight="balanced"` (preferred method for this course)
- SMOTE

## What did we learn today?

- A number of possible ways to evaluate machine learning classification models
- What precision, recall, confusion matrix etc. are
- How thresholds can change performance
- The difference between metrics at a single threshold (f1) and metrics for all thresholds (AP)
