{
  "hash": "642743cb22dcec03d42ba1a978622e0e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Lecture 2: Terminology, Baselines, Decision Trees'\nauthor: \"Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)\"\ndescription: Terminology, decision Trees\n\nformat:\n    revealjs:\n        html-math-method: plain\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\n\neditor:\n  render-on-save: true\n---\n\n\n\n## Announcements \n\n- Slide link [https://aroth85.github.io/cpsc330-slides](https://aroth85.github.io/cpsc330-slides)\n- Things due this week \n    - Homework 1 (hw1): Due Jan 14 11:59pm \n- Homework 2 (hw2) has been released (Due: Jan 20, 11:59pm)\n    - There is some autograding in this homework. \n- You can find the tentative due dates for all deliverables [here](https://github.com/UBC-CS/cpsc330-2024W2).\n- Please monitor Piazza (especially pinned posts and instructor posts) for announcements. \n- I'll assume that you've watched the pre-lecture videos.\n\n## Suggested workflow for working with Jupyter Notebooks {.smaller}\n\n- Create a folder on your computer that will have all the CPSC 330 repos:\n  - `~/School/Year3/CPSC330/` <-- Consider this your CPSC parent folder\n- Create subfolders for: `hw`, `class`, `practice`\n- In the `hw` folder, you will then clone `hw1`, `hw2`, `hw3`, etc...\n- In the `class` folder, you will clone the `CPSC330-2024W2` repo which contains all the class jupyter notebooks\n  - Do **not** make any changes to files in this directory/repo, you will have trouble when you pull stuff during each class.\n  - If you did make changes, you can reset to the last commit and DESTROY any changes you made (be careful with this command) using: `git reset --hard`\n- In the `practice` folder, you can **copy** any notebooks (`.ipynb`) and files (like data/\\*.csv) you want to try running locally and experiment\n\n## Learning outcomes {.smaller}\nFrom this lecture, you will be able to \n\n- Identify whether a given problem could be solved using supervised machine learning or not; \n- Differentiate between supervised and unsupervised machine learning;\n- Explain machine learning terminology such as features, targets, predictions, training, and error;\n- Differentiate between classification and regression problems;\n\n## Learning outcomes (contd) {.smaller}\n- Use `DummyClassifier` and `DummyRegressor` as baselines for machine learning problems;\n- Explain the `fit` and `predict` paradigm and use `score` method of ML models; \n- Broadly describe how decision tree prediction works;\n- Use `DecisionTreeClassifier` and `DecisionTreeRegressor` to build decision trees using `scikit-learn`; \n- Visualize decision trees; \n- Explain the difference between parameters and hyperparameters; \n- Explain the concept of decision boundaries;\n- Explain the relation between model complexity and decision boundaries.\n\n## Big picture\n\nIn this lecture, we'll talk about our first machine learning model: Decision trees. \nWe will also familiarize ourselves with some common terminology in supervised machine learning.\n\n## Recap: What is ML? \n\n- ML uses data to build models that find patterns, make predictions, or generate content.\n- It helps computers learn from data to make decisions.\n- No one model works for every situation.\n\n## Recap: Supervised learning\n\n- We wish to find a model function $f$ that relates $X$ to $y$.\n- We use the model function to predict targets of new examples. \n\n![](img/sup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\nIn the first part of this course, we'll focus on supervised machine learning.\n\n## Unsupervised learning\n\n- In **unsupervised learning** training data consists of observations ($X$) **without any corresponding targets**. \n- Unsupervised learning could be used to **group similar things together** in $X$ or to provide **concise summary** of the data.\n\n![](img/unsup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\nWe'll learn more about this topic later.\n\n\n## iClicker 2.1: Supervised vs unsupervised {.smaller}\n\nClicker cloud join link: [https://join.iclicker.com/HTRZ](https://join.iclicker.com/HTRZ)\n\nSelect all of the following statements which are examples of supervised machine learning\n\n- (A) Finding groups of similar properties in a real estate data set.\n- (B) Predicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n- (C) Grouping articles on different topics from different news sources (something like the Google News app).\n- (D) Detecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n- (E) Given some measure of employee performance, identify the key factors which are likely to influence their performance.\n\n## Framework\n\n- There are many frameworks to do do machine learning. \n- We'll mainly be using [`scikit-learn` framework](https://scikit-learn.org/stable/). \n\n::: {#5c0ee9f8 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <iframe\n            width=\"1000\"\n            height=\"650\"\n            src=\"https://scikit-learn.org\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n## Running example {.smaller}\n\nImagine youâ€™re in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people. Here are the first few rows of this toy dataset.\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 600px;\"}\n\n::: {#55617081 .cell execution_count=3}\n``` {.python .cell-code}\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n# Terminology\n\n## Features, target, example\n- What are the **features** $X$? \n  - features = inputs = predictors = explanatory variables = regressors = independent variables = covariates \n- What's the target $y$?\n  - target = output = outcome = response variable = dependent variable = labels \n- Can you think of other relevant features for the job happiness problem?\n\n## Classification vs. Regression {.smaller}\nIn supervised machine learning, there are two main kinds of learning problems based on what they are trying to predict.\n\n- **Classification problem**: predicting among two or more discrete classes\n    - Example1: Predict whether a patient has a liver disease or not\n    - Example2: Predict whether a student would get an A+ or not in quiz2.  \n\n- **Regression problem**: predicting a continuous value\n    - Example1: Predict housing prices \n    - Example2: Predict a student's score in quiz2.\n\n## iClicker 2.2: Classification vs. Regression {.smaller}\n\nClicker cloud join link: [https://join.iclicker.com/HTRZ](https://join.iclicker.com/HTRZ)\n \nSelect all of the following statements which are examples of regression problems\n\n- (A) Predicting the price of a house based on features such as number of bedrooms and the year built.\n- (B) Predicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n- (C) Predicting percentage grade in CPSC 330 based on past grades.\n- (D) Predicting whether you should bicycle tomorrow or not based on the weather forecast.\n- (E) Predicting appropriate thermostat temperature based on the wind speed and the number of people in a room.\n\n## Classification vs. Regression {.smaller}\n\n- Is this a **classification** problem or a **regression** problem?  \n\n::: {#565fafab .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Prediction vs. Inference\n- **Inference** is using the model to understand the relationship between the features and the target \n  - Why certain factors influence happiness? \n- **Prediction** is using the model to predict the target value for new examples based on learned patterns.\n- Of course these goals are related, and in many situations we need both. \n\n## Training \n- In supervised ML, the goal is to learn a function that maps input features ($X$) to a target ($y$).\n- The relationship between $X$ and $y$ is often complex, making it difficult to  define mathematically.\n- We use algorithms to approximate this complex relationship between $X$ and $y$.\n- **Training** is the process of applying an algorithm to learn the best function (or model) that maps $X$ to $y$. \n- In this course, I'll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline.\n\n# Baselines\n\n## Separating $X$ and $y$\n\n- In order to train a model we need to separate $X$ and $y$ from the dataframe. \n\n::: {#b099298e .cell execution_count=5}\n``` {.python .cell-code}\nX = toy_happiness_df.drop(columns=[\"happy?\"]) # Extract the feature set by removing the target column \"happy?\"\ny = toy_happiness_df[\"happy?\"] # Extract the target variable \"happy?\"\n```\n:::\n\n\n## Baseline {.smaller}\n\n- Let's try a simplest algorithm of predicting the most popular target! \n\n::: {#09d1e5b5 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.dummy import DummyClassifier\n# Initialize the DummyClassifier to always predict the most frequent class\nmodel = DummyClassifier(strategy=\"most_frequent\") \n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\n# Add the predicted values as a new column in the dataframe\ntoy_happiness_df['dummy_predictions'] = model.predict(X) \ntoy_happiness_df\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n      <th>dummy_predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## `score` your model\n\n- How do you know how well your model is doing?\n- For classification problems, by default, `score` gives the **accuracy** of the model, i.e., proportion of correctly predicted targets.\n\n::: {#987dd0a3 .cell execution_count=7}\n``` {.python .cell-code}\nmodel.score(X, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.5714285714285714\n```\n:::\n:::\n\n\n## Steps to train a classifier using `sklearn` \n\n1. Read the data\n2. Create $X$ and $y$\n3. Create a classifier object\n4. `fit` the classifier\n5. `predict` on new examples\n6. `score` the model\n\n\n## [`DummyRegressor`](https://scikit-learn.org/0.15/modules/generated/sklearn.dummy.DummyRegressor.html)\n\nYou can also do the same thing for regression problems using `DummyRegressor`, which predicts mean, median, or constant value of the training set for all examples. \n\n# Decision trees \n\n## Pre-Intuition\n\nLet's play [20 questions](https://en.wikipedia.org/wiki/Twenty_questions)!\nYou can ask me up to 20 Yes/No questions to figure out the answer.\n<br>\n<br>\n\n## Intuition\n- Decision trees find the \"best\" way to split data to make predictions.\n- Each split is based on a question, like 'Are the colleagues supportive?'\n- The goal is to group data by similar outcomes at each step.\n- Now, let's see a decision tree using sklearn.\n\n## Decision tree with `sklearn`\nLet's train a simple decision tree on our toy dataset.  \n\n::: {#bb9aac51 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\n# Create a decision tree object\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1)\n# Train the model on the feature set X and target variable y\nmodel.fit(X, y)\nplot_tree(\n  model, filled=True, feature_names=X.columns, \n  class_names=[\"Happy\", \"Unhappy\"], impurity=False, fontsize=12\n);\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-9-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Prediction \n- Given a new example, how does a decision tree predict the class of this example?  \n- What would be the prediction for the example below using the tree above? \n  - supportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,  \n\n::: {#10eecb94 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-10-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Prediction with `sklearn`\n- What would be the prediction for the example below using the tree above? \n  - supportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,  \n\n::: {#c6726496 .cell execution_count=10}\n``` {.python .cell-code}\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel prediction:  ['Unhappy']\n```\n:::\n:::\n\n\n::: {#f2fd7cc8 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-12-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Training (high level) {.smaller}\n\n- How many possible questions could we ask in this context?\n\n::: {#4752a00d .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Training (high level)\n- Decision tree learning is a search process to find the \"best\" tree among many possible ones.\n- We evaluate questions using measures like **information gain** or the **Gini index** to find the most effective split.\n- At each step, we aim to split the data into groups with more certainty in their outcomes.\n\n## Parameters vs. Hyperparameters \n- Parameters \n  - The questions (features and thresholds) used to split the data at each node.\n  - Example: salary <= 75000 at the root node  \n- Hyperparameters\n  - Settings that control tree growth, like `max_depth`, which limits how deep the tree can go.\n\n## Decision boundary with `max_depth=1`\n\n::: {#39ec823e .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-14-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## Decision boundary with `max_depth=2`\n\n::: {#487509f6 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-15-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## iClicker 2.3: Baselines and Decision trees {.smaller}\n\niClicker cloud join link: [https://join.iclicker.com/HTRZ](https://join.iclicker.com/HTRZ)\n\nSelect all of the following statements which are TRUE.\n\n- (A) Change in features (i.e., binarizing features above) would change DummyClassifier predictions.\n- (B) Predict takes only X as argument whereas fit and score take both X and y as arguments.\n- (C) For the decision tree algorithm to work, the feature values must be binary.\n- (D) The prediction in a decision tree works by routing the example from the root to the leaf.\n\n## Question for next lecture: Decision boundaries {.smaller}\n\nSelect the TRUE statement.\n\n- (A) The decision boundary in the image below could come from a decision tree.\n- (B) The decision boundary in the image below could **not** come from a decision tree.\n- (C) There is not enough information to determine if a decision tree could create this boundary.\n\n::: {#d0152584 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](slides-02_files/figure-revealjs/cell-16-output-1.png){width=897 height=454}\n:::\n:::\n\n\n## What we learned today\n\n- There is a lot of terminology and jargon used in ML.\n- Steps to train a supervised machine learning model.\n- What baselines are and why they are useful.\n- What decision trees are and how they work intuitively.\n- What decision boundaries are.\n\n",
    "supporting": [
      "slides-02_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}