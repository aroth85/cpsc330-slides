{
  "hash": "0b288b830d1bb385664ed5bd86ad87c9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Lecture 5: Preprocessing and sklearn pipelines'\nauthor: \"Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)\"\ndescription: \"Preprocessing motivation, Common transformations in `sklearn`, `sklearn` transformers vs. Estimators, The golden rule in the feature transformations, `sklearn` pipelines\"\ndescription-short: 'Pre-processing, Transformations, and pipelines.'\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n\n\n- HW1 grades have been posted.\n    - See syllabus about regrade request etiquette.\n- Homework 1 solutions have been posted on Canvas under Files tab. Please do not share them with anyone or do not post them anywhere.\n- Syllabus quiz is due Jan 24.\n- HW3 should be available.\n\n## Learning outcomes\n\nFrom this lecture, you will be able to \n\n- Explain motivation for preprocessing in supervised machine learning;\n- Discuss golden rule in the context of feature transformations;\n- Identify when to implement feature transformations such as imputation, scaling, and one-hot encoding in a machine learning model development pipeline; \n- Use `sklearn` transformers for applying feature transformations on your dataset;\n- Use `sklearn.pipeline.Pipeline` and `sklearn.pipeline.make_pipeline` to build a preliminary machine learning pipeline.\n\n## Recap \n\n- Decision trees: Split data into subsets based on feature values to create decision rules \n- $k$-NNs: Classify based on the majority vote from k nearest neighbors\n- SVM RBFs: Create a boundary using an RBF kernel to separate classes\n\n## Motivation\n::: {.incremental}\n- So far we have seen\n    - Three ML models (decision trees, $k$-NNs, SVMs with RBF kernel)\n    - ML fundamentals (train-validation-test split, cross-validation, the fundamental tradeoff, the golden rule)\n\n- Are we ready to do machine learning on real-world datasets?\n    - Very often real-world datasets need preprocessing before we use them to build ML models.\n:::\n\n## (iClicker) Exercise 5.1\niClicker cloud join link: **https://join.iclicker.com/HTRZ**\n\nTake a guess: In your machine learning project, how much time will you typically spend on data preparation and transformation?\n\n- (A) ~80% of the project time\n- (B) ~20% of the project time\n- (C) ~50% of the project time\n- (D) None. Most of the time will be spent on model building\n\nThe question is adapted from [here](https://developers.google.com/machine-learning/crash-course/numerical-data).\n\n## Preprocessing motivation: example \n\nYouâ€™re trying to find a suitable date based on:\n\n- Age (closer to yours is better).\n- Number of Facebook Friends (how should we interpret?).\n\n## Preprocessing motivation: example \n\n- You are 30 years old and have 250 Facebook friends.\n\n| Person | Age | #FB Friends | Euclidean Distance Calculation  | Distance    |\n|--------|-----|-------------|---------------------------------|-------------|\n| A      | 25  | 400         | âˆš(5Â² + 150Â²)                    | 150.08      |\n| B      | 27  | 300         | âˆš(3Â² + 50Â²)                     | 50.09       |\n| C      | 30  | 500         | âˆš(0Â² + 250Â²)                    | 250.00      |\n| D      | 60  | 250         | âˆš(30Â² + 0Â²)                     | 30.00       |\n\nBased on the distances, the two nearest neighbors (2-NN) are:\n\n- **Person D** (Distance: 30.00)\n- **Person B** (Distance: 50.09)\n\nWhat's the problem here? \n\n# Common transformations\n\n## Imputation: Fill the gaps! (ðŸŸ© ðŸŸ§ ðŸŸ¦)\nFill in missing data using a chosen strategy:\n\n- **Mean**: Replace missing values with the average of the available data.\n- **Median**: Use the middle value.\n- **Most Frequent**: Use the most common value (mode).\n- **KNN Imputation**: Fill based on similar neighbors.\n\n### Example:\nImputation is like filling in your average or median or most frequent grade for an assessment you missed. \n\n```python\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\n```\n\n## Scaling: Everything to the same range! (ðŸ“‰ ðŸ“ˆ)\nEnsure all features have a comparable range.\n\n- **StandardScaler**: Mean = 0, Standard Deviation = 1.\n\n### Example:\nScaling is like adjusting the number of everyoneâ€™s Facebook friends so that both the number of friends and their age are on a comparable scale. This way, one feature doesn't dominate the other when making comparisons.\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n\n## (iClicker) Exercise 5.2\niClicker cloud join link: **https://join.iclicker.com/HTRZ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) `StandardScaler` ensures a fixed range (i.e., minimum and maximum values) for the features.\n- (B) `StandardScaler` calculates mean and standard deviation for each feature separately.\n- (C) In general, itâ€™s a good idea to apply scaling on numeric features before training $k$-NN or SVM RBF models.\n- (D) The transformed feature values might be hard to interpret for humans.\n- (E) After applying `SimpleImputer` the transformed data has a different shape than the original data.\n\n## One-Hot encoding: ðŸŽ  â†’ 1ï¸âƒ£ 0ï¸âƒ£ 0ï¸âƒ£\n\nConvert categorical features into binary columns.\n\n- Creates new binary columns for each category.\n- Useful for handling categorical data in machine learning models.\n\n### Example:\nTurn \"Apple, Banana, Orange\" into binary columns:\n\n| Fruit   | ðŸŽ | ðŸŒ | ðŸŠ |\n|---------|-------|--------|--------|\n| Apple ðŸŽ  |   1   |   0    |   0    |\n| Banana ðŸŒ |   0   |   1    |   0    |\n| Orange ðŸŠ |   0   |   0    |   1    |\n\n```python\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)\n```\n\n## Ordinal encoding: Ranking matters! (â­ï¸â­ï¸â­ï¸ â†’ 3ï¸âƒ£)\nConvert categories into integer values that have a meaningful order.\n\n- Assign integers based on order or rank.\n- Useful when there is an inherent ranking in the data.\n\n### Example:\nTurn \"Poor, Average, Good\" into 1, 2, 3:\n\n| Rating   | Ordinal |\n|----------|---------|\n| Poor     |    1    |\n| Average  |    2    |\n| Good     |    3    |\n\n```python\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nX_ordinal = encoder.fit_transform(X)\n```\n\n# `sklearn` Transformers vs Estimators\n\n## Transformers\n- Are used to transform or preprocess data.\n- Implement the `fit` and `transform` methods.\n  - `fit(X)`: Learns parameters from the data.\n  - `transform(X)`: Applies the learned transformation to the data.\n  \n- **Examples**:\n  - **Imputation** (`SimpleImputer`): Fills missing values.\n  - **Scaling** (`StandardScaler`): Standardizes features.\n\n. . .\n\n> `fit_transform(X)`: Convenience method for calling `fit` and then `transform` on the same data.\n\n## Estimators\n\n- Used to make predictions.\n- Implement `fit` and `predict` methods.\n    - `fit(X, y)`: Learns from labeled data.\n    - `predict(X)`: Makes predictions on new data.\n\n- Examples: `DecisionTreeClassifier`, `SVC`, `KNeighborsClassifier`\n\n. . .\n\n> Regression models are also estimators\n\n# Feature transformations and the golden rule\n\n## How to carry out cross-validation? (improper)\n\n::: {#c7f0687e .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-05_files/figure-revealjs/cell-3-output-1.png){width=1135 height=801}\n:::\n:::\n\n\n## How to carry out cross-validation? (proper)\n\n::: {#5c6409a0 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-05_files/figure-revealjs/cell-4-output-1.png){width=1135 height=653}\n:::\n:::\n\n\n## The golden rule in feature transformations\n- **Never** transform the entire dataset at once!\n- **Why**? It leads to **data leakage** â€” using information from the test set in your training process, which can artificially inflate model performance.\n- **Fit** transformers like scalers and imputers on the **training set only**.\n- **Apply** the transformations to both the training and test sets **separately**.\n\n### Example:\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\n## `sklearn` Pipelines\n\n- Pipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\n- Simplify the code and improves readability.\n- Reduce the risk of data leakage by ensuring proper transformation of the training and test sets.\n- Automatically apply transformations in sequence.\n\n### Example:\nChaining a `StandardScaler` with a `KNeighborsClassifier` model.\n\n```python\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n```\n\n## (iClicker) Exercise 5.3\niClicker cloud join link: **https://join.iclicker.com/HTRZ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) You can have scaling of numeric features, one-hot encoding of categorical features, and scikit-learn estimator within a single pipeline.\n- (B) Once you have a `scikit-learn` pipeline object with an estimator as the last step, you can call `fit`, `predict`, and `score` on it.\n- (C) You can carry out data splitting within `scikit-learn` pipeline.\n- (D) We have to be careful of the order we put each transformation and model in a pipeline.\n\n",
    "supporting": [
      "slides-05_files"
    ],
    "filters": [],
    "includes": {}
  }
}