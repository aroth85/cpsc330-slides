{
  "hash": "a7753c16b8fc1ee90c120eecb0d17dc6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'CPSC 330 Lecture 3: ML fundamentals'\nauthor: \"Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)\"\ndescription: Supervised Machine Learning Fundamentals\ndescription-short: 'generalization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule'\nformat:\n  revealjs:\n    slide-number: true\n    theme:\n      - slides.scss\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/\n---\n\n\n\n## Announcements \n\n- Homework 2 (hw2) (Due: Jan 20, 11:59pm)\n  - You are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.\n  - Group submissions are not allowed for this assignment.\n- Advice on keeping up with the material\n  - Practice!\n  - Make sure you run the lecture notes on your laptop and experiment with the code. \n  - Start early on homework assignments.\n- If you are still on the waitlist, it‚Äôs your responsibility to keep up with the material and submit assignments.\n- Last day to drop without a W standing: Jan 17\n\n\n## Learning outcomes\n\nFrom this lecture, you will be able to \n\n- Explain how decision boundaries change with the `max_depth` hyperparameter and this relates to model complexity\n- Explain the concept of generalization\n- Explain how and why we split data for training\n- Describe the fundamental tradeoff between training score and the train-test gap\n- State the golden rule\n\n## Big picture {.smaller}\n\nIn machine learning we want to learn a mapping function from labeled data so that we can predict labels of **unlabeled** data. \n\nFor example, suppose we want to build a spam filtering system.  We will take a large number of spam/non-spam messages from the past, learn patterns associated with spam/non-spam from them, and predict whether **a new incoming message** in someone's inbox is spam or non-spam based on these patterns. \n\nSo we want to learn from the past but ultimately we want to apply it on the **future** email messages.\n\n## Review of decision boundaries {.smaller}\n\nSelect the TRUE statement.\n\n- (A) The decision boundary in the image below could come from a decision tree.\n- (B) The decision boundary in the image below could **not** come from a decision tree.\n- (C) There is not enough information to determine if a decision tree could create this boundary.\n\n::: {#e4a3ba00 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-3-output-1.png){width=897 height=454}\n:::\n:::\n\n\n# Generalization\n\n## Running example\n\n::: {#3f0ddc3d .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ml_experience</th>\n      <th>class_attendance</th>\n      <th>lab1</th>\n      <th>lab2</th>\n      <th>lab3</th>\n      <th>lab4</th>\n      <th>quiz1</th>\n      <th>quiz2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>92</td>\n      <td>93</td>\n      <td>84</td>\n      <td>91</td>\n      <td>92</td>\n      <td>A+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>94</td>\n      <td>90</td>\n      <td>80</td>\n      <td>83</td>\n      <td>91</td>\n      <td>not A+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>78</td>\n      <td>85</td>\n      <td>83</td>\n      <td>80</td>\n      <td>80</td>\n      <td>not A+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>91</td>\n      <td>94</td>\n      <td>92</td>\n      <td>91</td>\n      <td>89</td>\n      <td>A+</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Setup\n\n::: {#58ecfef0 .cell execution_count=4}\n``` {.python .cell-code}\nX = classification_df.drop([\"quiz2\"], axis=1)\ny = classification_df[\"quiz2\"]\nX_subset = X[[\"lab4\", \"quiz1\"]]  # Let's consider a subset of the data for visualization\nX_subset.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lab4</th>\n      <th>quiz1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>91</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>83</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>91</td>\n      <td>89</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Depth = 1\n\n::: {#f9826500 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-6-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## Depth = 2\n\n::: {#6d9a7b59 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-7-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## Depth = 6\n\n::: {#63890a70 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-8-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## Complex models decrease training error\n\n::: {#ee5c64b2 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-9-output-1.png){width=846 height=454}\n:::\n:::\n\n\n## Question\n\n- How to pick the best depth? \n- How can we make sure that the model we have built would do reasonably well on new data in the wild when it's deployed? \n- Which of the following rules learned by the decision tree algorithm are likely to generalize better to new data? \n\n> Rule 1: If class_attendance == 1 then grade is A+. \n\n> Rule 2: If lab3 > 83.5 and quiz1 <= 83.5 and lab2 <= 88 then quiz2 grade is A+\n\nThink about these questions on your own or discuss them with your friend/neighbour.\n\n\n## Generalization: Fundamental goal of ML\n\n> **To generalize beyond what we see in the training examples**\n\nWe only have access to limited amount of training data and we want to learn a mapping function which would predict targets reasonably well for examples beyond this training data. \n\n\n## Generalizing to unseen data\n\n- What prediction would you expect for each image?   \n\n![](img/generalization-predict.png)\n\n\n## Training error vs. Generalization error \n\n- Given a model $M$, in ML, people usually talk about two kinds of errors of $M$. \n    1. Error on the training data: $error_{training}(M)$ \n    2. Error on the entire distribution $D$ of data: $error_{D}(M)$\n- We are interested in the error on the entire distribution\n\n. . .\n\n... But we do not have access to the entire distribution üòû\n\n# Data splitting\n\n## How to approximate generalization error? \n\nA common way is **data splitting**. \n\n::: {.incremental}\n- Keep aside some randomly selected portion from the training data.\n- `fit` (train) a model on the training portion only. \n- `score` (assess) the trained model on this set aside data to get a sense of how well the model would be able to generalize.\n- Pretend that the kept aside data is representative of the real distribution $D$ of data. \n:::\n\n## Train/test split\n\n![](img/train-test-split.png)\n\n\n## `train_test_split`\n\n::: {#eb5cb8da .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n# 80%-20% train test split on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=99\n) \n```\n:::\n\n\n::: {#6a268426 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Data portion</th>\n      <th>Shape</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>X</td>\n      <td>(21, 7)</td>\n    </tr>\n    <tr>\n      <td>y</td>\n      <td>(21,)</td>\n    </tr>\n    <tr>\n      <td>X_train</td>\n      <td>(16, 7)</td>\n    </tr>\n    <tr>\n      <td>y_train</td>\n      <td>(16,)</td>\n    </tr>\n    <tr>\n      <td>X_test</td>\n      <td>(5, 7)</td>\n    </tr>\n    <tr>\n      <td>y_test</td>\n      <td>(5,)</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Training vs test error (`max_depth=2`)\n\n::: {#40e2fe7a .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-12-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {#33164422 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain error:   0.125\nTest error:   0.400\n```\n:::\n:::\n\n\n## Training vs test error (`max_depth=6`)\n\n::: {#c9f771b7 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-14-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {#1d2ea78c .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain error:   0.000\nTest error:   0.600\n```\n:::\n:::\n\n\n## Train/validation/test split\n\n- Sometimes it's a good idea to have a separate data for hyperparameter tuning.\n\n![](img/train-valid-test-split.png)\n\n\n## Summary of train, validation, test, and deployment data\n\n|         | `fit` | `score` | `predict` |\n|----------|-------|---------|-----------|\n| Train    | ‚úîÔ∏è      | ‚úîÔ∏è      | ‚úîÔ∏è         |\n| Validation |      | ‚úîÔ∏è      | ‚úîÔ∏è         |\n| Test    |       |  once   | once         |\n| Deployment    |       |       | ‚úîÔ∏è         |\n\n. . .\n\nYou can typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$.\n\n## iClicker 3.1\n\niClicker cloud join link: https://join.iclicker.com/HTRZ\n\n**Select all of the following statements which are TRUE.**\n\n- (A) A decision tree model with no depth (the default `max_depth` in `sklearn`) is likely to perform very well on the deployment data.\n- (B) Data splitting helps us assess how well our model would generalize.\n- (C) Deployment data is scored only once.\n- (D) Validation data could be used for hyperparameter optimization.\n- (E) It‚Äôs recommended that data be shuffled before splitting it into train and test sets.\n\n# Cross validation\n\n## Problems with single train/validation split\n\n- If your dataset is small you might end up with a tiny training and/or validation set.\n- You might be unlucky with your splits such that they don't align well or don't well represent your test data.\n\n![](img/train-valid-test-split.png)\n\n## Cross-validation to the rescue!!\n\n- Split the data into $k$ folds ($k>2$, often $k=10$). In the picture below $k=4$.\n- Each \"fold\" gets a turn at being the validation set.\n\n![](img/cross-validation.png)\n\n\n## Cross-validation using `scikit-learn`\n\n::: {#f1da1e13 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score, cross_validate\nmodel = DecisionTreeClassifier(max_depth=4)\ncv_scores = cross_val_score(model, X_train, y_train, cv=4)\ncv_scores\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([0.5 , 0.75, 0.5 , 0.75])\n```\n:::\n:::\n\n\n<br>\n\n::: {#d6f5dd73 .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage cross-validation score = 0.62\nStandard deviation of cross-validation score = 0.12\n```\n:::\n:::\n\n\n<br>\n\n. . .\n\n::: {#701bdc38 .cell execution_count=17}\n``` {.python .cell-code}\ncv_errors = 1 - cv_scores\n```\n:::\n\n\n<br>\n\n::: {#1b87e75a .cell execution_count=18}\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage cross-validation error = 0.38\nStandard deviation of cross-validation error = 0.12\n```\n:::\n:::\n\n\n## Under the hood\n\n- Cross-validation doesn't shuffle the data; it's done in `train_test_split`.\n\n::: {#b3b4399d .cell execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-20-output-1.png){width=1276 height=251}\n:::\n:::\n\n\n## Our typical supervised learning set up is as follows: \n\n::: {.incremental}\n- We are given training data with features `X` and target `y`\n- We split the data into train and test portions: `X_train, y_train, X_test, y_test`\n- We carry out hyperparameter optimization using cross-validation on the train portion: `X_train` and `y_train`. \n- We assess our best performing model on the test portion: `X_test` and `y_test`.  \n- What we care about is the **test error**, which tells us how well our model can be generalized.\n:::\n\n# The golden rule\n\n## Types of errors\n\nImagine that your train and validation errors do not align with each other. How do you diagnose the problem?  \n\nWe're going to think about 4 types of errors:\n\n- $E_\\textrm{train}$ is your training error (or mean train error from cross-validation).\n- $E_\\textrm{valid}$ is your validation error (or mean validation error from cross-validation).\n- $E_\\textrm{test}$ is your test error.\n- $E_\\textrm{best}$ is the best possible error you could get for a given problem.\n\n## Underfitting\n\n::: {#a29f58fd .cell execution_count=20}\n``` {.python .cell-code}\nmodel = DecisionTreeClassifier(max_depth=1)  # decision stump\nscores = cross_validate(model, X_train, y_train, cv=4, return_train_score=True)\n```\n:::\n\n\n::: {#3228c0e7 .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain error:   0.229\nValidation error:   0.438\n```\n:::\n:::\n\n\n## Overfitting \n\n::: {#a599cb5b .cell execution_count=22}\n``` {.python .cell-code}\nmodel = DecisionTreeClassifier(max_depth=6)\nscores = cross_validate(model, X_train, y_train, cv=4, return_train_score=True)\n```\n:::\n\n\n::: {#557fe931 .cell execution_count=23}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain error:   0.000\nValidation error:   0.438\n```\n:::\n:::\n\n\n## The \"fundamental tradeoff\" of supervised learning:\n\n\n**As you increase model complexity, $E_\\textrm{train}$ tends to go down but $E_\\textrm{valid}-E_\\textrm{train}$ tends to go up.**\n\n\n## Bias vs variance tradeoff \n\n- The fundamental trade-off is also called the bias/variance tradeoff in supervised machine learning.\n\n**Bias**\n: the tendency to consistently learn the same wrong thing (high bias corresponds to underfitting)\n\n**Variance** \n: the tendency to learn random things irrespective of the real signal (high variance corresponds to overfitting)\n\n## How to pick a model that would generalize better?\n\n- We want to avoid both underfitting and overfitting. \n- We want to be consistent with the training data but we don't to rely too much on it. \n\n![](img/malp_0201.png) \n\nThere are many subtleties here and there is no perfect answer but a  common practice is to pick the model with minimum cross-validation error.\n\n<font size=\"2\">[source](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#relation-of-model-complexity-to-dataset-size)</font>\n\n\n## The golden rule <a name=\"4\"></a>\n\n- Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. \n- We have to be very careful not to violate it while developing our ML pipeline. \n- Even experts end up breaking it sometimes which leads to misleading results and lack of generalization on the real data. \n\n## Here is the workflow we'll generally follow. \n\n::: {.incremental}\n\n- **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`. \n\n- **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) \n\n- **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance.\n\n:::\n\n. . .\n\n**Again, there are many subtleties here we'll discuss the golden rule multiple times throughout the course.**  \n\n## iClicker 3.2\n\niClicker cloud join link: https://join.iclicker.com/HTRZ\n\n**Select all of the following statements which are TRUE.**\n\n- (A) $k$-fold cross-validation calls fit $k$ times\n- (B) We use cross-validation to get a more robust estimate of model performance.\n- (C) If the mean train accuracy is much higher than the mean cross-validation accuracy it's likely to be a case of overfitting.\n- (D) The fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n- (E) A decision stump on a complicated classification problem is likely to underfit.\n\n\n\n## What we learned today?\n\n- Importance of generalization in supervised machine learning\n- Data splitting as a way to approximate generalization error\n- Train, test, validation, deployment data\n- Cross-validation\n- Overfitting, underfitting, the fundamental tradeoff, and the golden rule.\n\n## Class demo (Time permitting)\n\nCopy this notebook to your working directory and follow along.\n\n[https://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_03-ml-fundamentals.ipynb](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/lectures/204-Andy-lectures/class_demos/demo_03-ml-fundamentals.ipynb)\n\n",
    "supporting": [
      "slides-03_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}