{
  "hash": "cb171c146314cdb7dbe67bbcaf859187",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 8: Hyperparameter Optimization\"\nauthor: \"Andrew Roth (Slides adapted from Varada Kolhatkar and Firas Moosvi)\"\ndescription: \"Linear regression, logistic regression\"\ndescription-short: \"Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/m4ujp0s4xgm5o5/post/204\n- HW3 is due next week Monday, Feb 3rd, 11:59 pm. \n- Reminder my office hours\n  - Tuesday from 12:30 to 1:30 in my office ICCS 353\n\n\n\n## Learning outcomes\n\n- Explain the need for hyperparameter optimization  \n- Carry out hyperparameter optimization using `sklearn`'s `GridSearchCV` and `RandomizedSearchCV` \n- Explain different hyperparameters of `GridSearchCV`\n- Explain the importance of selecting a good range for the values. \n- Explain optimization bias\n- Identify and reason when to trust and not trust reported accuracies \n\n# Recap\n\n## Recap: Logistic regression\n- A **linear model used for binary classification** tasks. \n  - (Optional) There is am extension of logistic regression called multinomial logistic regression for multiclass classification.\n- Parameters: \n  - **Coefficients (Weights)**: The model learns a coefficient or a weight associated with each feature that represents its importance.\n  - **Bias (Intercept)**: A constant term added to the linear combination of features and their coefficients.\n\n## Recap: Logistic regression \n- The model computes a weighted sum of the input features‚Äô values, adjusted by their respective coefficients and the bias term.\n- This weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the \"positive\" class.\n\n\\begin{equation}\n\\hat{p} = \\sigma\\left(\\sum_{j=1}^d w_j x_j + b\\right) \n\\end{equation}\n\n- $\\hat{p}$ is the predicted probability of the example belonging to the positive class. \n- $w_j$ is the learned weight associated with feature $j$\n- $x_j$ is the value of the input feature $j$\n- $b$ is the bias term \n\n## Recap: Logistic regression\n\n- For a dataset with $d$ features, the decision boundary that \nseparates the classes is a $d-1$ dimensional hyperplane.  \n- Complexity hyperparameter: `C` in `sklearn`. \n  - Higher `C` $\\rightarrow$ more complex model meaning larger coefficients\n  - Lower `C` $\\rightarrow$ less complex model meaning smaller coefficients\n\n\n## Recap: `CountVectorizer` input \n\n- Primarily designed to accept either a `pandas.Series` of text data or a 1D `numpy` array. It can also process a list of string data directly.\n- Unlike many transformers that handle multiple features (`DataFrame` or 2D `numpy` array), `CountVectorizer` a single text column at a time.\n- If your dataset contains multiple text columns, you will need to instantiate separate `CountVectorizer` objects for each text feature.\n- This approach ensures that the unique vocabulary and tokenization processes are correctly applied to each specific text column without interference.\n\n# Motivation\n\n## Hyperparameter optimization\n\n![](img/hyperparam-optimization.png)\n\n\n## Data\n\n::: {#278c5bad .cell execution_count=2}\n``` {.python .cell-code}\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3130</th>\n      <td>spam</td>\n      <td>LookAtMe!: Thanks for your purchase of a video...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>ham</td>\n      <td>Aight, I'll hit you up when I get some cash</td>\n    </tr>\n    <tr>\n      <th>4697</th>\n      <td>ham</td>\n      <td>Don no da:)whats you plan?</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>ham</td>\n      <td>Going to take your babe out ?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Model building \n\n- Let's define a pipeline \n\n::: {#e8958f55 .cell execution_count=3}\n``` {.python .cell-code}\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n```\n:::\n\n\n- What are some hyperparameters for this pipeline?\n\n. . .\n\n\n- Suppose we want to try out different hyperparameter values. \n\n::: {#9ff682c0 .cell execution_count=4}\n``` {.python .cell-code}\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}\n```\n:::\n\n\n## Hyperparameters: the problem\n\n- In order to improve the generalization performance, finding the best values for the important hyperparameters of a model is necessary for almost all models and datasets. \n\n- Picking good hyperparameters is important because if we don't do it, we might end up with an underfit or overfit model. \n\n## Manual hyperparameter optimization procedure\n\n- Define a parameter space.\n- Iterate through possible combinations.\n- Evaluate model performance.\n\n. . .\n\n- What are some limitations of this approach? \n\n## Manual hyperparameter optimization\n\n- Advantage: we may have some intuition about what might work.\n  - E.g. if I'm massively overfitting, try decreasing `max_depth` or `C`.\n\n- Disadvantages\n    - It takes a lot of work\n    - Not reproducible\n    - In very complicated cases, our intuition might be worse than a data-driven approach\n\n## Automated hyperparameter optimization \n\n- Formulate the hyperparamter optimization as a one big search problem. \n\n- Often we have many hyperparameters of different types: categorical, integer, and continuous.\n\n- Often, the search space is quite big and systematic search for optimal values is infeasible. \n\n## `sklearn` methods \n\n- `sklearn` provides two main methods for hyperparameter optimization\n  - Grid Search\n  - Random Search\n\n# Grid search\n\n## Grid search overview\n\n- Covers all possible combinations from the provided grid. \n- Can be parallelized easily.\n- Integrates cross-validation.\n\n## Grid search in practice\n\n- For `GridSearchCV` we need\n    - An instantiated model or a pipeline\n    - A parameter grid: A user specifies a set of values for each hyperparameter. \n    - Other optional arguments \n\nThe method considers product of the sets and evaluates each combination one by one.  \n\n## Grid search example \n\n::: {#bdc96725 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(\n  pipe_svm, \n  param_grid=param_grid, \n  n_jobs=-1, \n  return_train_score=True\n)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\nnp.float64(0.9782606272997375)\n```\n:::\n:::\n\n\n> `njobs=-1` will use all available cores\n\n## Problems with exhaustive grid search \n\n- Required number of models to evaluate grows exponentially with the dimensionally of the configuration space. \n\n- Example: Suppose you have\n    - 5 hyperparameters \n    \n    - 10 different values for each hyperparameter\n\n    - You'll be evaluating $10^5=100,000$ models! That is you'll be calling `cross_validate` 100,000 times!\n\n    - Exhaustive search may become infeasible fairly quickly. \n\n- Other options? \n\n# Random search\n\n## Random search overview\n\n- More efficient than grid search when dealing with large hyperparameter spaces.\n- Samples a given number of parameter settings from distributions.\n\n## Random search example \n\n::: {#fbdbdbe6 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform, randint, uniform\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(\n  pipe_svm,                                    \n  param_distributions=param_dist, \n  n_iter=10, \n  n_jobs=-1, \n  return_train_score=True\n)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\nnp.float64(0.9828480625986312)\n```\n:::\n:::\n\n\n## `n_iter`\n\n- Note the `n_iter`, we didn't need this for `GridSearchCV`.\n- Larger `n_iter` will take longer but it'll do more searching.\n  - Remember you still need to multiply by number of folds!\n- You can set `random_state` for reproducibility but you don't have to do it.\n\n\n## Advantages of `RandomizedSearchCV`\n\n- Faster compared to `GridSearchCV`.\n- Adding parameters that do not influence the performance does not affect efficiency.\n- Works better when some parameters are more important than others. \n- In general, I recommend using `RandomizedSearchCV` rather than `GridSearchCV`.\n\n\n## Advantages of `RandomizedSearchCV`\n\n![](img/randomsearch_bergstra.png)\n\n## Questions for class discussion\n\n- Suppose you have 10 hyperparameters, each with 4 possible values. If you run `GridSearchCV` with this parameter grid, how many cross-validation experiments will be carried out? \n- Suppose you have 10 hyperparameters and each takes 4 values. If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`, how many cross-validation experiments will be carried out? \n\n## (iClicker) Exercise 8.1\n\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) If you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n- (B) Grid search is guaranteed to find the best hyperparameter values.\n- (C) It is possible to get different hyperparameters in different runs of RandomizedSearchCV.\n\n# Optimization bias\n\n## Optimization bias (motivation) \n\n- Why do we need to evaluate the model on the test set in the end?\n- Why not just use cross-validation on the whole dataset? \n- While carrying out hyperparameter optimization, we usually try over many possibilities.  \n- If our dataset is small and if your validation set is hit too many times, we suffer from **optimization bias** or **overfitting the validation set**. \n\n## Optimization bias of parameter learning\n- Overfitting of the training error\n- An example: \n    - During training, we could search over tons of different decision trees.    \n    - So we can get \"lucky\" and find a tree with low training error by chance.\n\n## Optimization bias of hyper-parameter learning\n\n- Overfitting of the validation error\n- An example: \n    - Here, we might optimize the validation error over 1000 values of `max_depth`.\n    - One of the 1000 trees might have low validation error by chance.\n\n## (Optional) Example 1: Optimization bias\n\nConsider a multiple-choice (a,b,c,d) \"test\" with 10 questions:\n\n- If you choose answers randomly, expected grade is 25% (no bias).\n- If you fill out two tests randomly and pick the best, expected grade is 33%.\n    - Optimization bias of ~8%.\n- If you take the best among 10 random tests, expected grade is ~47%.\n- If you take the best among 100, expected grade is ~62%.\n- If you take the best among 1000, expected grade is ~73%.\n- If you take the best among 10000, expected grade is ~82%.\n    - You have so many \"chances\" that you expect to do well.\n\n. . .\n\n**But on new questions the \"random choice\" accuracy is still 25%.**\n\n## (Optional) Example 2: Optimization bias {.smaller}\n\nIf we instead used a 100-question test then:\n\n- Expected grade from best over 1 randomly-filled test is 25%.\n- Expected grade from best over 2 randomly-filled test is ~27%.\n- Expected grade from best over 10 randomly-filled test is ~32%.\n- Expected grade from best over 100 randomly-filled test is ~36%.\n- Expected grade from best over 1000 randomly-filled test is ~40%.\n- Expected grade from best over 10000 randomly-filled test is ~43%.\n\n. . .\n\n- The optimization bias **grows with the number of things we try**.\n    - ‚ÄúComplexity‚Äù of the set of models we search over.\n\n. . .\n\n- But, optimization bias **shrinks quickly with the number of examples**.\n    - But it‚Äôs still non-zero and growing if you over-use your validation set!    \n\n## Optimization bias overview\n\n- Why do we need separate validation and test datasets? \n\n![](img/optimization-bias.png)\n\n\n## This is why we need a test set\n- The frustrating part is that if our dataset is small then our test set is also small üòî. \n- But we don't have a lot of better alternatives, unfortunately, if we have a small dataset. \n\n## When test score is much lower than CV score\n- What to do if your test score is much lower than your cross-validation score:\n    - Try simpler models and use the test set a couple of times; it's not the end of the world.\n    - Communicate this clearly when you report the results. \n\n## Mitigating optimization bias.\n  - Cross-validation\n  - Ensembles \n  - Regularization and choosing a simpler model  \n\n## Questions for you\n\n- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n  - Probably\n  - Probably not\n\n# Summary\n\n## Automated hyperparameter optimization\n\n- Advantages \n    - Reduce human effort\n    - Less prone to error and improve reproducibility\n    - Data-driven approaches may be effective\n\n- Disadvantages\n    - May be hard to incorporate intuition\n    - Be careful about overfitting on the validation set\n\n# Extra\n\n## Discussion\n\nLet's say that, for a particular feature, the histograms of that feature are identical for the two target classes. Does that mean the feature is not useful for predicting the target class?\n\n# Looking Ahead\n\n## Group Work: Invention Activity\n\nSo far we have looked only at `score` as a metric for evaluating our metrics.\n\nWhat else could be used as a possible metric? Think of what else might be important for machine learning practioners and stakeholders?\n\nIn your group, brainstorm 4 alternative options:\n\n",
    "supporting": [
      "slides-08_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}